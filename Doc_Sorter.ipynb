{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNW1X1kCLxIbxmW/MOGlBV1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "88472ad664964356ab6f3a9ede90bea5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f3b779fcbad74e6e84d2192289d24867",
              "IPY_MODEL_530d6fc7f35b4f1882f886a97ebae796",
              "IPY_MODEL_bed76d22f8704e428469a41f97818552"
            ],
            "layout": "IPY_MODEL_5302534e91fb423a8f7214bff1fa36ec"
          }
        },
        "f3b779fcbad74e6e84d2192289d24867": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_800779ab2d70466ab56e8da21c9ee2a7",
            "placeholder": "​",
            "style": "IPY_MODEL_24e5b25d702245c6a4aaca6449acd88b",
            "value": "README.md: 100%"
          }
        },
        "530d6fc7f35b4f1882f886a97ebae796": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0d2ed423b4d74eb58af37e31e85f9363",
            "max": 617,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_70d89014814c458bb2537e1131241f9c",
            "value": 617
          }
        },
        "bed76d22f8704e428469a41f97818552": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f419195f5a974d71b3718d928749b2c2",
            "placeholder": "​",
            "style": "IPY_MODEL_9ddd22467a194a0786828e5870993ecd",
            "value": " 617/617 [00:00&lt;00:00, 37.7kB/s]"
          }
        },
        "5302534e91fb423a8f7214bff1fa36ec": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "800779ab2d70466ab56e8da21c9ee2a7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "24e5b25d702245c6a4aaca6449acd88b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0d2ed423b4d74eb58af37e31e85f9363": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "70d89014814c458bb2537e1131241f9c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f419195f5a974d71b3718d928749b2c2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9ddd22467a194a0786828e5870993ecd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "463cc08fc5294d688cd168b12b6ddf0a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2f17446206404f2f8e7b3519b9e2bf62",
              "IPY_MODEL_0ff5d6bb025f4325adf496e5602bc6fd",
              "IPY_MODEL_b7f5efc95edb43e9b422128b53e415c4"
            ],
            "layout": "IPY_MODEL_a4149c4cea964a0bb2ca31e4ceca409c"
          }
        },
        "2f17446206404f2f8e7b3519b9e2bf62": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3a4c12b50fd641389c8a4ba62131331e",
            "placeholder": "​",
            "style": "IPY_MODEL_365e1c09a9194dfa835c3421963f969f",
            "value": "data.csv: 100%"
          }
        },
        "0ff5d6bb025f4325adf496e5602bc6fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_99033e3e1e4c4519bb2e7fb9abd1f697",
            "max": 53217245,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6c0fc56e6f4a4ecd80f5be2c886f529b",
            "value": 53217245
          }
        },
        "b7f5efc95edb43e9b422128b53e415c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1adc4b7a4de64eba87f504ebc29fe43f",
            "placeholder": "​",
            "style": "IPY_MODEL_b6c49459a46a49c48f77e65039652df6",
            "value": " 53.2M/53.2M [00:00&lt;00:00, 155MB/s]"
          }
        },
        "a4149c4cea964a0bb2ca31e4ceca409c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3a4c12b50fd641389c8a4ba62131331e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "365e1c09a9194dfa835c3421963f969f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "99033e3e1e4c4519bb2e7fb9abd1f697": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6c0fc56e6f4a4ecd80f5be2c886f529b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1adc4b7a4de64eba87f504ebc29fe43f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b6c49459a46a49c48f77e65039652df6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "201c60e79e1a4a648b165f8df3862e57": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d42a2f44aa0843939fd6619cf92c633e",
              "IPY_MODEL_fcbcafa66ee7420e9f65d23e3c665d7f",
              "IPY_MODEL_815a537df7ca42939eca011900429a96"
            ],
            "layout": "IPY_MODEL_e38adf060d1a4f0ca775c798fe562949"
          }
        },
        "d42a2f44aa0843939fd6619cf92c633e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2112780fd42c483580e2190ac4a21228",
            "placeholder": "​",
            "style": "IPY_MODEL_67d1693fe0254ae0b194443e388685b6",
            "value": "Generating train split: "
          }
        },
        "fcbcafa66ee7420e9f65d23e3c665d7f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e6ac802f8ba34e6ea56411a8cf52e37f",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_219371f0243a4caeb7e251a439fe53c0",
            "value": 1
          }
        },
        "815a537df7ca42939eca011900429a96": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0e04be6390494a329aa5b614bed2e70f",
            "placeholder": "​",
            "style": "IPY_MODEL_b065da6ad63b43bf9dbcfe45e6390fbe",
            "value": " 44949/0 [00:01&lt;00:00, 30577.20 examples/s]"
          }
        },
        "e38adf060d1a4f0ca775c798fe562949": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2112780fd42c483580e2190ac4a21228": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "67d1693fe0254ae0b194443e388685b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e6ac802f8ba34e6ea56411a8cf52e37f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "219371f0243a4caeb7e251a439fe53c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0e04be6390494a329aa5b614bed2e70f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b065da6ad63b43bf9dbcfe45e6390fbe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fe0709f1c70643d2874600e54f3aad04": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_51b9744f4d724b599eba7901cd28e2f1",
              "IPY_MODEL_05e830b1b9f44836bde15b5e8c2d9cd7",
              "IPY_MODEL_0a9c4725b97741ada7366f348e727e68"
            ],
            "layout": "IPY_MODEL_5491f3c7d92b4598adea1f77b12d09dd"
          }
        },
        "51b9744f4d724b599eba7901cd28e2f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_98a200d284844d48854a21bb309e4e36",
            "placeholder": "​",
            "style": "IPY_MODEL_e0be0a34e69f4ced9b7df3818eb94674",
            "value": "modules.json: 100%"
          }
        },
        "05e830b1b9f44836bde15b5e8c2d9cd7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_64c1f1eaa4a345d69f0784560b21acec",
            "max": 385,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_20392996f12f4afa907355a0fd77ba0f",
            "value": 385
          }
        },
        "0a9c4725b97741ada7366f348e727e68": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_22c268cd8e934257b882bc3700af7615",
            "placeholder": "​",
            "style": "IPY_MODEL_6a5ec42176264ded84cf4c5ca948bbd7",
            "value": " 385/385 [00:00&lt;00:00, 39.7kB/s]"
          }
        },
        "5491f3c7d92b4598adea1f77b12d09dd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "98a200d284844d48854a21bb309e4e36": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e0be0a34e69f4ced9b7df3818eb94674": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "64c1f1eaa4a345d69f0784560b21acec": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "20392996f12f4afa907355a0fd77ba0f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "22c268cd8e934257b882bc3700af7615": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6a5ec42176264ded84cf4c5ca948bbd7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "abe9e25d8e174915b3e5172b9fab47dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f96768db1b994296bc5efd2c877d6d89",
              "IPY_MODEL_0c946566e742476aaa9bc802ee3dc14b",
              "IPY_MODEL_5f0ef976731b4bb0bd005b943b31536f"
            ],
            "layout": "IPY_MODEL_806d8cc89cdf44dd9d40ba91bf2e42ac"
          }
        },
        "f96768db1b994296bc5efd2c877d6d89": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b2ecd5466438494f8dbc2dfffcedd516",
            "placeholder": "​",
            "style": "IPY_MODEL_930730e9d5054959b2e7a8b460884275",
            "value": "README.md: 100%"
          }
        },
        "0c946566e742476aaa9bc802ee3dc14b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4eaa5024e1384309a3db6b93a03ee352",
            "max": 68084,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4f08a828156e496e8ca2863aec066031",
            "value": 68084
          }
        },
        "5f0ef976731b4bb0bd005b943b31536f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4bc8c1a917e24222977c923c855bf4e5",
            "placeholder": "​",
            "style": "IPY_MODEL_23dfb06413d84f4da939c518a7acefd8",
            "value": " 68.1k/68.1k [00:00&lt;00:00, 6.81MB/s]"
          }
        },
        "806d8cc89cdf44dd9d40ba91bf2e42ac": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b2ecd5466438494f8dbc2dfffcedd516": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "930730e9d5054959b2e7a8b460884275": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4eaa5024e1384309a3db6b93a03ee352": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4f08a828156e496e8ca2863aec066031": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4bc8c1a917e24222977c923c855bf4e5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "23dfb06413d84f4da939c518a7acefd8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d80d2639362346ce91d8e908c745c3c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_dbcbcd11dcf14947ae1cc91bcc7116c6",
              "IPY_MODEL_346c277025984002808791df3e19b410",
              "IPY_MODEL_33ef64a4fa324c50a4e13c3b0c469b96"
            ],
            "layout": "IPY_MODEL_e45f18246a4540df84e925f93b770499"
          }
        },
        "dbcbcd11dcf14947ae1cc91bcc7116c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_35acc08d8c834676a66a22af387b5ebf",
            "placeholder": "​",
            "style": "IPY_MODEL_016cdcd4ddee442dba05ccc7841ad949",
            "value": "sentence_bert_config.json: 100%"
          }
        },
        "346c277025984002808791df3e19b410": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_57ea03eb02c3426690cf7f9c2e7faea6",
            "max": 57,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_cb4a149be48948cc91cf4a153be73847",
            "value": 57
          }
        },
        "33ef64a4fa324c50a4e13c3b0c469b96": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2b6d1d0704be46819917f3c528adc94c",
            "placeholder": "​",
            "style": "IPY_MODEL_88f37ab9203147de86e51ee9d10845b1",
            "value": " 57.0/57.0 [00:00&lt;00:00, 5.12kB/s]"
          }
        },
        "e45f18246a4540df84e925f93b770499": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "35acc08d8c834676a66a22af387b5ebf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "016cdcd4ddee442dba05ccc7841ad949": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "57ea03eb02c3426690cf7f9c2e7faea6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cb4a149be48948cc91cf4a153be73847": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2b6d1d0704be46819917f3c528adc94c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "88f37ab9203147de86e51ee9d10845b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "62ffad936be74274bbc117871235fdd5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7a5db3592af2480aa531df442d2108d4",
              "IPY_MODEL_559a8b2ff42f4ea4a4a27091b406ba7e",
              "IPY_MODEL_2f3845fbe07e42848ad69e3dba056858"
            ],
            "layout": "IPY_MODEL_339f407463544204aade5f7beeb51288"
          }
        },
        "7a5db3592af2480aa531df442d2108d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fb390a1dd3d84ff8903f4846e5cdabbe",
            "placeholder": "​",
            "style": "IPY_MODEL_4fc82d6231e8423893f2a9914fac06de",
            "value": "config.json: 100%"
          }
        },
        "559a8b2ff42f4ea4a4a27091b406ba7e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c36fc67bea70480e9130cf947a6f1988",
            "max": 583,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_26992231639b43a1a7405ae37e9499a3",
            "value": 583
          }
        },
        "2f3845fbe07e42848ad69e3dba056858": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_67421ca406154fce8e02cc609f0717dd",
            "placeholder": "​",
            "style": "IPY_MODEL_72d741d75594414bbbbb37f1cc775dcd",
            "value": " 583/583 [00:00&lt;00:00, 67.1kB/s]"
          }
        },
        "339f407463544204aade5f7beeb51288": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fb390a1dd3d84ff8903f4846e5cdabbe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4fc82d6231e8423893f2a9914fac06de": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c36fc67bea70480e9130cf947a6f1988": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "26992231639b43a1a7405ae37e9499a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "67421ca406154fce8e02cc609f0717dd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "72d741d75594414bbbbb37f1cc775dcd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cacecc3c797e48adb2ca4e50efa3a7b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fe7fd9b8fea541be8d5574a52d68bb6e",
              "IPY_MODEL_f3de6db1b6824e619bb8dd4961ab9006",
              "IPY_MODEL_684611b1a2a04c72bb052fc17fe3ce3f"
            ],
            "layout": "IPY_MODEL_2f4f4883499345319f973a7d8006a63c"
          }
        },
        "fe7fd9b8fea541be8d5574a52d68bb6e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a5a9feedccfa4124a001c8d3e8e0bc08",
            "placeholder": "​",
            "style": "IPY_MODEL_ed857605e6074187a5ab77e6ea21277f",
            "value": "model.safetensors: 100%"
          }
        },
        "f3de6db1b6824e619bb8dd4961ab9006": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_86e29ee0c56b434a9e956f145286a22c",
            "max": 66746168,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_afe78a5465474fdbad72670cd7943fde",
            "value": 66746168
          }
        },
        "684611b1a2a04c72bb052fc17fe3ce3f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c4155ec7fded4932853d1034c4c16c84",
            "placeholder": "​",
            "style": "IPY_MODEL_bfadd685e1dd492bbbb020c39165c6c4",
            "value": " 66.7M/66.7M [00:00&lt;00:00, 265MB/s]"
          }
        },
        "2f4f4883499345319f973a7d8006a63c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a5a9feedccfa4124a001c8d3e8e0bc08": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ed857605e6074187a5ab77e6ea21277f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "86e29ee0c56b434a9e956f145286a22c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "afe78a5465474fdbad72670cd7943fde": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c4155ec7fded4932853d1034c4c16c84": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bfadd685e1dd492bbbb020c39165c6c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2f8d89bfeb3e43eaaf3fcd842a581f6f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_22421af7eaf342a886e69281d3a77597",
              "IPY_MODEL_7b3d910d40b449c69d78734db5cb9ce6",
              "IPY_MODEL_15f1da8b3c9e4f12a36df3359bd02b37"
            ],
            "layout": "IPY_MODEL_ef008e9be1cd48da82787c3d5135ffa5"
          }
        },
        "22421af7eaf342a886e69281d3a77597": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_35a15dadef31410b832b8ff5fb2000de",
            "placeholder": "​",
            "style": "IPY_MODEL_23d95b64b61447d2b2d100a8784dfdae",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "7b3d910d40b449c69d78734db5cb9ce6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5ed051acdcfe4a8fbda27d9282b45ce6",
            "max": 394,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_af0896a5d1fb417cb227577a32c3624e",
            "value": 394
          }
        },
        "15f1da8b3c9e4f12a36df3359bd02b37": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_50fb68eeb8754de49c19b77e8bf278ba",
            "placeholder": "​",
            "style": "IPY_MODEL_2635d97cc4b848f3a1fe20d5f50597e8",
            "value": " 394/394 [00:00&lt;00:00, 32.0kB/s]"
          }
        },
        "ef008e9be1cd48da82787c3d5135ffa5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "35a15dadef31410b832b8ff5fb2000de": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "23d95b64b61447d2b2d100a8784dfdae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5ed051acdcfe4a8fbda27d9282b45ce6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "af0896a5d1fb417cb227577a32c3624e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "50fb68eeb8754de49c19b77e8bf278ba": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2635d97cc4b848f3a1fe20d5f50597e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "007392afb06e42e19ba8d838ba8ec01b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7e434a9809cf4034ac132e88f0e94127",
              "IPY_MODEL_6d887fd86a544358be94fe8ed7d42107",
              "IPY_MODEL_a07669fb844d46698e667779d290eb63"
            ],
            "layout": "IPY_MODEL_ec9e07f1f14d4d99a37b838c530de8e1"
          }
        },
        "7e434a9809cf4034ac132e88f0e94127": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_00084c2392944e95b55adaeeac9d7c66",
            "placeholder": "​",
            "style": "IPY_MODEL_3574d62cc7be428eb07f92ae11647c65",
            "value": "vocab.txt: 100%"
          }
        },
        "6d887fd86a544358be94fe8ed7d42107": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6e4f370e401244c29ef4759e26fb9334",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8247661683dd493fbf1be2fe1f339758",
            "value": 231508
          }
        },
        "a07669fb844d46698e667779d290eb63": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_314dc4033b1640e8adef839b291940d4",
            "placeholder": "​",
            "style": "IPY_MODEL_b337edd123514a9b874e128830d1959b",
            "value": " 232k/232k [00:00&lt;00:00, 543kB/s]"
          }
        },
        "ec9e07f1f14d4d99a37b838c530de8e1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "00084c2392944e95b55adaeeac9d7c66": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3574d62cc7be428eb07f92ae11647c65": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6e4f370e401244c29ef4759e26fb9334": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8247661683dd493fbf1be2fe1f339758": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "314dc4033b1640e8adef839b291940d4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b337edd123514a9b874e128830d1959b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6185fc21129f490b9d49a7879936e93d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_aa5dfd092c4b4a8fb0645743762954bd",
              "IPY_MODEL_1981c4a97f7e4653a68882539cb20bab",
              "IPY_MODEL_33e42c974dd545b9a9ad8ef981a7d8a8"
            ],
            "layout": "IPY_MODEL_47921d3e9cfd4d26b469af79cb0024bc"
          }
        },
        "aa5dfd092c4b4a8fb0645743762954bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_744f468845bf4811bd2bd23cb162b963",
            "placeholder": "​",
            "style": "IPY_MODEL_b2f2134440bf4aef9e3857ac49ea0897",
            "value": "tokenizer.json: 100%"
          }
        },
        "1981c4a97f7e4653a68882539cb20bab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e78a4d7c0d27495eaccaf77b9f0948ca",
            "max": 711661,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_59e6fb7a33c0417cbfb6a0c38b5e780d",
            "value": 711661
          }
        },
        "33e42c974dd545b9a9ad8ef981a7d8a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ef474b814ceb451094ed4bebef24ae01",
            "placeholder": "​",
            "style": "IPY_MODEL_8a8fbf1845d64d23a485aa98c121576e",
            "value": " 712k/712k [00:00&lt;00:00, 3.25MB/s]"
          }
        },
        "47921d3e9cfd4d26b469af79cb0024bc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "744f468845bf4811bd2bd23cb162b963": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b2f2134440bf4aef9e3857ac49ea0897": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e78a4d7c0d27495eaccaf77b9f0948ca": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "59e6fb7a33c0417cbfb6a0c38b5e780d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ef474b814ceb451094ed4bebef24ae01": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8a8fbf1845d64d23a485aa98c121576e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ebdd1e4e459a4b1480ecec598ab73982": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_12dbc33037914895bd87188392cb4fbb",
              "IPY_MODEL_f87cebc04fb64a1888a96c438aa9b31c",
              "IPY_MODEL_a723b3239d1a40b48fdf3aead16e4e79"
            ],
            "layout": "IPY_MODEL_0e0be00ac1e34152a623fca249498886"
          }
        },
        "12dbc33037914895bd87188392cb4fbb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2fa606d03117408b9bd311d2cd691746",
            "placeholder": "​",
            "style": "IPY_MODEL_c262889daafe483d8a9c84fab65d1140",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "f87cebc04fb64a1888a96c438aa9b31c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2bcc0bac7f7342ef9aab0f2ea1c395c8",
            "max": 125,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c21856871e1c412093e0551af380dcf6",
            "value": 125
          }
        },
        "a723b3239d1a40b48fdf3aead16e4e79": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f587a00d215a4c98b35ea9eee8d51b7f",
            "placeholder": "​",
            "style": "IPY_MODEL_8a65d119cbf440bfbd5abe54a1849fd3",
            "value": " 125/125 [00:00&lt;00:00, 14.1kB/s]"
          }
        },
        "0e0be00ac1e34152a623fca249498886": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2fa606d03117408b9bd311d2cd691746": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c262889daafe483d8a9c84fab65d1140": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2bcc0bac7f7342ef9aab0f2ea1c395c8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c21856871e1c412093e0551af380dcf6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f587a00d215a4c98b35ea9eee8d51b7f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8a65d119cbf440bfbd5abe54a1849fd3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "98b3b20670624c0886ac1336a9fbc2eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5c553942a0954e0a95c4a97d885ed112",
              "IPY_MODEL_f9d44b6d92d0422da9517ad19966bac6",
              "IPY_MODEL_957db9cdc91d4f859e7c20308ba2d8ac"
            ],
            "layout": "IPY_MODEL_79c314ece5fa4b8f853c9b4520d24392"
          }
        },
        "5c553942a0954e0a95c4a97d885ed112": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c1ad6b223e5946c2a07aa1f9b2d67530",
            "placeholder": "​",
            "style": "IPY_MODEL_e5528c7f7a124ebfb1fb58409512d955",
            "value": "1_Pooling%2Fconfig.json: 100%"
          }
        },
        "f9d44b6d92d0422da9517ad19966bac6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_077ef9909cc04af7b0e6011d74db33ed",
            "max": 190,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f7e6ce002e6f401fab31d99112554b4b",
            "value": 190
          }
        },
        "957db9cdc91d4f859e7c20308ba2d8ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dc004049d4954161a48c6d5aa99a6fb2",
            "placeholder": "​",
            "style": "IPY_MODEL_599fe391bc8b410a9eb9ffb48c759708",
            "value": " 190/190 [00:00&lt;00:00, 14.6kB/s]"
          }
        },
        "79c314ece5fa4b8f853c9b4520d24392": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c1ad6b223e5946c2a07aa1f9b2d67530": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e5528c7f7a124ebfb1fb58409512d955": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "077ef9909cc04af7b0e6011d74db33ed": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f7e6ce002e6f401fab31d99112554b4b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "dc004049d4954161a48c6d5aa99a6fb2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "599fe391bc8b410a9eb9ffb48c759708": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "673cc64aa4694c56aa06d3ea3e29b36b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f25fc9b617814aa7b2602c7f54ca6cc8",
              "IPY_MODEL_f1b0bc7bcd6f4c968cb414e6131eda83",
              "IPY_MODEL_d6b14af18ea7496e833c4867a291a7ad"
            ],
            "layout": "IPY_MODEL_e4fb0eb401654505950a4ddaf68961c4"
          }
        },
        "f25fc9b617814aa7b2602c7f54ca6cc8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_494ec6f14412485cb7879a77dddaac41",
            "placeholder": "​",
            "style": "IPY_MODEL_aedc2d2135c34490a5f44c69d6e0479c",
            "value": "Batches: 100%"
          }
        },
        "f1b0bc7bcd6f4c968cb414e6131eda83": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6fbbe909f7f04a3b863ee139396b8b32",
            "max": 1405,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c5585051b9204460be2ac5f5b40e0f1a",
            "value": 1405
          }
        },
        "d6b14af18ea7496e833c4867a291a7ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2cb3d89c75684b19a8ad512fb9a8f357",
            "placeholder": "​",
            "style": "IPY_MODEL_282138f5cbca4dde93e8cda2c78bcb63",
            "value": " 1405/1405 [03:44&lt;00:00, 18.93it/s]"
          }
        },
        "e4fb0eb401654505950a4ddaf68961c4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "494ec6f14412485cb7879a77dddaac41": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aedc2d2135c34490a5f44c69d6e0479c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6fbbe909f7f04a3b863ee139396b8b32": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c5585051b9204460be2ac5f5b40e0f1a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2cb3d89c75684b19a8ad512fb9a8f357": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "282138f5cbca4dde93e8cda2c78bcb63": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3da675de37074ebcb32cb1e65b581b00": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_08f793230a1d4e9d85208cee7f1d8a4a",
              "IPY_MODEL_be1a6d60991f4d4984164cb1016c137b",
              "IPY_MODEL_377586755a424849b4ef9a846a59611d"
            ],
            "layout": "IPY_MODEL_6ebcb229a2e34cbbb7cc3fad4c4fdf51"
          }
        },
        "08f793230a1d4e9d85208cee7f1d8a4a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d6cb083ab8d547a3958b0639786b7a3e",
            "placeholder": "​",
            "style": "IPY_MODEL_df440e48aeb642fe82fa4edad66f8d85",
            "value": "Fetching 1 files: 100%"
          }
        },
        "be1a6d60991f4d4984164cb1016c137b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_33954e2986eb461badb674728f58cc84",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a17598e3c5c84932bb21c22864fbc122",
            "value": 1
          }
        },
        "377586755a424849b4ef9a846a59611d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_94f545a3d2b845d4827fa66d0599471d",
            "placeholder": "​",
            "style": "IPY_MODEL_f5f5ded240154d9394bfd34aecb6b9f2",
            "value": " 1/1 [00:00&lt;00:00,  1.79it/s]"
          }
        },
        "6ebcb229a2e34cbbb7cc3fad4c4fdf51": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d6cb083ab8d547a3958b0639786b7a3e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "df440e48aeb642fe82fa4edad66f8d85": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "33954e2986eb461badb674728f58cc84": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a17598e3c5c84932bb21c22864fbc122": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "94f545a3d2b845d4827fa66d0599471d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f5f5ded240154d9394bfd34aecb6b9f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a3a2bbe6a4774f61b67620c2ec13df8e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1c4559d365044f81a36eff6cd3424bdf",
              "IPY_MODEL_439baa10f9c448acb176735e2f0b9e1e",
              "IPY_MODEL_30f8e882d6434389bb1b8fa8f632ba35"
            ],
            "layout": "IPY_MODEL_3132967d837043ceb6d4a6416d394c60"
          }
        },
        "1c4559d365044f81a36eff6cd3424bdf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_30fd7ccff8414125b742096ecaa3cb05",
            "placeholder": "​",
            "style": "IPY_MODEL_682d10718cd54c41a1dec40feab8ec40",
            "value": "config.json: 100%"
          }
        },
        "439baa10f9c448acb176735e2f0b9e1e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5df98896f84d437c8fd45b9a80a41add",
            "max": 31,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6d4b242d31994847a666f6a41ca84985",
            "value": 31
          }
        },
        "30f8e882d6434389bb1b8fa8f632ba35": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4611bb8b57864781a3df5ad9aa2884ce",
            "placeholder": "​",
            "style": "IPY_MODEL_249e41084e11414191e50d15610c8d47",
            "value": " 31.0/31.0 [00:00&lt;00:00, 3.16kB/s]"
          }
        },
        "3132967d837043ceb6d4a6416d394c60": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "30fd7ccff8414125b742096ecaa3cb05": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "682d10718cd54c41a1dec40feab8ec40": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5df98896f84d437c8fd45b9a80a41add": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6d4b242d31994847a666f6a41ca84985": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4611bb8b57864781a3df5ad9aa2884ce": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "249e41084e11414191e50d15610c8d47": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e951e0be021f4a2e9e7e2d438709e58c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5c1b72bfd99845cb9d0cedf94399534b",
              "IPY_MODEL_5936169bef7549898ebc15a0b20183b5",
              "IPY_MODEL_9da12ab1ffce46d7a789f0d735bede69"
            ],
            "layout": "IPY_MODEL_0cd71b3ae40848448f979e247cf778b7"
          }
        },
        "5c1b72bfd99845cb9d0cedf94399534b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_90046946acf54424a8935522d5ef36c7",
            "placeholder": "​",
            "style": "IPY_MODEL_6c9022e72d374d44a2591cc93e489ecb",
            "value": "Fetching 1 files: 100%"
          }
        },
        "5936169bef7549898ebc15a0b20183b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c9d47c959a8548849785f82cf2478e74",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_497f466eb2684a6abeb312d988bf425d",
            "value": 1
          }
        },
        "9da12ab1ffce46d7a789f0d735bede69": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b23fdc5a998849b8a67d532a20754d75",
            "placeholder": "​",
            "style": "IPY_MODEL_04913e603a024bc09f5a5df18a64a13d",
            "value": " 1/1 [02:56&lt;00:00, 176.23s/it]"
          }
        },
        "0cd71b3ae40848448f979e247cf778b7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "90046946acf54424a8935522d5ef36c7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6c9022e72d374d44a2591cc93e489ecb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c9d47c959a8548849785f82cf2478e74": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "497f466eb2684a6abeb312d988bf425d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b23fdc5a998849b8a67d532a20754d75": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "04913e603a024bc09f5a5df18a64a13d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "15c50aded4f245a7b62316252108898c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5ed55225189946e38e8fe1b64db43c23",
              "IPY_MODEL_75ac977658b34657a553e705c32e0e13",
              "IPY_MODEL_9097ce966b134be1999444470d8b3837"
            ],
            "layout": "IPY_MODEL_11bc8be87783481985954badcad32b65"
          }
        },
        "5ed55225189946e38e8fe1b64db43c23": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1d4c6a41c0b349008440de522b6cfee9",
            "placeholder": "​",
            "style": "IPY_MODEL_70e72d35a82c436ea4d14d41ef0eea71",
            "value": "zephyr-7b-alpha.Q4_K_M.gguf: 100%"
          }
        },
        "75ac977658b34657a553e705c32e0e13": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ba3f9daf092e41f7a345337a30f90d48",
            "max": 4368438976,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5dad5845af4142bca8e1f80cd94af2c0",
            "value": 4368438976
          }
        },
        "9097ce966b134be1999444470d8b3837": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e944014f134b40948c31c2f2240d99b4",
            "placeholder": "​",
            "style": "IPY_MODEL_eab437ae93514c69920ebd24c2340064",
            "value": " 4.37G/4.37G [02:55&lt;00:00, 25.9MB/s]"
          }
        },
        "11bc8be87783481985954badcad32b65": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1d4c6a41c0b349008440de522b6cfee9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "70e72d35a82c436ea4d14d41ef0eea71": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ba3f9daf092e41f7a345337a30f90d48": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5dad5845af4142bca8e1f80cd94af2c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e944014f134b40948c31c2f2240d99b4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eab437ae93514c69920ebd24c2340064": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5df15fb0cec04d05b3f0e236665dc9f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c0c73aa484d94d3ba6ceee813c6f3f01",
              "IPY_MODEL_67e4497b7b9b4164a165b9a7dd1976b6",
              "IPY_MODEL_56c0956c8d924048904dfadd19cbcca0"
            ],
            "layout": "IPY_MODEL_461d65b4084b4437913e041a904bb5f8"
          }
        },
        "c0c73aa484d94d3ba6ceee813c6f3f01": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5fde867f24504125b07ec0aef4627ee2",
            "placeholder": "​",
            "style": "IPY_MODEL_3149001d1fe54f51883b2b58854256e7",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "67e4497b7b9b4164a165b9a7dd1976b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_eeb42725fd6547548ede1df7a64f2e27",
            "max": 1431,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_cf4e072472b543ff95e8f64bce404c01",
            "value": 1431
          }
        },
        "56c0956c8d924048904dfadd19cbcca0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_81c948d09de54610a1e6ceb85de17885",
            "placeholder": "​",
            "style": "IPY_MODEL_f5d39d5aaf7243bbbb419ee5e5038d8b",
            "value": " 1.43k/1.43k [00:00&lt;00:00, 113kB/s]"
          }
        },
        "461d65b4084b4437913e041a904bb5f8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5fde867f24504125b07ec0aef4627ee2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3149001d1fe54f51883b2b58854256e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "eeb42725fd6547548ede1df7a64f2e27": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cf4e072472b543ff95e8f64bce404c01": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "81c948d09de54610a1e6ceb85de17885": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f5d39d5aaf7243bbbb419ee5e5038d8b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "024f5e2506b44f19a2ab06a75614be2d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b423ac7985b24a9eb079bff5d5e3cfac",
              "IPY_MODEL_14e38e26b69545b49c195aa4d036605c",
              "IPY_MODEL_610955c453cb491ab49debe93e06bf60"
            ],
            "layout": "IPY_MODEL_3fc2ff3fb1634eb3be030a4234ff7ce2"
          }
        },
        "b423ac7985b24a9eb079bff5d5e3cfac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2781d8f6256440d5a4c423020b92c374",
            "placeholder": "​",
            "style": "IPY_MODEL_e7b7952c0c6a4680854a0d2c81737094",
            "value": "tokenizer.model: 100%"
          }
        },
        "14e38e26b69545b49c195aa4d036605c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e17c272c294b47a6bcb49045766a951b",
            "max": 493443,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_38ac05b10da741a4b0423aac4434a67a",
            "value": 493443
          }
        },
        "610955c453cb491ab49debe93e06bf60": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_90ea7fdfe1d84cc8a30c8210809cc8b3",
            "placeholder": "​",
            "style": "IPY_MODEL_73ccd49037104c83adb336b172b0d7ea",
            "value": " 493k/493k [00:00&lt;00:00, 15.8MB/s]"
          }
        },
        "3fc2ff3fb1634eb3be030a4234ff7ce2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2781d8f6256440d5a4c423020b92c374": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e7b7952c0c6a4680854a0d2c81737094": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e17c272c294b47a6bcb49045766a951b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "38ac05b10da741a4b0423aac4434a67a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "90ea7fdfe1d84cc8a30c8210809cc8b3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "73ccd49037104c83adb336b172b0d7ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e0c8f706f70142e6870c4499ee9b5bfa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d96a5d8d91aa476e8bdb4d59a7c7f328",
              "IPY_MODEL_8fc46b526da2471fb7e106af83a7d42b",
              "IPY_MODEL_ad1dc12621d340fab58d4b65b867b942"
            ],
            "layout": "IPY_MODEL_74084662c69b421a9046dc995aa6e1ac"
          }
        },
        "d96a5d8d91aa476e8bdb4d59a7c7f328": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6681db9a83ce492783d6a44944c9edfc",
            "placeholder": "​",
            "style": "IPY_MODEL_7d32a8d66d504a3aaa986f8057fa1caa",
            "value": "tokenizer.json: 100%"
          }
        },
        "8fc46b526da2471fb7e106af83a7d42b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bb01cdf8a8d1445ca6202e86b28fe1ce",
            "max": 1795303,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ee496ba2eecc4f699ba7b1873c2d7fde",
            "value": 1795303
          }
        },
        "ad1dc12621d340fab58d4b65b867b942": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6c1b53e895be4ea49735d4b9a89138f9",
            "placeholder": "​",
            "style": "IPY_MODEL_a248144bd5444c7c90496909f046b03e",
            "value": " 1.80M/1.80M [00:00&lt;00:00, 3.91MB/s]"
          }
        },
        "74084662c69b421a9046dc995aa6e1ac": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6681db9a83ce492783d6a44944c9edfc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7d32a8d66d504a3aaa986f8057fa1caa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bb01cdf8a8d1445ca6202e86b28fe1ce": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ee496ba2eecc4f699ba7b1873c2d7fde": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6c1b53e895be4ea49735d4b9a89138f9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a248144bd5444c7c90496909f046b03e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2f8c4838d26f4140a43438b0f421081c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0beacf615fd043b5a7f3420fcadde514",
              "IPY_MODEL_68ce2d81807c4bc885e941140110e9b5",
              "IPY_MODEL_80dd97d656fe437d81d0fb8e9d312a34"
            ],
            "layout": "IPY_MODEL_1638528905454fba8d0c62b9a87dda85"
          }
        },
        "0beacf615fd043b5a7f3420fcadde514": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9be4c601fe0848d88ae3398951aab830",
            "placeholder": "​",
            "style": "IPY_MODEL_f16df2a0504b4f62a735e04f4c2f1357",
            "value": "added_tokens.json: 100%"
          }
        },
        "68ce2d81807c4bc885e941140110e9b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8d1d9401722d41b48684c070c5463b31",
            "max": 42,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8a1587968d1b4c6692ba283bf581ef82",
            "value": 42
          }
        },
        "80dd97d656fe437d81d0fb8e9d312a34": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bcbf00253ab042dbb4ef4623c6d429ca",
            "placeholder": "​",
            "style": "IPY_MODEL_1f55370bf98442d3be4b96760fc7ef4a",
            "value": " 42.0/42.0 [00:00&lt;00:00, 3.88kB/s]"
          }
        },
        "1638528905454fba8d0c62b9a87dda85": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9be4c601fe0848d88ae3398951aab830": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f16df2a0504b4f62a735e04f4c2f1357": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8d1d9401722d41b48684c070c5463b31": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8a1587968d1b4c6692ba283bf581ef82": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "bcbf00253ab042dbb4ef4623c6d429ca": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1f55370bf98442d3be4b96760fc7ef4a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c141505bbd0d43e0bd665c199063737d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_96bd5899c6c14020816281026b5e0ded",
              "IPY_MODEL_9586c9f4c6dd48e68ad3f75203cd468f",
              "IPY_MODEL_315dbfaaa53f405785ea2418039d3704"
            ],
            "layout": "IPY_MODEL_ecf7493dbfb5479f83be65385fd409d3"
          }
        },
        "96bd5899c6c14020816281026b5e0ded": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_def996ec909343bb9566c990c428acb9",
            "placeholder": "​",
            "style": "IPY_MODEL_00f0a6d2768d45009909ef8d7cee7712",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "9586c9f4c6dd48e68ad3f75203cd468f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_220cfd4fa04e4ca1874f0de1e294c791",
            "max": 168,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8c1c335051d048dead16bd93e550fd6d",
            "value": 168
          }
        },
        "315dbfaaa53f405785ea2418039d3704": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e0ed9e626ac74446aa41eedd63d0494f",
            "placeholder": "​",
            "style": "IPY_MODEL_19b7996eaba74e29ab349d9d87eb1595",
            "value": " 168/168 [00:00&lt;00:00, 18.8kB/s]"
          }
        },
        "ecf7493dbfb5479f83be65385fd409d3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "def996ec909343bb9566c990c428acb9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "00f0a6d2768d45009909ef8d7cee7712": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "220cfd4fa04e4ca1874f0de1e294c791": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8c1c335051d048dead16bd93e550fd6d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e0ed9e626ac74446aa41eedd63d0494f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "19b7996eaba74e29ab349d9d87eb1595": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2be8c4184f8843ed85b728b88e2763a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9d386126a002492eb192fad62d14ddfa",
              "IPY_MODEL_f500fc4c56af47bb8172903e7f8ccb28",
              "IPY_MODEL_d1f3d04737274ee58fefbdab1fa02cc0"
            ],
            "layout": "IPY_MODEL_6999822b75404b2282673e6803ab020a"
          }
        },
        "9d386126a002492eb192fad62d14ddfa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_70cd847bab024cc6995e54f375c29ebe",
            "placeholder": "​",
            "style": "IPY_MODEL_f1f94ee51f9d49649335c73a0cb57b5e",
            "value": "config.json: 100%"
          }
        },
        "f500fc4c56af47bb8172903e7f8ccb28": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_77130c4ee6cb4080ae0d510515e5c066",
            "max": 1401,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3775f7b1db044b07bdaf3658580a763c",
            "value": 1401
          }
        },
        "d1f3d04737274ee58fefbdab1fa02cc0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_07c63f5873194197a3b59fb6e173fcde",
            "placeholder": "​",
            "style": "IPY_MODEL_7a0df5c7a500423c987e8242b0b993af",
            "value": " 1.40k/1.40k [00:00&lt;00:00, 29.9kB/s]"
          }
        },
        "6999822b75404b2282673e6803ab020a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "70cd847bab024cc6995e54f375c29ebe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f1f94ee51f9d49649335c73a0cb57b5e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "77130c4ee6cb4080ae0d510515e5c066": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3775f7b1db044b07bdaf3658580a763c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "07c63f5873194197a3b59fb6e173fcde": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7a0df5c7a500423c987e8242b0b993af": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4b79b7197502486dac3b080856eb6149": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c59ba5d4add44862b450d5eb9157ba09",
              "IPY_MODEL_e6f1569903d84ed8a0ac5d0c3253e975",
              "IPY_MODEL_a64b4a5abaf34094bd155a7a790a5663"
            ],
            "layout": "IPY_MODEL_384a883726fd4564957174bfc7cfee9c"
          }
        },
        "c59ba5d4add44862b450d5eb9157ba09": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3cfdd160330f415d93b5de3ef9d7e190",
            "placeholder": "​",
            "style": "IPY_MODEL_c9f81ee447834ee889e70d0a35d82f57",
            "value": "model.safetensors: 100%"
          }
        },
        "e6f1569903d84ed8a0ac5d0c3253e975": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ce648ed1356d471cbe837d7f9a5f04b1",
            "max": 307867048,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7e2a3de771ae4ca5bc93bf2d4bf65dd6",
            "value": 307867048
          }
        },
        "a64b4a5abaf34094bd155a7a790a5663": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f66072c1ae934ecdb5479ca6ca756511",
            "placeholder": "​",
            "style": "IPY_MODEL_0500277c739841bfa91a9d0172b3a533",
            "value": " 308M/308M [00:02&lt;00:00, 107MB/s]"
          }
        },
        "384a883726fd4564957174bfc7cfee9c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3cfdd160330f415d93b5de3ef9d7e190": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c9f81ee447834ee889e70d0a35d82f57": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ce648ed1356d471cbe837d7f9a5f04b1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7e2a3de771ae4ca5bc93bf2d4bf65dd6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f66072c1ae934ecdb5479ca6ca756511": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0500277c739841bfa91a9d0172b3a533": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "af4309cfc8e44353935dd5e97cf3d1b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e613f48e2e564731b1f13dec39bd249d",
              "IPY_MODEL_e8761f44640542cb862b8e786c2c0498",
              "IPY_MODEL_f024c6b3aff944d5ab50d08c4b12a476"
            ],
            "layout": "IPY_MODEL_c1dcb7acf18342c7adafc719cff992f8"
          }
        },
        "e613f48e2e564731b1f13dec39bd249d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_31d50a5aaf954ba8b485b0a1b3bfbee6",
            "placeholder": "​",
            "style": "IPY_MODEL_d000aafd059844a2b9ff5d0439991ed0",
            "value": "generation_config.json: 100%"
          }
        },
        "e8761f44640542cb862b8e786c2c0498": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cd386ad4705346e9801107c701d5a5c2",
            "max": 147,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7b8b7973730742289fcc1ddc97e86794",
            "value": 147
          }
        },
        "f024c6b3aff944d5ab50d08c4b12a476": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_310a911492c54d5bb122e5ca1a024607",
            "placeholder": "​",
            "style": "IPY_MODEL_83f37c1ef4954e2689c86e464fab9f15",
            "value": " 147/147 [00:00&lt;00:00, 3.50kB/s]"
          }
        },
        "c1dcb7acf18342c7adafc719cff992f8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "31d50a5aaf954ba8b485b0a1b3bfbee6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d000aafd059844a2b9ff5d0439991ed0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cd386ad4705346e9801107c701d5a5c2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7b8b7973730742289fcc1ddc97e86794": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "310a911492c54d5bb122e5ca1a024607": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "83f37c1ef4954e2689c86e464fab9f15": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7bcaadea7a7a42daaa32cf3f75db7f0d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e9c1f89d58004db48b36c314832fd09c",
              "IPY_MODEL_89e01fb7aed9460f967816e72856bff4",
              "IPY_MODEL_43c0cd87944d43d88acb6cc5bde32c15"
            ],
            "layout": "IPY_MODEL_f3abd6dfc34141389a8e1b34f657ecc1"
          }
        },
        "e9c1f89d58004db48b36c314832fd09c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_39aaece502344d82b513ebefa57a65c9",
            "placeholder": "​",
            "style": "IPY_MODEL_ff9fddc95e584c7baded77215a3901c4",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "89e01fb7aed9460f967816e72856bff4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3557d8d7eb2a4064a63b523c31cd39ca",
            "max": 2539,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_70c02e57e1354c0c8dc76eb9acb807c1",
            "value": 2539
          }
        },
        "43c0cd87944d43d88acb6cc5bde32c15": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ad36570582de4b24b62e43d25a263730",
            "placeholder": "​",
            "style": "IPY_MODEL_10895bc78b874cbf81a109e8b0969ff9",
            "value": " 2.54k/2.54k [00:00&lt;00:00, 87.7kB/s]"
          }
        },
        "f3abd6dfc34141389a8e1b34f657ecc1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "39aaece502344d82b513ebefa57a65c9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ff9fddc95e584c7baded77215a3901c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3557d8d7eb2a4064a63b523c31cd39ca": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "70c02e57e1354c0c8dc76eb9acb807c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ad36570582de4b24b62e43d25a263730": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "10895bc78b874cbf81a109e8b0969ff9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "44c82d2752f54352a0195b765fd1eaeb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c3b435f1fd0f485482d70715c4e380a3",
              "IPY_MODEL_7898e0d34754490693d5b203ecca80b7",
              "IPY_MODEL_ab25396f47a14e85a52dcf28819ddf4e"
            ],
            "layout": "IPY_MODEL_5b33767d6a2b4fbcab4b1cd4f1a0d7e3"
          }
        },
        "c3b435f1fd0f485482d70715c4e380a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9d96002987354bd1b106addcf118d8d5",
            "placeholder": "​",
            "style": "IPY_MODEL_cee9f25416b24ae4aa24d16c98ab6148",
            "value": "spiece.model: 100%"
          }
        },
        "7898e0d34754490693d5b203ecca80b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_65dffb18535945fb96b587a2bc86bb1c",
            "max": 791656,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_562772e91d204f7c8fc0057f53bf7758",
            "value": 791656
          }
        },
        "ab25396f47a14e85a52dcf28819ddf4e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c0181ef6f45149b396b12b76b87f8fda",
            "placeholder": "​",
            "style": "IPY_MODEL_9ba1ef5dbc714dbb834dfd2dda6b7467",
            "value": " 792k/792k [00:00&lt;00:00, 21.2MB/s]"
          }
        },
        "5b33767d6a2b4fbcab4b1cd4f1a0d7e3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9d96002987354bd1b106addcf118d8d5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cee9f25416b24ae4aa24d16c98ab6148": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "65dffb18535945fb96b587a2bc86bb1c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "562772e91d204f7c8fc0057f53bf7758": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c0181ef6f45149b396b12b76b87f8fda": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9ba1ef5dbc714dbb834dfd2dda6b7467": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ff1358f1374940afbe9c849d37c3d758": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_aec44cd4887c4f81b0b99df0b5c51f0a",
              "IPY_MODEL_5cdef7aee3de4f328e7eb9d179eaf78c",
              "IPY_MODEL_d9734653376b46cb9fa8fad0657d487d"
            ],
            "layout": "IPY_MODEL_e64105093aa24e44a50ef0417febc926"
          }
        },
        "aec44cd4887c4f81b0b99df0b5c51f0a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b4858d5d1ade4b26b3e9943ed114b116",
            "placeholder": "​",
            "style": "IPY_MODEL_a1645d0254de4f12bb546e951143c57e",
            "value": "tokenizer.json: 100%"
          }
        },
        "5cdef7aee3de4f328e7eb9d179eaf78c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f161fc5bbaf04999a06754458e6b922d",
            "max": 2424064,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e3c753b418bc426d83875b2243e8e21f",
            "value": 2424064
          }
        },
        "d9734653376b46cb9fa8fad0657d487d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bae15804491c46fab31295fc09ce403b",
            "placeholder": "​",
            "style": "IPY_MODEL_1942969ddfd7483e95f68a41287f5d53",
            "value": " 2.42M/2.42M [00:01&lt;00:00, 1.63MB/s]"
          }
        },
        "e64105093aa24e44a50ef0417febc926": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b4858d5d1ade4b26b3e9943ed114b116": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a1645d0254de4f12bb546e951143c57e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f161fc5bbaf04999a06754458e6b922d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e3c753b418bc426d83875b2243e8e21f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "bae15804491c46fab31295fc09ce403b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1942969ddfd7483e95f68a41287f5d53": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9ef3078ef2864c109b277b9c1e5eb1a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5c4ba008e6ea4696bc3b87cb4aa88702",
              "IPY_MODEL_0f160dd4c1c04166ab90b7ca8eb39872",
              "IPY_MODEL_2897ec4e9a334649b0611e5a53d5b5d4"
            ],
            "layout": "IPY_MODEL_344a8edb435d4fc899ca03ac924deed7"
          }
        },
        "5c4ba008e6ea4696bc3b87cb4aa88702": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ed924ec452cb48739c95a0aa95d21b85",
            "placeholder": "​",
            "style": "IPY_MODEL_aadee93982bf4854bdcf2c4bf90d429e",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "0f160dd4c1c04166ab90b7ca8eb39872": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b9f6113d323a414ab2f2ab9c4cd1831b",
            "max": 2201,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d7d3a18d2a5f43aab9dbc7710c5bc53f",
            "value": 2201
          }
        },
        "2897ec4e9a334649b0611e5a53d5b5d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b676ca0968644331bba4077f6cd72e28",
            "placeholder": "​",
            "style": "IPY_MODEL_ade3e7300bb642118251b4d24fd7e8f6",
            "value": " 2.20k/2.20k [00:00&lt;00:00, 52.6kB/s]"
          }
        },
        "344a8edb435d4fc899ca03ac924deed7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ed924ec452cb48739c95a0aa95d21b85": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aadee93982bf4854bdcf2c4bf90d429e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b9f6113d323a414ab2f2ab9c4cd1831b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d7d3a18d2a5f43aab9dbc7710c5bc53f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b676ca0968644331bba4077f6cd72e28": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ade3e7300bb642118251b4d24fd7e8f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "de28307598e149deb6652e8682f8f244": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cf316b8d0b9d472eaabbfaecde9088ce",
              "IPY_MODEL_6faaa257d1ed4aea981d986606851a06",
              "IPY_MODEL_e11458e6d5e8499eae13b94bea484f0d"
            ],
            "layout": "IPY_MODEL_dbc492974f8e47a9af028a13bfc482fe"
          }
        },
        "cf316b8d0b9d472eaabbfaecde9088ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c0d543b508844380873eafe3d8d26522",
            "placeholder": "​",
            "style": "IPY_MODEL_c70e28971c4346bd87e9ef93fc7ecd76",
            "value": "Batches: 100%"
          }
        },
        "6faaa257d1ed4aea981d986606851a06": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c2d3ee339cef4b2ebfc6e1063282a6a6",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c19f4067b6ee4084a1ffa7d207563e31",
            "value": 1
          }
        },
        "e11458e6d5e8499eae13b94bea484f0d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9442aa3fc9884616998f68262bacff27",
            "placeholder": "​",
            "style": "IPY_MODEL_a9ec4250a2ac43dba1a8a27033e1a40f",
            "value": " 1/1 [00:00&lt;00:00, 16.04it/s]"
          }
        },
        "dbc492974f8e47a9af028a13bfc482fe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c0d543b508844380873eafe3d8d26522": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c70e28971c4346bd87e9ef93fc7ecd76": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c2d3ee339cef4b2ebfc6e1063282a6a6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c19f4067b6ee4084a1ffa7d207563e31": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9442aa3fc9884616998f68262bacff27": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a9ec4250a2ac43dba1a8a27033e1a40f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "74e6383d59624698905bf858efaacbc5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f823601892d1474bbbce4c0ce0a32760",
              "IPY_MODEL_d27bdc9783e74d5cac82c346bbc4eaa4",
              "IPY_MODEL_67ec1bdd9fa746b592b3b59eb9f6fbf6"
            ],
            "layout": "IPY_MODEL_395f7e1356004d3f9a1e84581db7ee5f"
          }
        },
        "f823601892d1474bbbce4c0ce0a32760": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_81154e2d4f4b406fbf7f7ee643b639ca",
            "placeholder": "​",
            "style": "IPY_MODEL_26363cbfb1b0437b9476949f5818bc58",
            "value": "Batches: 100%"
          }
        },
        "d27bdc9783e74d5cac82c346bbc4eaa4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fef046e5fd094098ab6519516166872e",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fbddce631f824743876d352e6654ec43",
            "value": 1
          }
        },
        "67ec1bdd9fa746b592b3b59eb9f6fbf6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1c5a346f327644f98631295fc954e019",
            "placeholder": "​",
            "style": "IPY_MODEL_75eafad6463a403a84241541836903fa",
            "value": " 1/1 [00:00&lt;00:00, 16.57it/s]"
          }
        },
        "395f7e1356004d3f9a1e84581db7ee5f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "81154e2d4f4b406fbf7f7ee643b639ca": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "26363cbfb1b0437b9476949f5818bc58": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fef046e5fd094098ab6519516166872e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fbddce631f824743876d352e6654ec43": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1c5a346f327644f98631295fc954e019": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "75eafad6463a403a84241541836903fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ad5503e2d7384a08887de7dd5cce3aac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ec9b90e1ff7e4f49b12d4aaeccdf5268",
              "IPY_MODEL_d30b485abd4d4e62af64ff1b18b968ad",
              "IPY_MODEL_7b48154ae5fe44daa98d49fd6e8f62c8"
            ],
            "layout": "IPY_MODEL_c3b40ae835d44b89979d6dc682aee2fc"
          }
        },
        "ec9b90e1ff7e4f49b12d4aaeccdf5268": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_534b03f360b449edbcf024f84b2c8ef6",
            "placeholder": "​",
            "style": "IPY_MODEL_a35130645bd94e28bde198404369b65c",
            "value": "Batches: 100%"
          }
        },
        "d30b485abd4d4e62af64ff1b18b968ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fb24bd4f0f9b424c81cbf9459f5ff360",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2fdd6b82469b47ddb636446c74ee407a",
            "value": 1
          }
        },
        "7b48154ae5fe44daa98d49fd6e8f62c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c9a1734c8b514ffe968758052e80f081",
            "placeholder": "​",
            "style": "IPY_MODEL_2db4ef61953a46ae9f74c405e89e7a0f",
            "value": " 1/1 [00:00&lt;00:00, 29.48it/s]"
          }
        },
        "c3b40ae835d44b89979d6dc682aee2fc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "534b03f360b449edbcf024f84b2c8ef6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a35130645bd94e28bde198404369b65c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fb24bd4f0f9b424c81cbf9459f5ff360": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2fdd6b82469b47ddb636446c74ee407a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c9a1734c8b514ffe968758052e80f081": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2db4ef61953a46ae9f74c405e89e7a0f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ThisIsFarhan/DocumentSorter-BERTopic/blob/main/Doc_Sorter.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yBb3w3M7rcXE",
        "outputId": "c6c84b6c-cd76-4e8f-c949-0f0e911d3218"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/huggingface/transformers\n",
            "  Cloning https://github.com/huggingface/transformers to /tmp/pip-req-build-2vnx38v1\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers /tmp/pip-req-build-2vnx38v1\n",
            "  Resolved https://github.com/huggingface/transformers to commit dcbdf7e962c4b36140cc9ee76f870016121e69e5\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting bertopic\n",
            "  Downloading bertopic-0.16.4-py3-none-any.whl.metadata (23 kB)\n",
            "Collecting datasets\n",
            "  Downloading datasets-3.3.2-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting ctransformers[cuda]\n",
            "  Downloading ctransformers-0.2.27-py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: hdbscan>=0.8.29 in /usr/local/lib/python3.11/dist-packages (from bertopic) (0.8.40)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.11/dist-packages (from bertopic) (1.26.4)\n",
            "Requirement already satisfied: pandas>=1.1.5 in /usr/local/lib/python3.11/dist-packages (from bertopic) (2.2.2)\n",
            "Requirement already satisfied: plotly>=4.7.0 in /usr/local/lib/python3.11/dist-packages (from bertopic) (5.24.1)\n",
            "Requirement already satisfied: scikit-learn>=0.22.2.post1 in /usr/local/lib/python3.11/dist-packages (from bertopic) (1.6.1)\n",
            "Requirement already satisfied: sentence-transformers>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from bertopic) (3.4.1)\n",
            "Requirement already satisfied: tqdm>=4.41.1 in /usr/local/lib/python3.11/dist-packages (from bertopic) (4.67.1)\n",
            "Requirement already satisfied: umap-learn>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from bertopic) (0.5.7)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.17.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.10.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.13)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.28.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: py-cpuinfo<10.0.0,>=9.0.0 in /usr/local/lib/python3.11/dist-packages (from ctransformers[cuda]) (9.0.0)\n",
            "Requirement already satisfied: nvidia-cublas-cu12 in /usr/local/lib/python3.11/dist-packages (from ctransformers[cuda]) (12.5.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12 in /usr/local/lib/python3.11/dist-packages (from ctransformers[cuda]) (12.5.82)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.50.0.dev0) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers==4.50.0.dev0) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.50.0.dev0) (0.5.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.11/dist-packages (from hdbscan>=0.8.29->bertopic) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.11/dist-packages (from hdbscan>=0.8.29->bertopic) (1.4.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.5->bertopic) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.5->bertopic) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.5->bertopic) (2025.1)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly>=4.7.0->bertopic) (9.0.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.22.2.post1->bertopic) (3.5.0)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=0.4.1->bertopic) (2.5.1+cu124)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=0.4.1->bertopic) (11.1.0)\n",
            "Requirement already satisfied: numba>=0.51.2 in /usr/local/lib/python3.11/dist-packages (from umap-learn>=0.5.0->bertopic) (0.61.0)\n",
            "Requirement already satisfied: pynndescent>=0.5 in /usr/local/lib/python3.11/dist-packages (from umap-learn>=0.5.0->bertopic) (0.5.13)\n",
            "Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.51.2->umap-learn>=0.5.0->bertopic) (0.44.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.1.5->bertopic) (1.17.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (3.1.5)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12 (from ctransformers[cuda])\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12 (from ctransformers[cuda])\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (3.0.2)\n",
            "Downloading bertopic-0.16.4-py3-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.7/143.7 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading datasets-3.3.2-py3-none-any.whl (485 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m485.4/485.4 kB\u001b[0m \u001b[31m41.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ctransformers-0.2.27-py3-none-any.whl (9.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m109.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m53.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m116.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m86.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m84.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: transformers\n",
            "  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for transformers: filename=transformers-4.50.0.dev0-py3-none-any.whl size=10860621 sha256=fab3d47d9ce0514c56b90ba42cd817665f9c54cd5b74c4a4e3ee74e5cd83012a\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-j5543hru/wheels/04/a3/f1/b88775f8e1665827525b19ac7590250f1038d947067beba9fb\n",
            "Successfully built transformers\n",
            "Installing collected packages: xxhash, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, dill, nvidia-cusparse-cu12, nvidia-cudnn-cu12, multiprocess, nvidia-cusolver-cu12, ctransformers, transformers, datasets, bertopic\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.48.3\n",
            "    Uninstalling transformers-4.48.3:\n",
            "      Successfully uninstalled transformers-4.48.3\n",
            "Successfully installed bertopic-0.16.4 ctransformers-0.2.27 datasets-3.3.2 dill-0.3.8 multiprocess-0.70.16 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 transformers-4.50.0.dev0 xxhash-3.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install bertopic datasets ctransformers[cuda] --upgrade git+https://github.com/huggingface/transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from umap import UMAP\n",
        "from hdbscan import HDBSCAN\n",
        "from ctransformers import AutoModelForCausalLM\n",
        "from transformers import AutoTokenizer, pipeline\n",
        "from bertopic.representation import TextGeneration\n",
        "from bertopic import BERTopic"
      ],
      "metadata": {
        "id": "9SPzMjdNrhNb"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(\"maartengr/arxiv_nlp\")[\"train\"]\n",
        "abstracts = dataset[\"Abstracts\"]\n",
        "abstracts"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "88472ad664964356ab6f3a9ede90bea5",
            "f3b779fcbad74e6e84d2192289d24867",
            "530d6fc7f35b4f1882f886a97ebae796",
            "bed76d22f8704e428469a41f97818552",
            "5302534e91fb423a8f7214bff1fa36ec",
            "800779ab2d70466ab56e8da21c9ee2a7",
            "24e5b25d702245c6a4aaca6449acd88b",
            "0d2ed423b4d74eb58af37e31e85f9363",
            "70d89014814c458bb2537e1131241f9c",
            "f419195f5a974d71b3718d928749b2c2",
            "9ddd22467a194a0786828e5870993ecd",
            "463cc08fc5294d688cd168b12b6ddf0a",
            "2f17446206404f2f8e7b3519b9e2bf62",
            "0ff5d6bb025f4325adf496e5602bc6fd",
            "b7f5efc95edb43e9b422128b53e415c4",
            "a4149c4cea964a0bb2ca31e4ceca409c",
            "3a4c12b50fd641389c8a4ba62131331e",
            "365e1c09a9194dfa835c3421963f969f",
            "99033e3e1e4c4519bb2e7fb9abd1f697",
            "6c0fc56e6f4a4ecd80f5be2c886f529b",
            "1adc4b7a4de64eba87f504ebc29fe43f",
            "b6c49459a46a49c48f77e65039652df6",
            "201c60e79e1a4a648b165f8df3862e57",
            "d42a2f44aa0843939fd6619cf92c633e",
            "fcbcafa66ee7420e9f65d23e3c665d7f",
            "815a537df7ca42939eca011900429a96",
            "e38adf060d1a4f0ca775c798fe562949",
            "2112780fd42c483580e2190ac4a21228",
            "67d1693fe0254ae0b194443e388685b6",
            "e6ac802f8ba34e6ea56411a8cf52e37f",
            "219371f0243a4caeb7e251a439fe53c0",
            "0e04be6390494a329aa5b614bed2e70f",
            "b065da6ad63b43bf9dbcfe45e6390fbe"
          ]
        },
        "id": "xmUbvFNosHlT",
        "outputId": "a2c6f1ed-6c5b-4d1c-f63a-1e8ec9abaf64"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md:   0%|          | 0.00/617 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "88472ad664964356ab6f3a9ede90bea5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "data.csv:   0%|          | 0.00/53.2M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "463cc08fc5294d688cd168b12b6ddf0a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split: 0 examples [00:00, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "201c60e79e1a4a648b165f8df3862e57"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['  In this paper Arabic was investigated from the speech recognition problem\\npoint of view. We propose a novel approach to build an Arabic Automated Speech\\nRecognition System (ASR). This system is based on the open source CMU Sphinx-4,\\nfrom the Carnegie Mellon University. CMU Sphinx is a large-vocabulary;\\nspeaker-independent, continuous speech recognition system based on discrete\\nHidden Markov Models (HMMs). We build a model using utilities from the\\nOpenSource CMU Sphinx. We will demonstrate the possible adaptability of this\\nsystem to Arabic voice recognition.\\n',\n",
              " '  In this paper we present the creation of an Arabic version of Automated\\nSpeech Recognition System (ASR). This system is based on the open source\\nSphinx-4, from the Carnegie Mellon University. Which is a speech recognition\\nsystem based on discrete hidden Markov models (HMMs). We investigate the\\nchanges that must be made to the model to adapt Arabic voice recognition.\\n  Keywords: Speech recognition, Acoustic model, Arabic language, HMMs,\\nCMUSphinx-4, Artificial intelligence.\\n',\n",
              " '  Intelligent Input Methods (IM) are essential for making text entries in many\\nEast Asian scripts, but their application to other languages has not been fully\\nexplored. This paper discusses how such tools can contribute to the development\\nof computer processing of other oriental languages. We propose a design\\nphilosophy that regards IM as a text service platform, and treats the study of\\nIM as a cross disciplinary subject from the perspectives of software\\nengineering, human-computer interaction (HCI), and natural language processing\\n(NLP). We discuss these three perspectives and indicate a number of possible\\nfuture research directions.\\n',\n",
              " '  This paper includes a reflection on the role of networks in the study of\\nEnglish language acquisition, as well as a collection of practical criteria to\\nannotate free-speech corpora from children utterances. At the theoretical\\nlevel, the main claim of this paper is that syntactic networks should be\\ninterpreted as the outcome of the use of the syntactic machinery. Thus, the\\nintrinsic features of such machinery are not accessible directly from (known)\\nnetwork properties. Rather, what one can see are the global patterns of its use\\nand, thus, a global view of the power and organization of the underlying\\ngrammar. Taking a look into more practical issues, the paper examines how to\\nbuild a net from the projection of syntactic relations. Recall that, as opposed\\nto adult grammars, early-child language has not a well-defined concept of\\nstructure. To overcome such difficulty, we develop a set of systematic criteria\\nassuming constituency hierarchy and a grammar based on lexico-thematic\\nrelations. At the end, what we obtain is a well defined corpora annotation that\\nenables us i) to perform statistics on the size of structures and ii) to build\\na network from syntactic relations over which we can perform the standard\\nmeasures of complexity. We also provide a detailed example.\\n',\n",
              " '  We test a segmentation algorithm, based on the calculation of the\\nJensen-Shannon divergence between probability distributions, to two symbolic\\nsequences of literary and musical origin. The first sequence represents the\\nsuccessive appearance of characters in a theatrical play, and the second\\nrepresents the succession of tones from the twelve-tone scale in a keyboard\\nsonata. The algorithm divides the sequences into segments of maximal\\ncompositional divergence between them. For the play, these segments are related\\nto changes in the frequency of appearance of different characters and in the\\ngeographical setting of the action. For the sonata, the segments correspond to\\ntonal domains and reveal in detail the characteristic tonal progression of such\\nkind of musical composition.\\n',\n",
              " '  This paper describes the Linguistic Annotation Framework under development\\nwithin ISO TC37 SC4 WG1. The Linguistic Annotation Framework is intended to\\nserve as a basis for harmonizing existing language resources as well as\\ndeveloping new ones.\\n',\n",
              " '  We show that a general model of lexical information conforms to an abstract\\nmodel that reflects the hierarchy of information found in a typical dictionary\\nentry. We show that this model can be mapped into a well-formed XML document,\\nand how the XSL transformation language can be used to implement a semantics\\ndefined over the abstract model to enable extraction and manipulation of the\\ninformation in any format.\\n',\n",
              " '  This research hypothesized that a practical approach in the form of a\\nsolution framework known as Natural Language Understanding and Reasoning for\\nIntelligence (NaLURI), which combines full-discourse natural language\\nunderstanding, powerful representation formalism capable of exploiting\\nontological information and reasoning approach with advanced features, will\\nsolve the following problems without compromising practicality factors: 1)\\nrestriction on the nature of question and response, and 2) limitation to scale\\nacross domains and to real-life natural language text.\\n',\n",
              " '  This dissertation presents several new methods of supervised and unsupervised\\nlearning of word sense disambiguation models. The supervised methods focus on\\nperforming model searches through a space of probabilistic models, and the\\nunsupervised methods rely on the use of Gibbs Sampling and the Expectation\\nMaximization (EM) algorithm. In both the supervised and unsupervised case, the\\nNaive Bayesian model is found to perform well. An explanation for this success\\nis presented in terms of learning rates and bias-variance decompositions.\\n',\n",
              " '  This paper describes experiments on learning Dutch phonotactic rules using\\nInductive Logic Programming, a machine learning discipline based on inductive\\nlogical operators. Two different ways of approaching the problem are\\nexperimented with, and compared against each other as well as with related work\\non the task. The results show a direct correspondence between the quality and\\ninformedness of the background knowledge and the constructed theory,\\ndemonstrating the ability of ILP to take good advantage of the prior domain\\nknowledge available. Further research is outlined.\\n',\n",
              " \"  We propose a range of deep lexical acquisition methods which make use of\\nmorphological, syntactic and ontological language resources to model word\\nsimilarity and bootstrap from a seed lexicon. The different methods are\\ndeployed in learning lexical items for a precision grammar, and shown to each\\nhave strengths and weaknesses over different word classes. A particular focus\\nof this paper is the relative accessibility of different language resource\\ntypes, and predicted ``bang for the buck'' associated with each in deep lexical\\nacquisition applications.\\n\",\n",
              " '  We examine an evolutionary naming-game model where communicating agents are\\nequipped with an evolutionarily selected learning ability. Such a coupling of\\nbiological and linguistic ingredients results in an abrupt transition: upon a\\nsmall change of a model control parameter a poorly communicating group of\\nlinguistically unskilled agents transforms into almost perfectly communicating\\ngroup with large learning abilities. When learning ability is kept fixed, the\\ntransition appears to be continuous. Genetic imprinting of the learning\\nabilities proceeds via Baldwin effect: initially unskilled communicating agents\\nlearn a language and that creates a niche in which there is an evolutionary\\npressure for the increase of learning ability.Our model suggests that when\\nlinguistic (or cultural) processes became intensive enough, a transition took\\nplace where both linguistic performance and biological endowment of our species\\nexperienced an abrupt change that perhaps triggered the rapid expansion of\\nhuman civilization.\\n',\n",
              " \"  Zipf's law states that if words of language are ranked in the order of\\ndecreasing frequency in texts, the frequency of a word is inversely\\nproportional to its rank. It is very robust as an experimental observation, but\\nto date it escaped satisfactory theoretical explanation. We suggest that Zipf's\\nlaw may arise from the evolution of word semantics dominated by expansion of\\nmeanings and competition of synonyms.\\n\",\n",
              " '  The task of finding a criterion allowing to distinguish a text from an\\narbitrary set of words is rather relevant in itself, for instance, in the\\naspect of development of means for internet-content indexing or separating\\nsignals and noise in communication channels. The Zipf law is currently\\nconsidered to be the most reliable criterion of this kind [3]. At any rate,\\nconventional stochastic word sets do not meet this law. The present paper deals\\nwith one of possible criteria based on the determination of the degree of data\\ncompression.\\n',\n",
              " \"  In the task of information retrieval the term relevance is taken to mean\\nformal conformity of a document given by the retrieval system to user's\\ninformation query. As a rule, the documents found by the retrieval system\\nshould be submitted to the user in a certain order. Therefore, a retrieval\\nperceived as a selection of documents formally solving the user's query, should\\nbe supplemented with a certain procedure of processing a relevant set. It would\\nbe natural to introduce a quantitative measure of document conformity to query,\\ni.e. the relevance measure. Since no single rule exists for the determination\\nof the relevance measure, we shall consider two of them which are the simplest\\nin our opinion. The proposed approach does not suppose any restrictions and can\\nbe applied to other relevance measures.\\n\",\n",
              " '  This paper describes experiments on identifying the language of a single name\\nin isolation or in a document written in a different language. A new corpus has\\nbeen compiled and made available, matching names against languages. This corpus\\nis used in a series of experiments measuring the performance of general\\nlanguage models and names-only language models on the language identification\\ntask. Conclusions are drawn from the comparison between using general language\\nmodels and names-only language models and between identifying the language of\\nisolated names and the language of very short document fragments. Future\\nresearch directions are outlined.\\n',\n",
              " '  Data mining allows the exploration of sequences of phenomena, whereas one\\nusually tends to focus on isolated phenomena or on the relation between two\\nphenomena. It offers invaluable tools for theoretical analyses and exploration\\nof the structure of sentences, texts, dialogues, and speech. We report here the\\nresults of an attempt at using it for inspecting sequences of verbs from French\\naccounts of road accidents. This analysis comes from an original approach of\\nunsupervised training allowing the discovery of the structure of sequential\\ndata. The entries of the analyzer were only made of the verbs appearing in the\\nsentences. It provided a classification of the links between two successive\\nverbs into four distinct clusters, allowing thus text segmentation. We give\\nhere an interpretation of these clusters by applying a statistical analysis to\\nindependent semantic annotations.\\n',\n",
              " '  In this treatment a text is considered to be a series of word impulses which\\nare read at a constant rate. The brain then assembles these units of\\ninformation into higher units of meaning. A classical systems approach is used\\nto model an initial part of this assembly process. The concepts of linguistic\\nsystem response, information energy, and ordering energy are defined and\\nanalyzed. Finally, as a demonstration, information energy is used to estimate\\nthe publication dates of a series of texts and the similarity of a set of\\ntexts.\\n',\n",
              " \"  We discuss the use of model building for temporal representations. We chose\\nPolish to illustrate our discussion because it has an interesting aspectual\\nsystem, but the points we wish to make are not language specific. Rather, our\\ngoal is to develop theoretical and computational tools for temporal model\\nbuilding tasks in computational semantics. To this end, we present a\\nfirst-order theory of time and events which is rich enough to capture\\ninteresting semantic distinctions, and an algorithm which takes minimal models\\nfor first-order theories and systematically attempts to ``perturb'' their\\ntemporal component to provide non-minimal, but semantically significant,\\nmodels.\\n\",\n",
              " '  The aim of this paper is to show how we can handle the Recognising Textual\\nEntailment (RTE) task by using Description Logics (DLs). To do this, we propose\\na representation of natural language semantics in DLs inspired by existing\\nrepresentations in first-order logic. But our most significant contribution is\\nthe definition of two novel inference tasks: A-Box saturation and subgraph\\ndetection which are crucial for our approach to RTE.\\n',\n",
              " '  In this paper we present a fresh look at the problem of summarizing evolving\\nevents from multiple sources. After a discussion concerning the nature of\\nevolving events we introduce a distinction between linearly and non-linearly\\nevolving events. We present then a general methodology for the automatic\\ncreation of summaries from evolving events. At its heart lie the notions of\\nSynchronic and Diachronic cross-document Relations (SDRs), whose aim is the\\nidentification of similarities and differences between sources, from a\\nsynchronical and diachronical perspective. SDRs do not connect documents or\\ntextual elements found therein, but structures one might call messages.\\nApplying this methodology will yield a set of messages and relations, SDRs,\\nconnecting them, that is a graph which we call grid. We will show how such a\\ngrid can be considered as the starting point of a Natural Language Generation\\nSystem. The methodology is evaluated in two case-studies, one for linearly\\nevolving events (descriptions of football matches) and another one for\\nnon-linearly evolving events (terrorist incidents involving hostages). In both\\ncases we evaluate the results produced by our computational systems.\\n',\n",
              " '  Despite its importance, the task of summarizing evolving events has received\\nsmall attention by researchers in the field of multi-document summariztion. In\\na previous paper (Afantenos et al. 2007) we have presented a methodology for\\nthe automatic summarization of documents, emitted by multiple sources, which\\ndescribe the evolution of an event. At the heart of this methodology lies the\\nidentification of similarities and differences between the various documents,\\nin two axes: the synchronic and the diachronic. This is achieved by the\\nintroduction of the notion of Synchronic and Diachronic Relations. Those\\nrelations connect the messages that are found in the documents, resulting thus\\nin a graph which we call grid. Although the creation of the grid completes the\\nDocument Planning phase of a typical NLG architecture, it can be the case that\\nthe number of messages contained in a grid is very large, exceeding thus the\\nrequired compression rate. In this paper we provide some initial thoughts on a\\nprobabilistic model which can be applied at the Content Determination stage,\\nand which tries to alleviate this problem.\\n',\n",
              " \"  In this paper we present an automated method for the classification of the\\norigin of non-native speakers. The origin of non-native speakers could be\\nidentified by a human listener based on the detection of typical pronunciations\\nfor each nationality. Thus we suppose the existence of several phoneme\\nsequences that might allow the classification of the origin of non-native\\nspeakers. Our new method is based on the extraction of discriminative sequences\\nof phonemes from a non-native English speech database. These sequences are used\\nto construct a probabilistic classifier for the speakers' origin. The existence\\nof discriminative phone sequences in non-native speech is a significant result\\nof this work. The system that we have developed achieved a significant correct\\nclassification rate of 96.3% and a significant error reduction compared to some\\nother tested techniques.\\n\",\n",
              " \"  In this paper, we present several adaptation methods for non-native speech\\nrecognition. We have tested pronunciation modelling, MLLR and MAP non-native\\npronunciation adaptation and HMM models retraining on the HIWIRE foreign\\naccented English speech database. The ``phonetic confusion'' scheme we have\\ndeveloped consists in associating to each spoken phone several sequences of\\nconfused phones. In our experiments, we have used different combinations of\\nacoustic models representing the canonical and the foreign pronunciations:\\nspoken and native models, models adapted to the non-native accent with MAP and\\nMLLR. The joint use of pronunciation modelling and acoustic adaptation led to\\nfurther improvements in recognition accuracy. The best combination of the above\\nmentioned techniques resulted in a relative word error reduction ranging from\\n46% to 71%.\\n\",\n",
              " '  In this article, we present an approach for non native automatic speech\\nrecognition (ASR). We propose two methods to adapt existing ASR systems to the\\nnon-native accents. The first method is based on the modification of acoustic\\nmodels through integration of acoustic models from the mother tong. The\\nphonemes of the target language are pronounced in a similar manner to the\\nnative language of speakers. We propose to combine the models of confused\\nphonemes so that the ASR system could recognize both concurrent\\npronounciations. The second method we propose is a refinment of the\\npronounciation error detection through the introduction of graphemic\\nconstraints. Indeed, non native speakers may rely on the writing of words in\\ntheir uttering. Thus, the pronounctiation errors might depend on the characters\\ncomposing the words. The average error rate reduction that we observed is\\n(22.5%) relative for the sentence error rate, and 34.5% (relative) in word\\nerror rate.\\n',\n",
              " '  A computer model of \"a sense of humour\" suggested previously\\n[arXiv:0711.2058,0711.2061], relating the humorous effect with a specific\\nmalfunction in information processing, is given in somewhat different\\nexposition. Psychological aspects of humour are elaborated more thoroughly. The\\nmechanism of laughter is formulated on the more general level. Detailed\\ndiscussion is presented for the higher levels of information processing, which\\nare responsible for a perception of complex samples of humour. Development of a\\nsense of humour in the process of evolution is discussed.\\n',\n",
              " '  This paper explores several extensions of proof nets for the Lambek calculus\\nin order to handle the different connectives of display logic in a natural way.\\nThe new proof net calculus handles some recent additions to the Lambek\\nvocabulary such as Galois connections and Grishin interactions. It concludes\\nwith an exploration of the generative capacity of the Lambek-Grishin calculus,\\npresenting an embedding of lexicalized tree adjoining grammars into the\\nLambek-Grishin calculus.\\n',\n",
              " '  Computer model of a \"sense of humour\" suggested previously [arXiv:0711.2058,\\n0711.2061, 0711.2270] is raised to the level of a realistic algorithm.\\n',\n",
              " '  This article describes an exclusively resource-based method of morphological\\nannotation of written Korean text. Korean is an agglutinative language. Our\\nannotator is designed to process text before the operation of a syntactic\\nparser. In its present state, it annotates one-stem words only. The output is a\\ngraph of morphemes annotated with accurate linguistic information. The\\ngranularity of the tagset is 3 to 5 times higher than usual tagsets. A\\ncomparison with a reference annotated corpus showed that it achieves 89% recall\\nwithout any corpus training. The language resources used by the system are\\nlexicons of stems, transducers of suffixes and transducers of generation of\\nallomorphs. All can be easily updated, which allows users to control the\\nevolution of the performances of the system. It has been claimed that\\nmorphological annotation of Korean text could only be performed by a\\nmorphological analysis module accessing a lexicon of morphemes. We show that it\\ncan also be performed directly with a lexicon of words and without applying\\nmorphological rules at annotation time, which speeds up annotation to 1,210\\nword/s. The lexicon of words is obtained from the maintainable language\\nresources through a fully automated compilation process.\\n',\n",
              " '  International standards for lexicon formats are in preparation. To a certain\\nextent, the proposed formats converge with prior results of standardization\\nprojects. However, their adequacy for (i) lexicon management and (ii)\\nlexicon-driven applications have been little debated in the past, nor are they\\nas a part of the present standardization effort. We examine these issues. IGM\\nhas developed XML formats compatible with the emerging international standards,\\nand we report experimental results on large-coverage lexica.\\n',\n",
              " '  Maurice Gross (1934-2001) was both a great linguist and a pioneer in natural\\nlanguage processing. This article is written in homage to his memory\\n',\n",
              " '  We describe a resource-based method of morphological annotation of written\\nKorean text. Korean is an agglutinative language. The output of our system is a\\ngraph of morphemes annotated with accurate linguistic information. The language\\nresources used by the system can be easily updated, which allows us-ers to\\ncontrol the evolution of the per-formances of the system. We show that\\nmorphological annotation of Korean text can be performed directly with a\\nlexicon of words and without morpho-logical rules.\\n',\n",
              " '  Shifting to a lexicalized grammar reduces the number of parsing errors and\\nimproves application results. However, such an operation affects a syntactic\\nparser in all its aspects. One of our research objectives is to design a\\nrealistic model for grammar lexicalization. We carried out experiments for\\nwhich we used a grammar with a very simple content and formalism, and a very\\ninformative syntactic lexicon, the lexicon-grammar of French elaborated by the\\nLADL. Lexicalization was performed by applying the parameterized-graph\\napproach. Our results tend to show that most information in the lexicon-grammar\\ncan be transferred into a grammar and exploited successfully for the syntactic\\nparsing of sentences.\\n',\n",
              " '  Existing syntactic grammars of natural languages, even with a far from\\ncomplete coverage, are complex objects. Assessments of the quality of parts of\\nsuch grammars are useful for the validation of their construction. We evaluated\\nthe quality of a grammar of French determiners that takes the form of a\\nrecursive transition network. The result of the application of this local\\ngrammar gives deeper syntactic information than chunking or information\\navailable in treebanks. We performed the evaluation by comparison with a corpus\\nindependently annotated with information on determiners. We obtained 86%\\nprecision and 92% recall on text not tagged for parts of speech.\\n',\n",
              " '  We discuss the characteristics and behaviour of two parallel classes of verbs\\nin two Romance languages, French and Portuguese. Examples of these verbs are\\nPort. abater [gado] and Fr. abattre [b\\\\\\'etail], both meaning \"slaughter\\n[cattle]\". In both languages, the definition of the class of verbs includes\\nseveral features: - They have only one essential complement, which is a direct\\nobject. - The nominal distribution of the complement is very limited, i.e., few\\nnouns can be selected as head nouns of the complement. However, this selection\\nis not restricted to a single noun, as would be the case for verbal idioms such\\nas Fr. monter la garde \"mount guard\". - We excluded from the class\\nconstructions which are reductions of more complex constructions, e.g. Port.\\nafinar [instrumento] com \"tune [instrument] with\".\\n',\n",
              " '  The Outilex software platform, which will be made available to research,\\ndevelopment and industry, comprises software components implementing all the\\nfundamental operations of written text processing: processing without lexicons,\\nexploitation of lexicons and grammars, language resource management. All data\\nare structured in XML formats, and also in more compact formats, either\\nreadable or binary, whenever necessary; the required format converters are\\nincluded in the platform; the grammar formats allow for combining statistical\\napproaches with resource-based approaches. Manually constructed lexicons for\\nFrench and English, originating from the LADL, and of substantial coverage,\\nwill be distributed with the platform under LGPL-LR license.\\n',\n",
              " \"  Speaking a language and achieving proficiency in another one is a highly\\ncomplex process which requires the acquisition of various kinds of knowledge\\nand skills, like the learning of words, rules and patterns and their connection\\nto communicative goals (intentions), the usual starting point. To help the\\nlearner to acquire these skills we propose an enhanced, electronic version of\\nan age old method: pattern drills (henceforth PDs). While being highly regarded\\nin the fifties, PDs have become unpopular since then, partially because of\\ntheir lack of grounding (natural context) and rigidity. Despite these\\nshortcomings we do believe in the virtues of this approach, at least with\\nregard to the acquisition of basic linguistic reflexes or skills (automatisms),\\nnecessary to survive in the new language. Of course, the method needs\\nimprovement, and we will show here how this can be achieved. Unlike tapes or\\nbooks, computers are open media, allowing for dynamic changes, taking users'\\nperformances and preferences into account. Building an electronic version of\\nPDs amounts to building an open resource, accomodatable to the users' ever\\nchanging needs.\\n\",\n",
              " '  This paper discusses two new procedures for extracting verb valences from raw\\ntexts, with an application to the Polish language. The first novel technique,\\nthe EM selection algorithm, performs unsupervised disambiguation of valence\\nframe forests, obtained by applying a non-probabilistic deep grammar parser and\\nsome post-processing to the text. The second new idea concerns filtering of\\nincorrect frames detected in the parsed text and is motivated by an observation\\nthat verbs which take similar arguments tend to have similar frames. This\\nphenomenon is described in terms of newly introduced co-occurrence matrices.\\nUsing co-occurrence matrices, we split filtering into two steps. The list of\\nvalid arguments is first determined for each verb, whereas the pattern\\naccording to which the arguments are combined into frames is computed in the\\nfollowing stage. Our best extracted dictionary reaches an $F$-score of 45%,\\ncompared to an $F$-score of 39% for the standard frame-based BHT filtering.\\n',\n",
              " '  Because of the wide variety of contemporary practices used in the automatic\\nsyntactic parsing of natural languages, it has become necessary to analyze and\\nevaluate the strengths and weaknesses of different approaches. This research is\\nall the more necessary because there are currently no genre- and\\ndomain-independent parsers that are able to analyze unrestricted text with 100%\\npreciseness (I use this term to refer to the correctness of analyses assigned\\nby a parser). All these factors create a need for methods and resources that\\ncan be used to evaluate and compare parsing systems. This research describes:\\n(1) A theoretical analysis of current achievements in parsing and parser\\nevaluation. (2) A framework (called FEPa) that can be used to carry out\\npractical parser evaluations and comparisons. (3) A set of new evaluation\\nresources: FiEval is a Finnish treebank under construction, and MGTS and RobSet\\nare parser evaluation resources in English. (4) The results of experiments in\\nwhich the developed evaluation framework and the two resources for English were\\nused for evaluating a set of selected parsers.\\n',\n",
              " '  A simple review by a linguist, citing many articles by physicists:\\nQuantitative methods, agent-based computer simulations, language dynamics,\\nlanguage typology, historical linguistics\\n',\n",
              " \"  We present a comparison of two english texts, written by Lewis Carroll, one\\n(Alice in wonderland) and the other (Through a looking glass), the former\\ntranslated into esperanto, in order to observe whether natural and artificial\\nlanguages significantly differ from each other. We construct one dimensional\\ntime series like signals using either word lengths or word frequencies. We use\\nthe multifractal ideas for sorting out correlations in the writings. In order\\nto check the robustness of the methods we also write the corresponding shuffled\\ntexts. We compare characteristic functions and e.g. observe marked differences\\nin the (far from parabolic) f(alpha) curves, differences which we attribute to\\nTsallis non extensive statistical features in the ''frequency time series'' and\\n''length time series''. The esperanto text has more extreme vallues. A very\\nrough approximation consists in modeling the texts as a random Cantor set if\\nresulting from a binomial cascade of long and short words (or words and\\nblanks). This leads to parameters characterizing the text style, and most\\nlikely in fine the author writings.\\n\",\n",
              " '  In the article, theoretical principles and practical realization for the\\ncompilation of the concordance to \"Perekhresni stezhky\" (\"The Cross-Paths\"), a\\nnovel by Ivan Franko, are described. Two forms for the context presentation are\\nproposed. The electronic version of this lexicographic work is available\\nonline.\\n',\n",
              " \"  Robustness in a parser refers to an ability to deal with exceptional\\nphenomena. A parser is robust if it deals with phenomena outside its normal\\nrange of inputs. This paper reports on a series of robustness evaluations of\\nstate-of-the-art parsers in which we concentrated on one aspect of robustness:\\nits ability to parse sentences containing misspelled words. We propose two\\nmeasures for robustness evaluation based on a comparison of a parser's output\\nfor grammatical input sentences and their noisy counterparts. In this paper, we\\nuse these measures to compare the overall robustness of the four evaluated\\nparsers, and we present an analysis of the decline in parser performance with\\nincreasing error levels. Our results indicate that performance typically\\ndeclines tens of percentage units when parsers are presented with texts\\ncontaining misspellings. When it was tested on our purpose-built test set of\\n443 sentences, the best parser in the experiment (C&C parser) was able to\\nreturn exactly the same parse tree for the grammatical and ungrammatical\\nsentences for 60.8%, 34.0% and 14.9% of the sentences with one, two or three\\nmisspelled words respectively.\\n\",\n",
              " '  Large scale surveys of public mood are costly and often impractical to\\nperform. However, the web is awash with material indicative of public mood such\\nas blogs, emails, and web queries. Inexpensive content analysis on such\\nextensive corpora can be used to assess public mood fluctuations. The work\\npresented here is concerned with the analysis of the public mood towards the\\nfuture. Using an extension of the Profile of Mood States questionnaire, we have\\nextracted mood indicators from 10,741 emails submitted in 2006 to futureme.org,\\na web service that allows its users to send themselves emails to be delivered\\nat a later date. Our results indicate long-term optimism toward the future, but\\nmedium-term apprehension and confusion.\\n',\n",
              " '  Most current word prediction systems make use of n-gram language models (LM)\\nto estimate the probability of the following word in a phrase. In the past\\nyears there have been many attempts to enrich such language models with further\\nsyntactic or semantic information. We want to explore the predictive powers of\\nLatent Semantic Analysis (LSA), a method that has been shown to provide\\nreliable information on long-distance semantic dependencies between words in a\\ncontext. We present and evaluate here several methods that integrate LSA-based\\ninformation with a standard language model: a semantic cache, partial\\nreranking, and different forms of interpolation. We found that all methods show\\nsignificant improvements, compared to the 4-gram baseline, and most of them to\\na simple cache model as well.\\n',\n",
              " '  In this paper we suggest a typed compositional seman-tics for nominal\\ncompounds of the form [Adj Noun] that models adjectives as higher-order\\npolymorphic functions, and where types are assumed to represent concepts in an\\nontology that reflects our commonsense view of the world and the way we talk\\nabout it in or-dinary language. In addition to [Adj Noun] compounds our\\nproposal seems also to suggest a plausible explana-tion for well known\\nadjective ordering restrictions.\\n',\n",
              " \"  Current research in author profiling to discover a legal author's fingerprint\\ndoes not only follow examinations based on statistical parameters only but\\ninclude more and more dynamic methods that can learn and that react adaptable\\nto the specific behavior of an author. But the question on how to appropriately\\nrepresent a text is still one of the fundamental tasks, and the problem of\\nwhich attribute should be used to fingerprint the author's style is still not\\nexactly defined. In this work, we focus on linguistic selection of attributes\\nto fingerprint the style of the authors Parkin, Bassewitz and Leander. We use\\ntexts of the genre Fairy Tale as it has a clear style and texts of a shorter\\nsize with a straightforward story-line and a simple language.\\n\",\n",
              " '  We investigate the grapheme-phoneme relation in Ukrainian and some properties\\nof the Ukrainian version of the Cyrillic alphabet.\\n',\n",
              " '  This research report introduces the generation of textual entailment within\\nthe project CSIEC (Computer Simulation in Educational Communication), an\\ninteractive web-based human-computer dialogue system with natural language for\\nEnglish instruction. The generation of textual entailment (GTE) is critical to\\nthe further improvement of CSIEC project. Up to now we have found few\\nliteratures related with GTE. Simulating the process that a human being learns\\nEnglish as a foreign language we explore our naive approach to tackle the GTE\\nproblem and its algorithm within the framework of CSIEC, i.e. rule annotation\\nin NLML, pattern recognition (matching), and entailment transformation. The\\ntime and space complexity of our algorithm is tested with some entailment\\nexamples. Further works include the rules annotation based on the English\\ntextbooks and a GUI interface for normal users to edit the entailment rules.\\n',\n",
              " \"  The recognition, involvement, and description of main actors influences the\\nstory line of the whole text. This is of higher importance as the text per se\\nrepresents a flow of words and expressions that once it is read it is lost. In\\nthis respect, the understanding of a text and moreover on how the actor exactly\\nbehaves is not only a major concern: as human beings try to store a given input\\non short-term memory while associating diverse aspects and actors with\\nincidents, the following approach represents a virtual architecture, where\\ncollocations are concerned and taken as the associative completion of the\\nactors' acting. Once that collocations are discovered, they become managed in\\nseparated memory blocks broken down by the actors. As for human beings, the\\nmemory blocks refer to associative mind-maps. We then present several priority\\nfunctions to represent the actual temporal situation inside a mind-map to\\nenable the user to reconstruct the recent events from the discovered temporal\\nresults.\\n\",\n",
              " '  A computational model of the construction of word meaning through exposure to\\ntexts is built in order to simulate the effects of co-occurrence values on word\\nsemantic similarities, paragraph by paragraph. Semantic similarity is here\\nviewed as association. It turns out that the similarity between two words W1\\nand W2 strongly increases with a co-occurrence, decreases with the occurrence\\nof W1 without W2 or W2 without W1, and slightly increases with high-order\\nco-occurrences. Therefore, operationalizing similarity as a frequency of\\nco-occurrence probably introduces a bias: first, there are cases in which there\\nis similarity without co-occurrence and, second, the frequency of co-occurrence\\noverestimates similarity.\\n',\n",
              " \"  A recent study reported development of Muscorian, a generic text processing\\ntool for extracting protein-protein interactions from text that achieved\\ncomparable performance to biomedical-specific text processing tools. This\\nresult was unexpected since potential errors from a series of text analysis\\nprocesses is likely to adversely affect the outcome of the entire process. Most\\nbiomedical entity relationship extraction tools have used biomedical-specific\\nparts-of-speech (POS) tagger as errors in POS tagging and are likely to affect\\nsubsequent semantic analysis of the text, such as shallow parsing. This study\\naims to evaluate the parts-of-speech (POS) tagging accuracy and attempts to\\nexplore whether a comparable performance is obtained when a generic POS tagger,\\nMontyTagger, was used in place of MedPost, a tagger trained in biomedical text.\\nOur results demonstrated that MontyTagger, Muscorian's POS tagger, has a POS\\ntagging accuracy of 83.1% when tested on biomedical text. Replacing MontyTagger\\nwith MedPost did not result in a significant improvement in entity relationship\\nextraction from text; precision of 55.6% from MontyTagger versus 56.8% from\\nMedPost on directional relationships and 86.1% from MontyTagger compared to\\n81.8% from MedPost on nondirectional relationships. This is unexpected as the\\npotential for poor POS tagging by MontyTagger is likely to affect the outcome\\nof the information extraction. An analysis of POS tagging errors demonstrated\\nthat 78.5% of tagging errors are being compensated by shallow parsing. Thus,\\ndespite 83.1% tagging accuracy, MontyTagger has a functional tagging accuracy\\nof 94.6%.\\n\",\n",
              " '  Documents in scientific newspapers are often marked by attitudes and opinions\\nof the author and/or other persons, who contribute with objective and\\nsubjective statements and arguments as well. In this respect, the attitude is\\noften accomplished by a linguistic modality. As in languages like english,\\nfrench and german, the modality is expressed by special verbs like can, must,\\nmay, etc. and the subjunctive mood, an occurrence of modalities often induces\\nthat these verbs take over the role of modality. This is not correct as it is\\nproven that modality is the instrument of the whole sentence where both the\\nadverbs, modal particles, punctuation marks, and the intonation of a sentence\\ncontribute. Often, a combination of all these instruments are necessary to\\nexpress a modality. In this work, we concern with the finding of modal verbs in\\nscientific texts as a pre-step towards the discovery of the attitude of an\\nauthor. Whereas the input will be an arbitrary text, the output consists of\\nzones representing modalities.\\n',\n",
              " '  We compare the performance of a recurrent neural network with the best\\nresults published so far on phoneme recognition in the TIMIT database. These\\npublished results have been obtained with a combination of classifiers.\\nHowever, in this paper we apply a single recurrent neural network to the same\\ntask. Our recurrent neural network attains an error rate of 24.6%. This result\\nis not significantly different from that obtained by the other best methods,\\nbut they rely on a combination of classifiers for achieving comparable\\nperformance.\\n',\n",
              " '  The derivation trees of a tree adjoining grammar provide a first insight into\\nthe sentence semantics, and are thus prime targets for generation systems. We\\ndefine a formalism, feature-based regular tree grammars, and a translation from\\nfeature based tree adjoining grammars into this new formalism. The translation\\npreserves the derivation structures of the original grammar, and accounts for\\nfeature unification.\\n',\n",
              " '  Proof nets are a graph theoretical representation of proofs in various\\nfragments of type-logical grammar. In spite of this basis in graph theory,\\nthere has been relatively little attention to the use of graph theoretic\\nalgorithms for type-logical proof search. In this paper we will look at several\\nways in which standard graph theoretic algorithms can be used to restrict the\\nsearch space. In particular, we will provide an O(n4) algorithm for selecting\\nan optimal axiom link at any stage in the proof search as well as a O(kn3)\\nalgorithm for selecting the k best proof candidates.\\n',\n",
              " '  In this paper we describe the conception of a software toolkit designed for\\nthe construction, maintenance and collaborative use of a Generative Lexicon. In\\norder to ease its portability and spreading use, this tool was built with free\\nand open source products. We eventually tested the toolkit and showed it\\nfilters the adequate form of anaphoric reference to the modifier in endocentric\\ncompounds.\\n',\n",
              " '  We describe a modular system for generating sentences from formal definitions\\nof underlying linguistic structures using domain-specific languages. The system\\nuses Java in general, Prolog for lexical entries and custom domain-specific\\nlanguages based on Functional Grammar and Functional Discourse Grammar\\nnotation, implemented using the ANTLR parser generator. We show how linguistic\\nand technological parts can be brought together in a natural language\\nprocessing system and how domain-specific languages can be used as a tool for\\nconsistent formal notation in linguistic description.\\n',\n",
              " '  The type-theoretic modelling of DRT that [degroote06] proposed features\\ncontinuations for the management of the context in which a clause has to be\\ninterpreted. This approach, while keeping the standard definitions of\\nquantifier scope, translates the rules of the accessibility constraints of\\ndiscourse referents inside the semantic recipes. In this paper, we deal with\\nadditional rules for these accessibility constraints. In particular in the case\\nof discourse referents introduced by proper nouns, that negation does not\\nblock, and in the case of rhetorical relations that structure discourses. We\\nshow how this continuation-based approach applies to those accessibility\\nconstraints and how we can consider the parallel management of various\\nprinciples.\\n',\n",
              " \"  The goal of this paper is to present a model of children's semantic memory,\\nwhich is based on a corpus reproducing the kinds of texts children are exposed\\nto. After presenting the literature in the development of the semantic memory,\\na preliminary French corpus of 3.2 million words is described. Similarities in\\nthe resulting semantic space are compared to human data on four tests:\\nassociation norms, vocabulary test, semantic judgments and memory tasks. A\\nsecond corpus is described, which is composed of subcorpora corresponding to\\nvarious ages. This stratified corpus is intended as a basis for developmental\\nstudies. Finally, two applications of these models of semantic memory are\\npresented: the first one aims at tracing the development of semantic\\nsimilarities paragraph by paragraph; the second one describes an implementation\\nof a model of text comprehension derived from the Construction-integration\\nmodel (Kintsch, 1988, 1998) and based on such models of semantic memory.\\n\",\n",
              " '  In this paper we present two original methods for recognizing textual\\ninference. First one is a modified resolution method such that some linguistic\\nconsiderations are introduced in the unification of two atoms. The approach is\\npossible due to the recent methods of transforming texts in logic formulas.\\nSecond one is based on semantic relations in text, as presented in WordNet.\\nSome similarities between these two methods are remarked.\\n',\n",
              " '  A large class of unsupervised algorithms for Word Sense Disambiguation (WSD)\\nis that of dictionary-based methods. Various algorithms have as the root Lesk\\'s\\nalgorithm, which exploits the sense definitions in the dictionary directly. Our\\napproach uses the lexical base WordNet for a new algorithm originated in\\nLesk\\'s, namely \"chain algorithm for disambiguation of all words\", CHAD. We show\\nhow translation from a language into another one and also text entailment\\nverification could be accomplished by this disambiguation.\\n',\n",
              " '  Meaning cannot be based on dictionary definitions all the way down: at some\\npoint the circularity of definitions must be broken in some way, by grounding\\nthe meanings of certain words in sensorimotor categories learned from\\nexperience or shaped by evolution. This is the \"symbol grounding problem.\" We\\nintroduce the concept of a reachable set -- a larger vocabulary whose meanings\\ncan be learned from a smaller vocabulary through definition alone, as long as\\nthe meanings of the smaller vocabulary are themselves already grounded. We\\nprovide simple algorithms to compute reachable sets for any given dictionary.\\n',\n",
              " '  Measuring the similarity of short written contexts is a fundamental problem\\nin Natural Language Processing. This article provides a unifying framework by\\nwhich short context problems can be categorized both by their intended\\napplication and proposed solution. The goal is to show that various problems\\nand methodologies that appear quite different on the surface are in fact very\\nclosely related. The axes by which these categorizations are made include the\\nformat of the contexts (headed versus headless), the way in which the contexts\\nare to be measured (first-order versus second-order similarity), and the\\ninformation used to represent the features in the contexts (micro versus macro\\nviews). The unifying thread that binds together many short context applications\\nand methods is the fact that similarity decisions must be made between contexts\\nthat share few (if any) words in common.\\n',\n",
              " '  The algorithm of the creation texts parallel corpora was presented. The\\nalgorithm is based on the use of \"key words\" in text documents, and on the\\nmeans of their automated translation. Key words were singled out by means of\\nusing Russian and Ukrainian morphological dictionaries, as well as dictionaries\\nof the translation of nouns for the Russian and Ukrainianlanguages. Besides, to\\ncalculate the weights of the terms in the documents, empiric-statistic rules\\nwere used. The algorithm under consideration was realized in the form of a\\nprogram complex, integrated into the content-monitoring InfoStream system. As a\\nresult, a parallel bilingual corpora of web-publications containing about 30\\nthousand documents, was created\\n',\n",
              " '  In this paper, we present an open-source parsing environment (Tuebingen\\nLinguistic Parsing Architecture, TuLiPA) which uses Range Concatenation Grammar\\n(RCG) as a pivot formalism, thus opening the way to the parsing of several\\nmildly context-sensitive formalisms. This environment currently supports\\ntree-based grammars (namely Tree-Adjoining Grammars, TAG) and Multi-Component\\nTree-Adjoining Grammars with Tree Tuples (TT-MCTAG)) and allows computation not\\nonly of syntactic structures, but also of the corresponding semantic\\nrepresentations. It is used for the development of a tree-based grammar for\\nGerman.\\n',\n",
              " '  The classical logical antinomy known as Richard-Berry paradox is combined\\nwith plausible assumptions about the size i.e. the descriptional complexity of\\nTuring machines formalizing certain sentences, to show that formalization of\\nlanguage leads to contradiction.\\n',\n",
              " \"  The ancient and extinct language Meroitic is investigated using Zipf's Law.\\nIn particular, since Meroitic is still undeciphered, the Zipf law analysis\\nallows us to assess the quality of current texts and possible avenues for\\nfuture investigation using statistical techniques.\\n\",\n",
              " \"  Julian Jaynes's profound humanitarian convictions not only prevented him from\\ngoing to war, but would have prevented him from ever kicking a dog. Yet\\naccording to his theory, not only are language-less dogs unconscious, but so\\ntoo were the speaking/hearing Greeks in the Bicameral Era, when they heard\\ngods' voices telling them what to do rather than thinking for themselves. I\\nargue that to be conscious is to be able to feel, and that all mammals (and\\nprobably lower vertebrates and invertebrates too) feel, hence are conscious.\\nJulian Jaynes's brilliant analysis of our concepts of consciousness\\nnevertheless keeps inspiring ever more inquiry and insights into the age-old\\nmind/body problem and its relation to cognition and language.\\n\",\n",
              " '  Meroitic is the still undeciphered language of the ancient civilization of\\nKush. Over the years, various techniques for decipherment such as finding a\\nbilingual text or cognates from modern or other ancient languages in the Sudan\\nand surrounding areas has not been successful. Using techniques borrowed from\\ninformation theory and natural language statistics, similar words are paired\\nand attempts are made to use currently defined words to extract at least\\npartial meaning from unknown words.\\n',\n",
              " '  Multilingual parallel texts (abbreviated to parallel texts) are linguistic\\nversions of the same content (\"translations\"); e.g., the Maastricht Treaty in\\nEnglish and Spanish are parallel texts. This document is about creating an open\\narchitecture for the whole Authoring, Translation and Publishing Chain\\n(ATP-chain) for the processing of parallel texts.\\n',\n",
              " '  The origin of long-range letter correlations in natural texts is studied\\nusing random walk analysis and Jensen-Shannon divergence. It is concluded that\\nthey result from slow variations in letter frequency distribution, which are a\\nconsequence of slow variations in lexical composition within the text. These\\ncorrelations are preserved by random letter shuffling within a moving window.\\nAs such, they do reflect structural properties of the text, but in a very\\nindirect manner.\\n',\n",
              " '  Recognizing analogies, synonyms, antonyms, and associations appear to be four\\ndistinct tasks, requiring distinct NLP algorithms. In the past, the four tasks\\nhave been treated independently, using a wide variety of algorithms. These four\\nsemantic classes, however, are a tiny sample of the full range of semantic\\nphenomena, and we cannot afford to create ad hoc algorithms for each semantic\\nphenomenon; we need to seek a unified approach. We propose to subsume a broad\\nrange of phenomena under analogies. To limit the scope of this paper, we\\nrestrict our attention to the subsumption of synonyms, antonyms, and\\nassociations. We introduce a supervised corpus-based machine learning algorithm\\nfor classifying analogous word pairs, and we show that it can solve\\nmultiple-choice SAT analogy questions, TOEFL synonym questions, ESL\\nsynonym-antonym questions, and similar-associated-both questions from cognitive\\npsychology.\\n',\n",
              " '  The paper deals with using descriptive mark-up to emphasize translation\\nmistakes. The author postulates the necessity to develop a standard and formal\\nXML-based way of describing translation mistakes. It is considered to be\\nimportant for achieving impersonal translation quality assessment. Marked-up\\ntranslations can be used in corpus translation studies; moreover, automatic\\ntranslation assessment based on marked-up mistakes is possible. The paper\\nconcludes with setting up guidelines for further activity within the described\\nfield.\\n',\n",
              " '  In the paper, we analyze the distribution of complexities in the Vai script,\\nan indigenous syllabic writing system from Liberia. It is found that the\\nuniformity hypothesis for complexities fails for this script. The models using\\nPoisson distribution for the number of components and hyper-Poisson\\ndistribution for connections provide good fits in the case of the Vai script.\\n',\n",
              " \"  In this article, some first elements of a computational modelling of the\\ngrammar of the Martiniquese French Creole dialect are presented. The sources of\\ninspiration for the modelling is the functional description given by Damoiseau\\n(1984), and Pinalie's & Bernabe's (1999) grammar manual. Based on earlier works\\nin text generation (Vaillant, 1997), a unification grammar formalism, namely\\nTree Adjoining Grammars (TAG), and a modelling of lexical functional categories\\nbased on syntactic and semantic properties, are used to implement a grammar of\\nMartiniquese Creole which is used in a prototype of text generation system. One\\nof the main applications of the system could be its use as a tool software\\nsupporting the task of learning Creole as a second language. -- Nous\\npr\\\\'esenterons dans cette communication les premiers travaux de mod\\\\'elisation\\ninformatique d'une grammaire de la langue cr\\\\'eole martiniquaise, en nous\\ninspirant des descriptions fonctionnelles de Damoiseau (1984) ainsi que du\\nmanuel de Pinalie & Bernab\\\\'e (1999). Prenant appui sur des travaux\\nant\\\\'erieurs en g\\\\'en\\\\'eration de texte (Vaillant, 1997), nous utilisons un\\nformalisme de grammaires d'unification, les grammaires d'adjonction d'arbres\\n(TAG d'apr\\\\`es l'acronyme anglais), ainsi qu'une mod\\\\'elisation de cat\\\\'egories\\nlexicales fonctionnelles \\\\`a base syntaxico-s\\\\'emantique, pour mettre en oeuvre\\nune grammaire du cr\\\\'eole martiniquais utilisable dans une maquette de\\nsyst\\\\`eme de g\\\\'en\\\\'eration automatique. L'un des int\\\\'er\\\\^ets principaux de ce\\nsyst\\\\`eme pourrait \\\\^etre son utilisation comme logiciel outil pour l'aide \\\\`a\\nl'apprentissage du cr\\\\'eole en tant que langue seconde.\\n\",\n",
              " '  This article describes the design of a common syntactic description for the\\ncore grammar of a group of related dialects. The common description does not\\nrely on an abstract sub-linguistic structure like a metagrammar: it consists in\\na single FS-LTAG where the actual specific language is included as one of the\\nattributes in the set of attribute types defined for the features. When the\\nlang attribute is instantiated, the selected subset of the grammar is\\nequivalent to the grammar of one dialect. When it is not, we have a model of a\\nhybrid multidialectal linguistic system. This principle is used for a group of\\ncreole languages of the West-Atlantic area, namely the French-based Creoles of\\nHaiti, Guadeloupe, Martinique and French Guiana.\\n',\n",
              " \"  We propose a theoretical framework within which information on the vocabulary\\nof a given corpus can be inferred on the basis of statistical information\\ngathered on that corpus. Inferences can be made on the categories of the words\\nin the vocabulary, and on their syntactical properties within particular\\nlanguages. Based on the same statistical data, it is possible to build matrices\\nof syntagmatic similarity (bigram transition matrices) or paradigmatic\\nsimilarity (probability for any pair of words to share common contexts). When\\nclustered with respect to their syntagmatic similarity, words tend to group\\ninto sublanguage vocabularies, and when clustered with respect to their\\nparadigmatic similarity, into syntactic or semantic classes. Experiments have\\nexplored the first of these two possibilities. Their results are interpreted in\\nthe frame of a Markov chain modelling of the corpus' generative processe(s): we\\nshow that the results of a spectral analysis of the transition matrix can be\\ninterpreted as probability distributions of words within clusters. This method\\nyields a soft clustering of the vocabulary into sublanguages which contribute\\nto the generation of heterogeneous corpora. As an application, we show how\\nmultilingual texts can be visually segmented into linguistically homogeneous\\nsegments. Our method is specifically useful in the case of related languages\\nwhich happened to be mixed in corpora.\\n\",\n",
              " '  Without prior knowledge, distinguishing different languages may be a hard\\ntask, especially when their borders are permeable. We develop an extension of\\nspectral clustering -- a powerful unsupervised classification toolbox -- that\\nis shown to resolve accurately the task of soft language distinction. At the\\nheart of our approach, we replace the usual hard membership assignment of\\nspectral clustering by a soft, probabilistic assignment, which also presents\\nthe advantage to bypass a well-known complexity bottleneck of the method.\\nFurthermore, our approach relies on a novel, convenient construction of a\\nMarkov chain out of a corpus. Extensive experiments with a readily available\\nsystem clearly display the potential of the method, which brings a visually\\nappealing soft distinction of languages that may define altogether a whole\\ncorpus.\\n',\n",
              " '  In this article we present a model of human written text based on statistical\\nmechanics approach by deriving the potential energy for different parts of the\\ntext using large text corpus. We have checked the results numerically and found\\nthat the specific heat parameter effectively separates the closed class words\\nfrom the specific terms used in the text.\\n',\n",
              " '  We examine a naming game with two agents trying to establish a common\\nvocabulary for n objects. Such efforts lead to the emergence of language that\\nallows for an efficient communication and exhibits some degree of homonymy and\\nsynonymy. Although homonymy reduces the communication efficiency, it seems to\\nbe a dynamical trap that persists for a long, and perhaps indefinite, time. On\\nthe other hand, synonymy does not reduce the efficiency of communication, but\\nappears to be only a transient feature of the language. Thus, in our model the\\nrole of synonymy decreases and in the long-time limit it becomes negligible. A\\nsimilar rareness of synonymy is observed in present natural languages. The role\\nof noise, that distorts the communicated words, is also examined. Although, in\\ngeneral, the noise reduces the communication efficiency, it also regroups the\\nwords so that they are more evenly distributed within the available \"verbal\"\\nspace.\\n',\n",
              " \"  For human beings, the processing of text streams of unknown size leads\\ngenerally to problems because e.g. noise must be selected out, information be\\ntested for its relevance or redundancy, and linguistic phenomenon like\\nambiguity or the resolution of pronouns be advanced. Putting this into\\nsimulation by using an artificial mind-map is a challenge, which offers the\\ngate for a wide field of applications like automatic text summarization or\\npunctual retrieval. In this work we present a framework that is a first step\\ntowards an automatic intellect. It aims at assembling a mind-map based on\\nincoming text streams and on a subject-verb-object strategy, having the verb as\\nan interconnection between the adjacent nouns. The mind-map's performance is\\nenriched by a pronoun resolution engine that bases on the work of D. Klein, and\\nC. D. Manning.\\n\",\n",
              " '  Content zoning can be understood as a segmentation of textual documents into\\nzones. This is inspired by [6] who initially proposed an approach for the\\nargumentative zoning of textual documents. With the prototypical CoZo+ engine,\\nwe focus on content zoning towards an automatic processing of textual streams\\nwhile considering only the actors as the zones. We gain information that can be\\nused to realize an automatic recognition of content for pre-defined actors. We\\nunderstand CoZo+ as a necessary pre-step towards an automatic generation of\\nsummaries and to make intellectual ownership of documents detectable.\\n',\n",
              " '  We present the architecture of the UNL-French deconverter, which \"generates\"\\nfrom the UNL interlingua by first\"localizing\" the UNL form for French, within\\nUNL, and then applying slightly adapted but classical transfer and generation\\ntechniques, implemented in GETA\\'s Ariane-G5 environment, supplemented by some\\nUNL-specific tools. Online interaction can be used during deconversion to\\nenhance output quality and is now used for development purposes. We show how\\ninteraction could be delayed and embedded in the postedition phase, which would\\nthen interact not directly with the output text, but indirectly with several\\ncomponents of the deconverter. Interacting online or offline can improve the\\nquality not only of the utterance at hand, but also of the utterances processed\\nlater, as various preferences may be automatically changed to let the\\ndeconverter \"learn\".\\n',\n",
              " '  Collocations are important for many tasks of Natural language processing such\\nas information retrieval, machine translation, computational lexicography etc.\\nSo far many statistical methods have been used for collocation extraction.\\nAlmost all the methods form a classical crisp set of collocation. We propose a\\nfuzzy logic approach of collocation extraction to form a fuzzy set of\\ncollocations in which each word combination has a certain grade of membership\\nfor being collocation. Fuzzy logic provides an easy way to express natural\\nlanguage into fuzzy logic rules. Two existing methods; Mutual information and\\nt-test have been utilized for the input of the fuzzy inference system. The\\nresulting membership function could be easily seen and demonstrated. To show\\nthe utility of the fuzzy logic some word pairs have been examined as an\\nexample. The working data has been based on a corpus of about one million words\\ncontained in different novels constituting project Gutenberg available on\\nwww.gutenberg.org. The proposed method has all the advantages of the two\\nmethods, while overcoming their drawbacks. Hence it provides a better result\\nthan the two methods.\\n',\n",
              " '  Two well-known databases of semantic relationships between pairs of words\\nused in psycholinguistics, feature-based and association-based, are studied as\\ncomplex networks. We propose an algorithm to disentangle feature based\\nrelationships from free association semantic networks. The algorithm uses the\\nrich topology of the free association semantic network to produce a new set of\\nrelationships between words similar to those observed in feature production\\nnorms.\\n',\n",
              " '  Many AI researchers and cognitive scientists have argued that analogy is the\\ncore of cognition. The most influential work on computational modeling of\\nanalogy-making is Structure Mapping Theory (SMT) and its implementation in the\\nStructure Mapping Engine (SME). A limitation of SME is the requirement for\\ncomplex hand-coded representations. We introduce the Latent Relation Mapping\\nEngine (LRME), which combines ideas from SME and Latent Relational Analysis\\n(LRA) in order to remove the requirement for hand-coded representations. LRME\\nbuilds analogical mappings between lists of words, using a large corpus of raw\\ntext to automatically discover the semantic relations among the words. We\\nevaluate LRME on a set of twenty analogical mapping problems, ten based on\\nscientific analogies and ten based on common metaphors. LRME achieves\\nhuman-level performance on the twenty problems. We compare LRME with a variety\\nof alternative approaches and find that they are not able to reach the same\\nlevel of performance.\\n',\n",
              " '  Recent research has shown that language and the socio-cognitive phenomena\\nassociated with it can be aptly modeled and visualized through networks of\\nlinguistic entities. However, most of the existing works on linguistic networks\\nfocus only on the local properties of the networks. This study is an attempt to\\nanalyze the structure of languages via a purely structural technique, namely\\nspectral analysis, which is ideally suited for discovering the global\\ncorrelations in a network. Application of this technique to PhoNet, the\\nco-occurrence network of consonants, not only reveals several natural\\nlinguistic principles governing the structure of the consonant inventories, but\\nis also able to quantify their relative importance. We believe that this\\npowerful technique can be successfully applied, in general, to study the\\nstructure of natural languages.\\n',\n",
              " \"  Background: Zipf's discovery that word frequency distributions obey a power\\nlaw established parallels between biological and physical processes, and\\nlanguage, laying the groundwork for a complex systems perspective on human\\ncommunication. More recent research has also identified scaling regularities in\\nthe dynamics underlying the successive occurrences of events, suggesting the\\npossibility of similar findings for language as well.\\n  Methodology/Principal Findings: By considering frequent words in USENET\\ndiscussion groups and in disparate databases where the language has different\\nlevels of formality, here we show that the distributions of distances between\\nsuccessive occurrences of the same word display bursty deviations from a\\nPoisson process and are well characterized by a stretched exponential (Weibull)\\nscaling. The extent of this deviation depends strongly on semantic type -- a\\nmeasure of the logicality of each word -- and less strongly on frequency. We\\ndevelop a generative model of this behavior that fully determines the dynamics\\nof word usage.\\n  Conclusions/Significance: Recurrence patterns of words are well described by\\na stretched exponential distribution of recurrence times, an empirical scaling\\nthat cannot be anticipated from Zipf's law. Because the use of words provides a\\nuniquely precise and powerful lens on human thought and activity, our findings\\nalso have implications for other overt manifestations of collective human\\ndynamics.\\n\",\n",
              " '  The Indus script is one of the major undeciphered scripts of the ancient\\nworld. The small size of the corpus, the absence of bilingual texts, and the\\nlack of definite knowledge of the underlying language has frustrated efforts at\\ndecipherment since the discovery of the remains of the Indus civilisation.\\nRecently, some researchers have questioned the premise that the Indus script\\nencodes spoken language. Building on previous statistical approaches, we apply\\nthe tools of statistical language processing, specifically $n$-gram Markov\\nchains, to analyse the Indus script for syntax. Our main results are that the\\nscript has well-defined signs which begin and end texts, that there is\\ndirectionality and strong correlations in the sign order, and that there are\\ngroups of signs which appear to have identical syntactic function. All these\\nrequire no {\\\\it a priori} suppositions regarding the syntactic or semantic\\ncontent of the signs, but follow directly from the statistical analysis. Using\\ninformation theoretic measures, we find the information in the script to be\\nintermediate between that of a completely random and a completely fixed\\nordering of signs. Our study reveals that the Indus script is a structured sign\\nsystem showing features of a formal language, but, at present, cannot\\nconclusively establish that it encodes {\\\\it natural} language. Our $n$-gram\\nMarkov model is useful for predicting signs which are missing or illegible in a\\ncorpus of Indus texts. This work forms the basis for the development of a\\nstochastic grammar which can be used to explore the syntax of the Indus script\\nin greater detail.\\n',\n",
              " '  We analyze the rank-frequency distributions of words in selected English and\\nPolish texts. We compare scaling properties of these distributions in both\\nlanguages. We also study a few small corpora of Polish literary texts and find\\nthat for a corpus consisting of texts written by different authors the basic\\nscaling regime is broken more strongly than in the case of comparable corpus\\nconsisting of texts written by the same author. Similarly, for a corpus\\nconsisting of texts translated into Polish from other languages the scaling\\nregime is broken more strongly than for a comparable corpus of native Polish\\ntexts. Moreover, based on the British National Corpus, we consider the\\nrank-frequency distributions of the grammatically basic forms of words (lemmas)\\ntagged with their proper part of speech. We find that these distributions do\\nnot scale if each part of speech is analyzed separately. The only part of\\nspeech that independently develops a trace of scaling is verbs.\\n',\n",
              " \"  In this article, we propose an automatic process to build multi-lingual\\nlexico-semantic resources. The goal of these resources is to browse\\nsemantically textual information contained in texts of different languages.\\nThis method uses a mathematical model called Atlas s\\\\'emantiques in order to\\nrepresent the different senses of each word. It uses the linguistic relations\\nbetween words to create graphs that are projected into a semantic space. These\\nprojections constitute semantic maps that denote the sense trends of each given\\nword. This model is fed with syntactic relations between words extracted from a\\ncorpus. Therefore, the lexico-semantic resource produced describes all the\\nwords and all their meanings observed in the corpus. The sense trends are\\nexpressed by syntactic contexts, typical for a given meaning. The link between\\neach sense trend and the utterances used to build the sense trend are also\\nstored in an index. Thus all the instances of a word in a particular sense are\\nlinked and can be browsed easily. And by using several corpora of different\\nlanguages, several resources are built that correspond with each other through\\nlanguages. It makes it possible to browse information through languages thanks\\nto syntactic contexts translations (even if some of them are partial).\\n\",\n",
              " \"  Cilibrasi and Vitanyi have demonstrated that it is possible to extract the\\nmeaning of words from the world-wide web. To achieve this, they rely on the\\nnumber of webpages that are found through a Google search containing a given\\nword and they associate the page count to the probability that the word appears\\non a webpage. Thus, conditional probabilities allow them to correlate one word\\nwith another word's meaning. Furthermore, they have developed a similarity\\ndistance function that gauges how closely related a pair of words is. We\\npresent a specific counterexample to the triangle inequality for this\\nsimilarity distance function.\\n\",\n",
              " '  This paper reports on results on the entropy of the Spanish language. They\\nare based on an analysis of natural language for n-word symbols (n = 1 to 18),\\ntrigrams, digrams, and characters. The results obtained in this work are based\\non the analysis of twelve different literary works in Spanish, as well as a\\n279917 word news file provided by the Spanish press agency EFE. Entropy values\\nare calculated by a direct method using computer processing and the probability\\nlaw of large numbers. Three samples of artificial Spanish language produced by\\na first-order model software source are also analyzed and compared with natural\\nSpanish language.\\n',\n",
              " \"  Human language, the most powerful communication system in history, is closely\\nassociated with cognition. Written text is one of the fundamental\\nmanifestations of language, and the study of its universal regularities can\\ngive clues about how our brains process information and how we, as a society,\\norganize and share it. Still, only classical patterns such as Zipf's law have\\nbeen explored in depth. In contrast, other basic properties like the existence\\nof bursts of rare words in specific documents, the topical organization of\\ncollections, or the sublinear growth of vocabulary size with the length of a\\ndocument, have only been studied one by one and mainly applying heuristic\\nmethodologies rather than basic principles and general mechanisms. As a\\nconsequence, there is a lack of understanding of linguistic processes as\\ncomplex emergent phenomena. Beyond Zipf's law for word frequencies, here we\\nfocus on Heaps' law, burstiness, and the topicality of document collections,\\nwhich encode correlations within and across documents absent in random null\\nmodels. We introduce and validate a generative model that explains the\\nsimultaneous emergence of all these patterns from simple rules. As a result, we\\nfind a connection between the bursty nature of rare words and the topical\\norganization of texts and identify dynamic word ranking and memory across\\ndocuments as key mechanisms explaining the non trivial organization of written\\ntext. Our research can have broad implications and practical applications in\\ncomputer science, cognitive science, and linguistics.\\n\",\n",
              " '  A confidence measure is able to estimate the reliability of an hypothesis\\nprovided by a machine translation system. The problem of confidence measure can\\nbe seen as a process of testing : we want to decide whether the most probable\\nsequence of words provided by the machine translation system is correct or not.\\nIn the following we describe several original word-level confidence measures\\nfor machine translation, based on mutual information, n-gram language model and\\nlexical features language model. We evaluate how well they perform individually\\nor together, and show that using a combination of confidence measures based on\\nmutual information yields a classification error rate as low as 25.1% with an\\nF-measure of 0.708.\\n',\n",
              " '  We introduce a way to represent word pairs instantiating arbitrary semantic\\nrelations that keeps track of the contexts in which the words in the pair occur\\nboth together and independently. The resulting features are of sufficient\\ngenerality to allow us, with the help of a standard supervised machine learning\\nalgorithm, to tackle a variety of unrelated semantic tasks with good results\\nand almost no task-specific tailoring.\\n',\n",
              " '  In this paper we present the first step in a larger series of experiments for\\nthe induction of predicate/argument structures. The structures that we are\\ninducing are very similar to the conceptual structures that are used in Frame\\nSemantics (such as FrameNet). Those structures are called messages and they\\nwere previously used in the context of a multi-document summarization system of\\nevolving events. The series of experiments that we are proposing are\\nessentially composed from two stages. In the first stage we are trying to\\nextract a representative vocabulary of words. This vocabulary is later used in\\nthe second stage, during which we apply to it various clustering approaches in\\norder to identify the clusters of predicates and arguments--or frames and\\nsemantic roles, to use the jargon of Frame Semantics. This paper presents in\\ndetail and evaluates the first stage.\\n',\n",
              " '  We report experiments about the syntactic variations of support verb\\nconstructions, a special type of multiword expressions (MWEs) containing\\npredicative nouns. In these expressions, the noun can occur with or without the\\nverb, with no clear-cut semantic difference. We extracted from a large French\\ncorpus a set of examples of the two situations and derived statistical results\\nfrom these data. The extraction involved large-coverage language resources and\\nfinite-state techniques. The results show that, most frequently, predicative\\nnouns occur without a support verb. This fact has consequences on methods of\\nextracting or recognising MWEs.\\n',\n",
              " '  Some statistical properties of a network of two-Chinese-character compound\\nwords in Japanese language are reported. In this network, a node represents a\\nChinese character and an edge represents a two-Chinese-character compound word.\\nIt is found that this network has properties of \"small-world\" and \"scale-free.\"\\nA network formed by only Chinese characters for common use ({\\\\it joyo-kanji} in\\nJapanese), which is regarded as a subclass of the original network, also has\\nsmall-world property. However, a degree distribution of the network exhibits no\\nclear power law. In order to reproduce disappearance of the power-law property,\\na model for a selecting process of the Chinese characters for common use is\\nproposed.\\n',\n",
              " '  Formal work in linguistics has both produced and used important mathematical\\ntools. Motivated by a survey of models for context and word meaning, syntactic\\ncategories, phrase structure rules and trees, an attempt is being made in the\\npresent paper to present a mathematical model for structuring of sentences from\\nactive voice to passive voice, which is is the form of a transitive verb whose\\ngrammatical subject serves as the patient, receiving the action of the verb.\\n  For this purpose we have parsed all sentences of a corpus and have generated\\nBoolean groups for each of them. It has been observed that when we take\\nconstituents of the sentences as subgroups, the sequences of phrases form\\npermutation roups. Application of isomorphism property yields permutation\\nmapping between the important subgroups. It has resulted in a model for\\ntransformation of sentences from active voice to passive voice. A computer\\nprogram has been written to enable the software developers to evolve grammar\\nsoftware for sentence transformations.\\n',\n",
              " '  n this paper, we attempt to explain the emergence of the linguistic diversity\\nthat exists across the consonant inventories of some of the major language\\nfamilies of the world through a complex network based growth model. There is\\nonly a single parameter for this model that is meant to introduce a small\\namount of randomness in the otherwise preferential attachment based growth\\nprocess. The experiments with this model parameter indicates that the choice of\\nconsonants among the languages within a family are far more preferential than\\nit is across the families. The implications of this result are twofold -- (a)\\nthere is an innate preference of the speakers towards acquiring certain\\nlinguistic structures over others and (b) shared ancestry propels the stronger\\npreferential connection between the languages within a family than across them.\\nFurthermore, our observations indicate that this parameter might bear a\\ncorrelation with the period of existence of the language families under\\ninvestigation.\\n',\n",
              " '  The paper presents a linguistic and computational model aiming at making the\\nmorphological structure of the lexicon emerge from the formal and semantic\\nregularities of the words it contains. The model is word-based. The proposed\\nmorphological structure consists of (1) binary relations that connect each\\nheadword with words that are morphologically related, and especially with the\\nmembers of its morphological family and its derivational series, and of (2) the\\nanalogies that hold between the words. The model has been tested on the lexicon\\nof French using the TLFi machine readable dictionary.\\n',\n",
              " '  This paper is about the technical design of a large computational lexicon,\\nits storage, and its access from a Prolog environment. Traditionally, efficient\\naccess and storage of data structures is implemented by a relational database\\nmanagement system. In Delilah, a lexicon-based NLP system, efficient access to\\nthe lexicon by the semantic generator is vital. We show that our highly\\ndetailed HPSG-style lexical specifications do not fit well in the Relational\\nModel, and that they cannot be efficiently retrieved. We argue that they fit\\nmore naturally in the Object-Oriented Model. Although storage of objects is\\nredundant, we claim that efficient access is still possible by applying\\nindexing, and compression techniques from the Relational Model to the\\nObject-Oriented Model. We demonstrate that it is possible to implement\\nobject-oriented storage and fast access in ISO Prolog.\\n',\n",
              " \"  There is a great deal of work in cognitive psychology, linguistics, and\\ncomputer science, about using word (or phrase) frequencies in context in text\\ncorpora to develop measures for word similarity or word association, going back\\nto at least the 1960s. The goal of this chapter is to introduce the\\nnormalizedis a general way to tap the amorphous low-grade knowledge available\\nfor free on the Internet, typed in by local users aiming at personal\\ngratification of diverse objectives, and yet globally achieving what is\\neffectively the largest semantic electronic database in the world. Moreover,\\nthis database is available for all by using any search engine that can return\\naggregate page-count estimates for a large range of search-queries. In the\\npaper introducing the NWD it was called `normalized Google distance (NGD),' but\\nsince Google doesn't allow computer searches anymore, we opt for the more\\nneutral and descriptive NWD. web distance (NWD) method to determine similarity\\nbetween words and phrases. It\\n\",\n",
              " '  We examine the issue of digital formats for document encoding, archiving and\\npublishing, through the specific example of \"born-digital\" scholarly journal\\narticles. We will begin by looking at the traditional workflow of journal\\nediting and publication, and how these practices have made the transition into\\nthe online domain. We will examine the range of different file formats in which\\nelectronic articles are currently stored and published. We will argue strongly\\nthat, despite the prevalence of binary and proprietary formats such as PDF and\\nMS Word, XML is a far superior encoding choice for journal articles. Next, we\\nlook at the range of XML document structures (DTDs, Schemas) which are in\\ncommon use for encoding journal articles, and consider some of their strengths\\nand weaknesses. We will suggest that, despite the existence of specialized\\nschemas intended specifically for journal articles (such as NLM), and more\\nbroadly-used publication-oriented schemas such as DocBook, there are strong\\narguments in favour of developing a subset or customization of the Text\\nEncoding Initiative (TEI) schema for the purpose of journal-article encoding;\\nTEI is already in use in a number of journal publication projects, and the\\nscale and precision of the TEI tagset makes it particularly appropriate for\\nencoding scholarly articles. We will outline the document structure of a\\nTEI-encoded journal article, and look in detail at suggested markup patterns\\nfor specific features of journal articles.\\n',\n",
              " '  It is shown that a real novel shares many characteristic features with a null\\nmodel in which the words are randomly distributed throughout the text. Such a\\ncommon feature is a certain translational invariance of the text. Another is\\nthat the functional form of the word-frequency distribution of a novel depends\\non the length of the text in the same way as the null model. This means that an\\napproximate power-law tail ascribed to the data will have an exponent which\\nchanges with the size of the text-section which is analyzed. A further\\nconsequence is that a novel cannot be described by text-evolution models like\\nthe Simon model. The size-transformation of a novel is found to be well\\ndescribed by a specific Random Book Transformation. This size transformation in\\naddition enables a more precise determination of the functional form of the\\nword-frequency distribution. The implications of the results are discussed.\\n',\n",
              " '  We study the class of quasi-alphabetic relations, i.e., tree transformations\\ndefined by tree bimorphisms with two quasi-alphabetic tree homomorphisms and a\\nregular tree language. We present a canonical representation of these\\nrelations; as an immediate consequence, we get the closure under union. Also,\\nwe show that they are not closed under intersection and complement, and do not\\npreserve most common operations on trees (branches, subtrees, v-product,\\nv-quotient, f-top-catenation). Moreover, we prove that the translations defined\\nby quasi-alphabetic tree bimorphism are exactly products of context-free string\\nlanguages. We conclude by presenting the connections between quasi-alphabetic\\nrelations, alphabetic relations and classes of tree transformations defined by\\nseveral types of top-down tree transducers. Furthermore, we get that\\nquasi-alphabetic relations preserve the recognizable and algebraic tree\\nlanguages.\\n',\n",
              " \"  An important part of textual inference is making deductions involving\\nmonotonicity, that is, determining whether a given assertion entails\\nrestrictions or relaxations of that assertion. For instance, the statement 'We\\nknow the epidemic spread quickly' does not entail 'We know the epidemic spread\\nquickly via fleas', but 'We doubt the epidemic spread quickly' entails 'We\\ndoubt the epidemic spread quickly via fleas'. Here, we present the first\\nalgorithm for the challenging lexical-semantics problem of learning linguistic\\nconstructions that, like 'doubt', are downward entailing (DE). Our algorithm is\\nunsupervised, resource-lean, and effective, accurately recovering many DE\\noperators that are missing from the hand-constructed lists that\\ntextual-inference systems currently use.\\n\",\n",
              " '  There are many on-line settings in which users publicly express opinions. A\\nnumber of these offer mechanisms for other users to evaluate these opinions; a\\ncanonical example is Amazon.com, where reviews come with annotations like \"26\\nof 32 people found the following review helpful.\" Opinion evaluation appears in\\nmany off-line settings as well, including market research and political\\ncampaigns. Reasoning about the evaluation of an opinion is fundamentally\\ndifferent from reasoning about the opinion itself: rather than asking, \"What\\ndid Y think of X?\", we are asking, \"What did Z think of Y\\'s opinion of X?\" Here\\nwe develop a framework for analyzing and modeling opinion evaluation, using a\\nlarge-scale collection of Amazon book reviews as a dataset. We find that the\\nperceived helpfulness of a review depends not just on its content but also but\\nalso in subtle ways on how the expressed evaluation relates to other\\nevaluations of the same product. As part of our approach, we develop novel\\nmethods that take advantage of the phenomenon of review \"plagiarism\" to control\\nfor the effects of text in opinion evaluation, and we provide a simple and\\nnatural mathematical model consistent with our findings. Our analysis also\\nallows us to distinguish among the predictions of competing theories from\\nsociology and social psychology, and to discover unexpected differences in the\\ncollective opinion-evaluation behavior of user populations from different\\ncountries.\\n',\n",
              " \"  We describe a statistical model over linguistic areas and phylogeny.\\n  Our model recovers known areas and identifies a plausible hierarchy of areal\\nfeatures. The use of areas improves genetic reconstruction of languages both\\nqualitatively and quantitatively according to a variety of metrics. We model\\nlinguistic areas by a Pitman-Yor process and linguistic phylogeny by Kingman's\\ncoalescent.\\n\",\n",
              " \"  A standard form of analysis for linguistic typology is the universal\\nimplication. These implications state facts about the range of extant\\nlanguages, such as ``if objects come after verbs, then adjectives come after\\nnouns.'' Such implications are typically discovered by painstaking hand\\nanalysis over a small sample of languages. We propose a computational model for\\nassisting at this process. Our model is able to discover both well-known\\nimplications as well as some novel implications that deserve further study.\\nMoreover, through a careful application of hierarchical analysis, we are able\\nto cope with the well-known sampling problem: languages are not independent.\\n\",\n",
              " '  Current research in automatic single document summarization is dominated by\\ntwo effective, yet naive approaches: summarization by sentence extraction, and\\nheadline generation via bag-of-words models. While successful in some tasks,\\nneither of these models is able to adequately capture the large set of\\nlinguistic devices utilized by humans when they produce summaries. One possible\\nexplanation for the widespread use of these models is that good techniques have\\nbeen developed to extract appropriate training data for them from existing\\ndocument/abstract and document/headline corpora. We believe that future\\nprogress in automatic summarization will be driven both by the development of\\nmore sophisticated, linguistically informed models, as well as a more effective\\nleveraging of document/abstract corpora. In order to open the doors to\\nsimultaneously achieving both of these goals, we have developed techniques for\\nautomatically producing word-to-word and phrase-to-phrase alignments between\\ndocuments and their human-written abstracts. These alignments make explicit the\\ncorrespondences that exist in such document/abstract pairs, and create a\\npotentially rich data source from which complex summarization algorithms may\\nlearn. This paper describes experiments we have carried out to analyze the\\nability of humans to perform such alignments, and based on these analyses, we\\ndescribe experiments for creating them automatically. Our model for the\\nalignment task is based on an extension of the standard hidden Markov model,\\nand learns to create alignments in a completely unsupervised fashion. We\\ndescribe our model in detail and present experimental results that show that\\nour model is able to learn to reliably identify word- and phrase-level\\nalignments in a corpus of <document,abstract> pairs.\\n',\n",
              " '  We present a document compression system that uses a hierarchical\\nnoisy-channel model of text production. Our compression system first\\nautomatically derives the syntactic structure of each sentence and the overall\\ndiscourse structure of the text given as input. The system then uses a\\nstatistical hierarchical model of text production in order to drop\\nnon-important syntactic and discourse constituents so as to generate coherent,\\ngrammatical document compressions of arbitrary length. The system outperforms\\nboth a baseline and a sentence-based compression system that operates by\\nsimplifying sequentially all sentences in a text. Our results support the claim\\nthat discourse knowledge plays an important role in document summarization.\\n',\n",
              " '  Entity detection and tracking (EDT) is the task of identifying textual\\nmentions of real-world entities in documents, extending the named entity\\ndetection and coreference resolution task by considering mentions other than\\nnames (pronouns, definite descriptions, etc.). Like NE tagging and coreference\\nresolution, most solutions to the EDT task separate out the mention detection\\naspect from the coreference aspect. By doing so, these solutions are limited to\\nusing only local features for learning. In contrast, by modeling both aspects\\nof the EDT task simultaneously, we are able to learn using highly complex,\\nnon-local features. We develop a new joint EDT model and explore the utility of\\nmany features, demonstrating their effectiveness on this task.\\n',\n",
              " \"  We present BayeSum (for ``Bayesian summarization''), a model for sentence\\nextraction in query-focused summarization. BayeSum leverages the common case in\\nwhich multiple documents are relevant to a single query. Using these documents\\nas reinforcement for query terms, BayeSum is not afflicted by the paucity of\\ninformation in short queries. We show that approximate inference in BayeSum is\\npossible on large data sets and results in a state-of-the-art summarization\\nsystem. Furthermore, we show how BayeSum can be understood as a justified query\\nexpansion technique in the language modeling for IR framework.\\n\",\n",
              " '  In this paper, we propose a pattern-based term extraction approach for\\nJapanese, applying ACABIT system originally developed for French. The proposed\\napproach evaluates termhood using morphological patterns of basic terms and\\nterm variants. After extracting term candidates, ACABIT system filters out\\nnon-terms from the candidates based on log-likelihood. This approach is\\nsuitable for Japanese term extraction because most of Japanese terms are\\ncompound nouns or simple phrasal patterns.\\n',\n",
              " '  We present a method of automatic translation (French/English) of Complex\\nLexical Units (CLU) for aiming at extracting a bilingual lexicon. Our modular\\nsystem is based on linguistic properties (compositionality, polysemy, etc.).\\nDifferent aspects of the multilingual Web are used to validate candidate\\ntranslations and collect new terms. We first build a French corpus of Web pages\\nto collect CLU. Three adapted processing stages are applied for each linguistic\\nproperty : compositional and non polysemous translations, compositional\\npolysemous translations and non compositional translations. Our evaluation on a\\nsample of CLU shows that our technique based on the Web can reach a very high\\nprecision.\\n',\n",
              " '  This paper presents the system called PATATRAS (PATent and Article Tracking,\\nRetrieval and AnalysiS) realized for the IP track of CLEF 2009. Our approach\\npresents three main characteristics: 1. The usage of multiple retrieval models\\n(KL, Okapi) and term index definitions (lemma, phrase, concept) for the three\\nlanguages considered in the present track (English, French, German) producing\\nten different sets of ranked results. 2. The merging of the different results\\nbased on multiple regression models using an additional validation set created\\nfrom the patent collection. 3. The exploitation of patent metadata and of the\\ncitation structures for creating restricted initial working sets of patents and\\nfor producing a final re-ranking regression model. As we exploit specific\\nmetadata of the patent documents and the citation relations only at the\\ncreation of initial working sets and during the final post ranking step, our\\narchitecture remains generic and easy to extend.\\n',\n",
              " '  OLAC was founded in 2000 for creating online databases of language resources.\\nThis paper intends to review the bottom-up distributed character of the project\\nand proposes an extension of the architecture for Dravidian languages. An\\nontological structure is considered for effective natural language processing\\n(NLP) and its advantages over statistical methods are reviewed\\n',\n",
              " '  The paper reviews the hurdles while trying to implement the OLAC extension\\nfor Dravidian / Indian languages. The paper further explores the possibilities\\nwhich could minimise or solve these problems. In this context, the Chinese\\nsystem of text processing and the anusaaraka system are scrutinised.\\n',\n",
              " \"  Sandhi means to join two or more words to coin new word. Sandhi literally\\nmeans `putting together' or combining (of sounds), It denotes all combinatory\\nsound-changes effected (spontaneously) for ease of pronunciation.\\nSandhi-vicheda describes [5] the process by which one letter (whether single or\\ncojoined) is broken to form two words. Part of the broken letter remains as the\\nlast letter of the first word and part of the letter forms the first letter of\\nthe next letter. Sandhi- Vicheda is an easy and interesting way that can give\\nentirely new dimension that add new way to traditional approach to Hindi\\nTeaching. In this paper using the Rule based algorithm we have reported an\\naccuracy of 60-80% depending upon the number of rules to be implemented.\\n\",\n",
              " '  Following the principles of Cognitive Grammar, we concentrate on a model for\\nreference resolution that attempts to overcome the difficulties previous\\napproaches, based on the fundamental assumption that all reference (independent\\non the type of the referring expression) is accomplished via access to and\\nrestructuring of domains of reference rather than by direct linkage to the\\nentities themselves. The model accounts for entities not explicitly mentioned\\nbut understood in a discourse, and enables exploitation of discursive and\\nperceptual context to limit the set of potential referents for a given\\nreferring expression. As the most important feature, we note that a single\\nmechanism is required to handle what are typically treated as diverse\\nphenomena. Our approach, then, provides a fresh perspective on the relations\\nbetween Cognitive Grammar and the problem of reference.\\n',\n",
              " '  We describe an encoding scheme for discourse structure and reference, based\\non the TEI Guidelines and the recommendations of the Corpus Encoding\\nSpecification (CES). A central feature of the scheme is a CES-based data\\narchitecture enabling the encoding of and access to multiple views of a\\nmarked-up document. We describe a tool architecture that supports the encoding\\nscheme, and then show how we have used the encoding scheme and the tools to\\nperform a discourse analytic task in support of a model of global discourse\\ncohesion called Veins Theory (Cristea & Ide, 1998).\\n',\n",
              " \"  It is widely recognized that the proliferation of annotation schemes runs\\ncounter to the need to re-use language resources, and that standards for\\nlinguistic annotation are becoming increasingly mandatory. To answer this need,\\nwe have developed a framework comprised of an abstract model for a variety of\\ndifferent annotation types (e.g., morpho-syntactic tagging, syntactic\\nannotation, co-reference annotation, etc.), which can be instantiated in\\ndifferent ways depending on the annotator's approach and goals. In this paper\\nwe provide an overview of the framework, demonstrate its applicability to\\nsyntactic annotation, and show how it can contribute to comparative evaluation\\nof parser output and diverse syntactic annotation schemes.\\n\",\n",
              " '  This paper presents an abstract data model for linguistic annotations and its\\nimplementation using XML, RDF and related standards; and to outline the work of\\na newly formed committee of the International Standards Organization (ISO),\\nISO/TC 37/SC 4 Language Resource Management, which will use this work as its\\nstarting point. The primary motive for presenting the latter is to solicit the\\nparticipation of members of the research community to contribute to the work of\\nthe committee.\\n',\n",
              " '  Handwriting is an alternative method for entering texts composing Short\\nMessage Services. However, a whole new language features the texts which are\\nproduced. They include for instance abbreviations and other consonantal writing\\nwhich sprung up for time saving and fashion. We have collected and processed a\\nsignificant number of such handwriting SMS, and used various strategies to\\ntackle this challenging area of handwriting recognition. We proposed to study\\nmore specifically three different phenomena: consonant skeleton, rebus, and\\nphonetic writing. For each of them, we compare the rough results produced by a\\nstandard recognition system with those obtained when using a specific language\\nmodel.\\n',\n",
              " '  Handwriting is an alternative method for entering texts which composed Short\\nMessage Services. However, a whole new language features the texts which are\\nproduced. They include for instance abbreviations and other consonantal writing\\nwhich sprung up for time saving and fashion. We have collected and processed a\\nsignificant number of such handwritten SMS, and used various strategies to\\ntackle this challenging area of handwriting recognition. We proposed to study\\nmore specifically three different phenomena: consonant skeleton, rebus, and\\nphonetic writing. For each of them, we compare the rough results produced by a\\nstandard recognition system with those obtained when using a specific language\\nmodel to take care of them.\\n',\n",
              " '  This article proposes a method to extract dependency structures from\\nphrase-structure level parsing with Interaction Grammars. Interaction Grammars\\nare a formalism which expresses interactions among words using a polarity\\nsystem. Syntactical composition is led by the saturation of polarities.\\nInteractions take place between constituents, but as grammars are lexicalized,\\nthese interactions can be translated at the level of words. Dependency\\nrelations are extracted from the parsing process: every dependency is the\\nconsequence of a polarity saturation. The dependency relations we obtain can be\\nseen as a refinement of the usual dependency tree. Generally speaking, this\\nwork sheds new light on links between phrase structure and dependency parsing.\\n',\n",
              " \"  We present a method for grouping the synonyms of a lemma according to its\\ndictionary senses. The senses are defined by a large machine readable\\ndictionary for French, the TLFi (Tr\\\\'esor de la langue fran\\\\c{c}aise\\ninformatis\\\\'e) and the synonyms are given by 5 synonym dictionaries (also for\\nFrench). To evaluate the proposed method, we manually constructed a gold\\nstandard where for each (word, definition) pair and given the set of synonyms\\ndefined for that word by the 5 synonym dictionaries, 4 lexicographers specified\\nthe set of synonyms they judge adequate. While inter-annotator agreement ranges\\non that task from 67% to at best 88% depending on the annotator pair and on the\\nsynonym dictionary being considered, the automatic procedure we propose scores\\na precision of 67% and a recall of 71%. The proposed method is compared with\\nrelated work namely, word sense disambiguation, synonym lexicon acquisition and\\nWordNet construction.\\n\",\n",
              " '  There are many scientific problems generated by the multiple and conflicting\\nalternative definitions of linguistic recursion and human recursive processing\\nthat exist in the literature. The purpose of this article is to make available\\nto the linguistic community the standard mathematical definition of recursion\\nand to apply it to discuss linguistic recursion. As a byproduct, we obtain an\\ninsight into certain \"soft universals\" of human languages, which are related to\\ncognitive constructs necessary to implement mathematical reasoning, i.e.\\nmathematical model theory.\\n',\n",
              " '  Multimodal interfaces, combining the use of speech, graphics, gestures, and\\nfacial expressions in input and output, promise to provide new possibilities to\\ndeal with information in more effective and efficient ways, supporting for\\ninstance: - the understanding of possibly imprecise, partial or ambiguous\\nmultimodal input; - the generation of coordinated, cohesive, and coherent\\nmultimodal presentations; - the management of multimodal interaction (e.g.,\\ntask completion, adapting the interface, error prevention) by representing and\\nexploiting models of the user, the domain, the task, the interactive context,\\nand the media (e.g. text, audio, video). The present document is intended to\\nsupport the discussion on multimodal content representation, its possible\\nobjectives and basic constraints, and how the definition of a generic\\nrepresentation framework for multimodal content representation may be\\napproached. It takes into account the results of the Dagstuhl workshop, in\\nparticular those of the informal working group on multimodal meaning\\nrepresentation that was active during the workshop (see\\nhttp://www.dfki.de/~wahlster/Dagstuhl_Multi_Modality, Working Group 4).\\n',\n",
              " '  Both syntax-phonology and syntax-semantics interfaces in Higher Order Grammar\\n(HOG) are expressed as axiomatic theories in higher-order logic (HOL), i.e. a\\nlanguage is defined entirely in terms of provability in the single logical\\nsystem. An important implication of this elegant architecture is that the\\nmeaning of a valid expression turns out to be represented not by a single, nor\\neven by a few \"discrete\" terms (in case of ambiguity), but by a \"continuous\"\\nset of logically equivalent terms. The note is devoted to precise formulation\\nand proof of this observation.\\n',\n",
              " '  Proofs, in Ludics, have an interpretation provided by their counter-proofs,\\nthat is the objects they interact with. We follow the same idea by proposing\\nthat sentence meanings are given by the counter-meanings they are opposed to in\\na dialectical interaction. The conception is at the intersection of a\\nproof-theoretic and a game-theoretic accounts of semantics, but it enlarges\\nthem by allowing to deal with possibly infinite processes.\\n',\n",
              " '  Machine Translation in India is relatively young. The earliest efforts date\\nfrom the late 80s and early 90s. The success of every system is judged from its\\nevaluation experimental results. Number of machine translation systems has been\\nstarted for development but to the best of author knowledge, no high quality\\nsystem has been completed which can be used in real applications. Recently,\\nPunjabi University, Patiala, India has developed Punjabi to Hindi Machine\\ntranslation system with high accuracy of about 92%. Both the systems i.e.\\nsystem under question and developed system are between same closely related\\nlanguages. Thus, this paper presents the evaluation results of Hindi to Punjabi\\nmachine translation system. It makes sense to use same evaluation criteria as\\nthat of Punjabi to Hindi Punjabi Machine Translation System. After evaluation,\\nthe accuracy of the system is found to be about 95%.\\n',\n",
              " '  We have participated in the SENSEVAL-2 English tasks (all words and lexical\\nsample) with an unsupervised system based on mutual information measured over a\\nlarge corpus (277 million words) and some additional heuristics. A supervised\\nextension of the system was also presented to the lexical sample task.\\n  Our system scored first among unsupervised systems in both tasks: 56.9%\\nrecall in all words, 40.2% in lexical sample. This is slightly worse than the\\nfirst sense heuristic for all words and 3.6% better for the lexical sample, a\\nstrong indication that unsupervised Word Sense Disambiguation remains being a\\nstrong challenge.\\n',\n",
              " '  This paper describes a hybrid system for WSD, presented to the English\\nall-words and lexical-sample tasks, that relies on two different unsupervised\\napproaches. The first one selects the senses according to mutual information\\nproximity between a context word a variant of the sense. The second heuristic\\nanalyzes the examples of use in the glosses of the senses so that simple\\nsyntactic patterns are inferred. This patterns are matched against the\\ndisambiguation contexts. We show that the first heuristic obtains a precision\\nand recall of .58 and .35 respectively in the all words task while the second\\nobtains .80 and .25. The high precision obtained recommends deeper research of\\nthe techniques. Results for the lexical sample task are also provided.\\n',\n",
              " '  In this paper we describe a WSD experiment based on bilingual English-Spanish\\ncomparable corpora in which individual noun phrases have been identified and\\naligned with their respective counterparts in the other language. The\\nevaluation of the experiment has been carried out against SemCor.\\n  We show that, with the alignment algorithm employed, potential precision is\\nhigh (74.3%), however the coverage of the method is low (2.7%), due to\\nalignments being far less frequent than we expected.\\n  Contrary to our intuition, precision does not rise consistently with the\\nnumber of alignments. The coverage is low due to several factors; there are\\nimportant domain differences, and English and Spanish are too close languages\\nfor this approach to be able to discriminate efficiently between senses,\\nrendering it unsuitable for WSD, although the method may prove more productive\\nin machine translation.\\n',\n",
              " \"  Automated language processing is central to the drive to enable facilitated\\nreferencing of increasingly available Sanskrit E texts. The first step towards\\nprocessing Sanskrit text involves the handling of Sanskrit compound words that\\nare an integral part of Sanskrit texts. This firstly necessitates the\\nprocessing of euphonic conjunctions or sandhis, which are points in words or\\nbetween words, at which adjacent letters coalesce and transform. The ancient\\nSanskrit grammarian Panini's codification of the Sanskrit grammar is the\\naccepted authority in the subject. His famed sutras or aphorisms, numbering\\napproximately four thousand, tersely, precisely and comprehensively codify the\\nrules of the grammar, including all the rules pertaining to sandhis. This work\\npresents a fresh new approach to processing sandhis in terms of a computational\\nschema. This new computational model is based on Panini's complex codification\\nof the rules of grammar. The model has simple beginnings and is yet powerful,\\ncomprehensive and computationally lean.\\n\",\n",
              " '  Artificial Neural Network (ANN) s has widely been used for recognition of\\noptically scanned character, which partially emulates human thinking in the\\ndomain of the Artificial Intelligence. But prior to recognition, it is\\nnecessary to segment the character from the text to sentences, words etc.\\nSegmentation of words into individual letters has been one of the major\\nproblems in handwriting recognition. Despite several successful works all over\\nthe work, development of such tools in specific languages is still an ongoing\\nprocess especially in the Indian context. This work explores the application of\\nANN as an aid to segmentation of handwritten characters in Assamese- an\\nimportant language in the North Eastern part of India. The work explores the\\nperformance difference obtained in applying an ANN-based dynamic segmentation\\nalgorithm compared to projection- based static segmentation. The algorithm\\ninvolves, first training of an ANN with individual handwritten characters\\nrecorded from different individuals. Handwritten sentences are separated out\\nfrom text using a static segmentation method. From the segmented line,\\nindividual characters are separated out by first over segmenting the entire\\nline. Each of the segments thus obtained, next, is fed to the trained ANN. The\\npoint of segmentation at which the ANN recognizes a segment or a combination of\\nseveral segments to be similar to a handwritten character, a segmentation\\nboundary for the character is assumed to exist and segmentation performed. The\\nsegmented character is next compared to the best available match and the\\nsegmentation boundary confirmed.\\n',\n",
              " '  Until recently, Chinese texts could not be studied using co-word analysis\\nbecause the words are not separated by spaces in Chinese (and Japanese). A word\\ncan be composed of one or more characters. The online availability of programs\\nthat separate Chinese texts makes it possible to analyze them using semantic\\nmaps. Chinese characters contain not only information, but also meaning. This\\nmay enhance the readability of semantic maps. In this study, we analyze 58\\nwords which occur ten or more times in the 1652 journal titles of the China\\nScientific and Technical Papers and Citations Database. The word occurrence\\nmatrix is visualized and factor-analyzed.\\n',\n",
              " '  This paper presents a theoretical research based approach to ellipsis\\nresolution in machine translation. The formula of discourse is applied in order\\nto resolve ellipses. The validity of the discourse formula is analyzed by\\napplying it to the real world text, i.e., newspaper fragments. The source text\\nis converted into mono-sentential discourses where complex discourses require\\nfurther dissection either directly into primitive discourses or first into\\ncompound discourses and later into primitive ones. The procedure of dissection\\nneeds further improvement, i.e., discovering as many primitive discourse forms\\nas possible. An attempt has been made to investigate new primitive discourses\\nor patterns from the given text.\\n',\n",
              " '  This paper presents a mechanism of resolving unidentified lexical units in\\nText-based Machine Translation (TBMT). In a Machine Translation (MT) system it\\nis unlikely to have a complete lexicon and hence there is intense need of a new\\nmechanism to handle the problem of unidentified words. These unknown words\\ncould be abbreviations, names, acronyms and newly introduced terms. We have\\nproposed an algorithm for the resolution of the unidentified words. This\\nalgorithm takes discourse unit (primitive discourse) as a unit of analysis and\\nprovides real time updates to the lexicon. We have manually applied the\\nalgorithm to news paper fragments. Along with anaphora and cataphora\\nresolution, many unknown words especially names and abbreviations were updated\\nto the lexicon.\\n',\n",
              " '  The goal of this paper is two-fold: to present an abstract data model for\\nlinguistic annotations and its implementation using XML, RDF and related\\nstandards; and to outline the work of a newly formed committee of the\\nInternational Standards Organization (ISO), ISO/TC 37/SC 4 Language Resource\\nManagement, which will use this work as its starting point.\\n',\n",
              " '  We propose and compare various sentence selection strategies for active\\nlearning for the task of detecting mentions of entities. The best strategy\\nemploys the sum of confidences of two statistical classifiers trained on\\ndifferent views of the data. Our experimental results show that, compared to\\nthe random selection strategy, this strategy reduces the amount of required\\nlabeled training data by over 50% while achieving the same performance. The\\neffect is even more significant when only named mentions are considered: the\\nsystem achieves the same performance by using only 42% of the training data\\nrequired by the random selection strategy.\\n',\n",
              " '  A simple method for finding the entropy and redundancy of a reasonable long\\nsample of English text by direct computer processing and from first principles\\naccording to Shannon theory is presented. As an example, results on the entropy\\nof the English language have been obtained based on a total of 20.3 million\\ncharacters of written English, considering symbols from one to five hundred\\ncharacters in length. Besides a more realistic value of the entropy of English,\\na new perspective on some classic entropy-related concepts is presented. This\\nmethod can also be extended to other Latin languages. Some implications for\\npractical applications such as plagiarism-detection software, and the minimum\\nnumber of words that should be used in social Internet network messaging, are\\ndiscussed.\\n',\n",
              " \"  Languages evolve over time in a process in which reproduction, mutation and\\nextinction are all possible, similar to what happens to living organisms. Using\\nthis similarity it is possible, in principle, to build family trees which show\\nthe degree of relatedness between languages.\\n  The method used by modern glottochronology, developed by Swadesh in the\\n1950s, measures distances from the percentage of words with a common historical\\norigin. The weak point of this method is that subjective judgment plays a\\nrelevant role.\\n  Recently we proposed an automated method that avoids the subjectivity, whose\\nresults can be replicated by studies that use the same database and that\\ndoesn't require a specific linguistic knowledge. Moreover, the method allows a\\nquick comparison of a large number of languages.\\n  We applied our method to the Indo-European and Austronesian families,\\nconsidering in both cases, fifty different languages. The resulting trees are\\nsimilar to those of previous studies, but with some important differences in\\nthe position of few languages and subgroups. We believe that these differences\\ncarry new information on the structure of the tree and on the phylogenetic\\nrelationships within families.\\n\",\n",
              " \"  The idea of measuring distance between languages seems to have its roots in\\nthe work of the French explorer Dumont D'Urville (D'Urville 1832). He collected\\ncomparative words lists of various languages during his voyages aboard the\\nAstrolabe from 1826 to1829 and, in his work about the geographical division of\\nthe Pacific, he proposed a method to measure the degree of relation among\\nlanguages. The method used by modern glottochronology, developed by Morris\\nSwadesh in the 1950s (Swadesh 1952), measures distances from the percentage of\\nshared cognates, which are words with a common historical origin. Recently, we\\nproposed a new automated method which uses normalized Levenshtein distance\\namong words with the same meaning and averages on the words contained in a\\nlist. Another classical problem in glottochronology is the study of the\\nstability of words corresponding to different meanings. Words, in fact, evolve\\nbecause of lexical changes, borrowings and replacement at a rate which is not\\nthe same for all of them. The speed of lexical evolution is different for\\ndifferent meanings and it is probably related to the frequency of use of the\\nassociated words (Pagel et al. 2007). This problem is tackled here by an\\nautomated methodology only based on normalized Levenshtein distance.\\n\",\n",
              " '  Co-words have been considered as carriers of meaning across different domains\\nin studies of science, technology, and society. Words and co-words, however,\\nobtain meaning in sentences, and sentences obtain meaning in their contexts of\\nuse. At the science/society interface, words can be expected to have different\\nmeanings: the codes of communication that provide meaning to words differ on\\nthe varying sides of the interface. Furthermore, meanings and interfaces may\\nchange over time. Given this structuring of meaning across interfaces and over\\ntime, we distinguish between metaphors and diaphors as reflexive mechanisms\\nthat facilitate the translation between contexts. Our empirical focus is on\\nthree recent scientific controversies: Monarch butterflies, Frankenfoods, and\\nstem-cell therapies. This study explores new avenues that relate the study of\\nco-word analysis in context with the sociological quest for the analysis and\\nprocessing of meaning.\\n',\n",
              " '  A survey of dictionary models and formats is presented as well as a\\npresentation of corresponding recent standardisation activities.\\n',\n",
              " '  This paper is about automatic acquisition of lexical information from\\ncorpora, especially subcategorization acquisition.\\n',\n",
              " '  A dictionary defines words in terms of other words. Definitions can tell you\\nthe meanings of words you don\\'t know, but only if you know the meanings of the\\ndefining words. How many words do you need to know (and which ones) in order to\\nbe able to learn all the rest from definitions? We reduced dictionaries to\\ntheir \"grounding kernels\" (GKs), about 10% of the dictionary, from which all\\nthe other words could be defined. The GK words turned out to have\\npsycholinguistic correlates: they were learned at an earlier age and more\\nconcrete than the rest of the dictionary. But one can compress still more: the\\nGK turns out to have internal structure, with a strongly connected \"kernel\\ncore\" (KC) and a surrounding layer, from which a hierarchy of definitional\\ndistances can be derived, all the way out to the periphery of the full\\ndictionary. These definitional distances, too, are correlated with\\npsycholinguistic variables (age of acquisition, concreteness, imageability,\\noral and written frequency) and hence perhaps with the \"mental lexicon\" in each\\nof our heads.\\n',\n",
              " '  Phylogenetic trees can be reconstructed from the matrix which contains the\\ndistances between all pairs of languages in a family. Recently, we proposed a\\nnew method which uses normalized Levenshtein distances among words with same\\nmeaning and averages on all the items of a given list. Decisions about the\\nnumber of items in the input lists for language comparison have been debated\\nsince the beginning of glottochronology. The point is that words associated to\\nsome of the meanings have a rapid lexical evolution. Therefore, a large\\nvocabulary comparison is only apparently more accurate then a smaller one since\\nmany of the words do not carry any useful information. In principle, one should\\nfind the optimal length of the input lists studying the stability of the\\ndifferent items. In this paper we tackle the problem with an automated\\nmethodology only based on our normalized Levenshtein distance. With this\\napproach, the program of an automated reconstruction of languages relationships\\nis completed.\\n',\n",
              " \"  The idea of measuring distance between languages seems to have its roots in\\nthe work of the French explorer Dumont D'Urville \\\\cite{Urv}. He collected\\ncomparative words lists of various languages during his voyages aboard the\\nAstrolabe from 1826 to 1829 and, in his work about the geographical division of\\nthe Pacific, he proposed a method to measure the degree of relation among\\nlanguages. The method used by modern glottochronology, developed by Morris\\nSwadesh in the 1950s, measures distances from the percentage of shared\\ncognates, which are words with a common historical origin. Recently, we\\nproposed a new automated method which uses normalized Levenshtein distance\\namong words with the same meaning and averages on the words contained in a\\nlist. Recently another group of scholars \\\\cite{Bak, Hol} proposed a refined of\\nour definition including a second normalization. In this paper we compare the\\ninformation content of our definition with the refined version in order to\\ndecide which of the two can be applied with greater success to resolve\\nrelationships among languages.\\n\",\n",
              " '  A natural language (or ordinary language) is a language that is spoken,\\nwritten, or signed by humans for general-purpose communication, as\\ndistinguished from formal languages (such as computer-programming languages or\\nthe \"languages\" used in the study of formal logic). The computational\\nactivities required for enabling a computer to carry out information processing\\nusing natural language is called natural language processing. We have taken\\nAssamese language to check the grammars of the input sentence. Our aim is to\\nproduce a technique to check the grammatical structures of the sentences in\\nAssamese text. We have made grammar rules by analyzing the structures of\\nAssamese sentences. Our parsing program finds the grammatical errors, if any,\\nin the Assamese sentence. If there is no error, the program will generate the\\nparse tree for the Assamese sentence\\n',\n",
              " '  In this chapter we present the main issues in representing machine readable\\ndictionaries in XML, and in particular according to the Text Encoding\\nDictionary (TEI) guidelines.\\n',\n",
              " '  Paraphrasing methods recognize, generate, or extract phrases, sentences, or\\nlonger natural language expressions that convey almost the same information.\\nTextual entailment methods, on the other hand, recognize, generate, or extract\\npairs of natural language expressions, such that a human who reads (and trusts)\\nthe first element of a pair would most likely infer that the other element is\\nalso true. Paraphrasing can be seen as bidirectional textual entailment and\\nmethods from the two areas are often similar. Both kinds of methods are useful,\\nat least in principle, in a wide range of natural language processing\\napplications, including question answering, summarization, text generation, and\\nmachine translation. We summarize key ideas from the two areas by considering\\nin turn recognition, generation, and extraction methods, also pointing to\\nprominent articles and resources.\\n',\n",
              " '  The recent resurgence of interest in spatio-temporal neural network as speech\\nrecognition tool motivates the present investigation. In this paper an approach\\nwas developed based on temporal radial basis function \"TRBF\" looking to many\\nadvantages: few parameters, speed convergence and time invariance. This\\napplication aims to identify vowels taken from natural speech samples from the\\nTimit corpus of American speech. We report a recognition accuracy of 98.06\\npercent in training and 90.13 in test on a subset of 6 vowel phonemes, with the\\npossibility to expend the vowel sets in future.\\n',\n",
              " '  In recent decades, Speech interactive systems gained increasing importance.\\nTo develop Dictation System like Dragon for Indian languages it is most\\nimportant to adapt the system to a speaker with minimum training. In this paper\\nwe focus on the importance of creating speech database at syllable units and\\nidentifying minimum text to be considered while training any speech recognition\\nsystem. There are systems developed for continuous speech recognition in\\nEnglish and in few Indian languages like Hindi and Tamil. This paper gives the\\nstatistical details of syllables in Telugu and its use in minimizing the search\\nspace during recognition of speech. The minimum words that cover maximum\\nsyllables are identified. This words list can be used for preparing a small\\ntext which can be used for collecting speech sample while training the\\ndictation system. The results are plotted for frequency of syllables and the\\nnumber of syllables in each word. This approach is applied on the CIIL Mysore\\ntext corpus which is of 3 million words.\\n',\n",
              " '  This paper presents a brief survey on Automatic Speech Recognition and\\ndiscusses the major themes and advances made in the past 60 years of research,\\nso as to provide a technological perspective and an appreciation of the\\nfundamental progress that has been accomplished in this important area of\\nspeech communication. After years of research and development the accuracy of\\nautomatic speech recognition remains one of the important research challenges\\n(e.g., variations of the context, speakers, and environment).The design of\\nSpeech Recognition system requires careful attentions to the following issues:\\nDefinition of various types of speech classes, speech representation, feature\\nextraction techniques, speech classifiers, database and performance evaluation.\\nThe problems that are existing in ASR and the various techniques to solve these\\nproblems constructed by various research workers have been presented in a\\nchronological order. Hence authors hope that this work shall be a contribution\\nin the area of speech recognition. The objective of this review paper is to\\nsummarize and compare some of the well known methods used in various stages of\\nspeech recognition system and identify research topic and applications which\\nare at the forefront of this exciting and challenging field.\\n',\n",
              " '  Accurate systems for extracting Protein-Protein Interactions (PPIs)\\nautomatically from biomedical articles can help accelerate biomedical research.\\nBiomedical Informatics researchers are collaborating to provide metaservices\\nand advance the state-of-art in PPI extraction. One problem often neglected by\\ncurrent Natural Language Processing systems is the characteristic complexity of\\nthe sentences in biomedical literature. In this paper, we report on the impact\\nthat automatic simplification of sentences has on the performance of a\\nstate-of-art PPI extraction system, showing a substantial improvement in recall\\n(8%) when the sentence simplification method is applied, without significant\\nimpact to precision.\\n',\n",
              " '  The complexity of sentences characteristic to biomedical articles poses a\\nchallenge to natural language parsers, which are typically trained on\\nlarge-scale corpora of non-technical text. We propose a text simplification\\nprocess, bioSimplify, that seeks to reduce the complexity of sentences in\\nbiomedical abstracts in order to improve the performance of syntactic parsers\\non the processed sentences. Syntactic parsing is typically one of the first\\nsteps in a text mining pipeline. Thus, any improvement in performance would\\nhave a ripple effect over all processing steps. We evaluated our method using a\\ncorpus of biomedical sentences annotated with syntactic links. Our empirical\\nresults show an improvement of 2.90% for the Charniak-McClosky parser and of\\n4.23% for the Link Grammar parser when processing simplified sentences rather\\nthan the original sentences in the corpus.\\n',\n",
              " \"  In this article, we record the main linguistic differences or singularities\\nof 17th century English, analyse them morphologically and syntactically and\\npropose equivalent forms in contemporary English. We show how 17th century\\ntexts may be transcribed into modern English, combining the use of electronic\\ndictionaries with rules of transcription implemented as transducers. Apr\\\\`es\\navoir expos\\\\'e la constitution du corpus, nous recensons les principales\\ndiff\\\\'erences ou particularit\\\\'es linguistiques de la langue anglaise du XVIIe\\nsi\\\\`ecle, les analysons du point de vue morphologique et syntaxique et\\nproposons des \\\\'equivalents en anglais contemporain (AC). Nous montrons comment\\nnous pouvons effectuer une transcription automatique de textes anglais du XVIIe\\nsi\\\\`ecle en anglais moderne, en combinant l'utilisation de dictionnaires\\n\\\\'electroniques avec des r\\\\`egles de transcriptions impl\\\\'ement\\\\'ees sous forme\\nde transducteurs.\\n\",\n",
              " '  If the use of the apostrophe in contemporary English often marks the Saxon\\ngenitive, it may also indicate the omission of one or more let-ters. Some\\nwriters (wrongly?) use it to mark the plural in symbols or abbreviations,\\nvisual-ised thanks to the isolation of the morpheme \"s\". This punctuation mark\\nwas imported from the Continent in the 16th century. During the 19th century\\nits use was standardised. However the rules of its usage still seem problematic\\nto many, including literate speakers of English. \"All too often, the apostrophe\\nis misplaced\", or \"errant apostrophes are springing up every-where\" is a\\ncomplaint that Internet users fre-quently come across when visiting grammar\\nwebsites. Many of them detail its various uses and misuses, and attempt to\\ncorrect the most common mistakes about it, especially its mis-use in the\\nplural, called greengrocers\\' apostro-phes and humorously misspelled\\n\"greengro-cers apostrophe\\'s\". While studying English travel accounts published\\nin the seventeenth century, we noticed that the different uses of this symbol\\nmay accompany various models of metaplasms. We were able to highlight the\\nlinguistic variations of some lexemes, and trace the origin of modern grammar\\nrules gov-erning its usage.\\n',\n",
              " \"  The recognition of Arabic Named Entities (NE) is a problem in different\\ndomains of Natural Language Processing (NLP) like automatic translation.\\nIndeed, NE translation allows the access to multilingual in-formation. This\\ntranslation doesn't always lead to expected result especially when NE contains\\na person name. For this reason and in order to ameliorate translation, we can\\ntransliterate some part of NE. In this context, we propose a method that\\nintegrates translation and transliteration together. We used the linguis-tic\\nNooJ platform that is based on local grammars and transducers. In this paper,\\nwe focus on sport domain. We will firstly suggest a refinement of the\\ntypological model presented at the MUC Conferences we will describe the\\nintegration of an Arabic transliteration module into translation system.\\nFinally, we will detail our method and give the results of the evaluation.\\n\",\n",
              " '  We are developing electronic dictionaries and transducers for the automatic\\nprocessing of the Albanian Language. We will analyze the words inside a linear\\nsegment of text. We will also study the relationship between units of sense and\\nunits of form. The composition of words takes different forms in Albanian. We\\nhave found that morphemes are frequently concatenated or simply juxtaposed or\\ncontracted. The inflected grammar of NooJ allows constructing the dictionaries\\nof flexed forms (declensions or conjugations). The diversity of word structures\\nrequires tools to identify words created by simple concatenation, or to treat\\ncontractions. The morphological tools of NooJ allow us to create grammatical\\ntools to represent and treat these phenomena. But certain problems exceed the\\nmorphological analysis and must be represented by syntactical grammars.\\n',\n",
              " \"  Maximum mutual information (MMI) is a model selection criterion used for\\nhidden Markov model (HMM) parameter estimation that was developed more than\\ntwenty years ago as a discriminative alternative to the maximum likelihood\\ncriterion for HMM-based speech recognition. It has been shown in the speech\\nrecognition literature that parameter estimation using the current MMI\\nparadigm, lattice-based MMI, consistently outperforms maximum likelihood\\nestimation, but this is at the expense of undesirable convergence properties.\\nIn particular, recognition performance is sensitive to the number of times that\\nthe iterative MMI estimation algorithm, extended Baum-Welch, is performed. In\\nfact, too many iterations of extended Baum-Welch will lead to degraded\\nperformance, despite the fact that the MMI criterion improves at each\\niteration. This phenomenon is at variance with the analogous behavior of\\nmaximum likelihood estimation -- at least for the HMMs used in speech\\nrecognition -- and it has previously been attributed to `over fitting'. In this\\npaper, we present an analysis of lattice-based MMI that demonstrates, first of\\nall, that the asymptotic behavior of lattice-based MMI is much worse than was\\npreviously understood, i.e. it does not appear to converge at all, and, second\\nof all, that this is not due to `over fitting'. Instead, we demonstrate that\\nthe `over fitting' phenomenon is the result of standard methodology that\\nexacerbates the poor behavior of two key approximations in the lattice-based\\nMMI machinery. We also demonstrate that if we modify the standard methodology\\nto improve the validity of these approximations, then the convergence\\nproperties of lattice-based MMI become benign without sacrificing improvements\\nto recognition accuracy.\\n\",\n",
              " '  Using Pustejovsky\\'s \"The Syntax of Event Structure\" and Fong\\'s \"On Mending a\\nTorn Dress\" we give a glimpse of a Pustejovsky-like analysis to some example\\nsentences in Fong. We attempt to give a framework for semantics to the noun\\nphrases and adverbs as appropriate as well as the lexical entries for all words\\nin the examples and critique both papers in light of our findings and\\ndifficulties.\\n',\n",
              " '  This document discusses an approach and its rudimentary realization towards\\nautomatic classification of PPs; the topic, that has not received as much\\nattention in NLP as NPs and VPs. The approach is a rule-based heuristics\\noutlined in several levels of our research. There are 7 semantic categories of\\nPPs considered in this document that we are able to classify from an annotated\\ncorpus.\\n',\n",
              " '  Rhetorical structure analysis (RSA) explores discourse relations among\\nelementary discourse units (EDUs) in a text. It is very useful in many text\\nprocessing tasks employing relationships among EDUs such as text understanding,\\nsummarization, and question-answering. Thai language with its distinctive\\nlinguistic characteristics requires a unique technique. This article proposes\\nan approach for Thai rhetorical structure analysis. First, EDUs are segmented\\nby two hidden Markov models derived from syntactic rules. A rhetorical\\nstructure tree is constructed from a clustering technique with its similarity\\nmeasure derived from Thai semantic rules. Then, a decision tree whose features\\nderived from the semantic rules is used to determine discourse relations.\\n',\n",
              " '  Combined with space-time coding, the orthogonal frequency division\\nmultiplexing (OFDM) system explores space diversity. It is a potential scheme\\nto offer spectral efficiency and robust high data rate transmissions over\\nfrequency-selective fading channel. However, space-time coding impairs the\\nsystem ability to suppress interferences as the signals transmitted from two\\ntransmit antennas are superposed and interfered at the receiver antennas. In\\nthis paper, we developed an adaptive beamforming based on least mean squared\\nerror algorithm and null deepening to combat co-channel interference (CCI) for\\nthe space-time coded OFDM (STC-OFDM) system. To illustrate the performance of\\nthe presented approach, it is compared to the null steering beamformer which\\nrequires a prior knowledge of directions of arrival (DOAs). The structure of\\nspace-time decoders are preserved although there is the use of beamformers\\nbefore decoding. By incorporating the proposed beamformer as a CCI canceller in\\nthe STC-OFDM systems, the performance improvement is achieved as shown in the\\nsimulation results.\\n',\n",
              " '  The syntactic topic model (STM) is a Bayesian nonparametric model of language\\nthat discovers latent distributions of words (topics) that are both\\nsemantically and syntactically coherent. The STM models dependency parsed\\ncorpora where sentences are grouped into documents. It assumes that each word\\nis drawn from a latent topic chosen by combining document-level features and\\nthe local syntactic context. Each document has a distribution over latent\\ntopics, as in topic models, which provides the semantic consistency. Each\\nelement in the dependency parse tree also has a distribution over the topics of\\nits children, as in latent-state syntax models, which provides the syntactic\\nconsistency. These distributions are convolved so that the topic of each word\\nis likely under both its document and syntactic context. We derive a fast\\nposterior inference algorithm based on variational methods. We report\\nqualitative and quantitative studies on both synthetic data and hand-parsed\\ndocuments. We show that the STM is a more predictive model of language than\\ncurrent models based only on syntax or only on topics.\\n',\n",
              " \"  This article presents SLAM, an Automatic Solver for Lexical Metaphors like\\n?d\\\\'eshabiller* une pomme? (to undress* an apple). SLAM calculates a\\nconventional solution for these productions. To carry on it, SLAM has to\\nintersect the paradigmatic axis of the metaphorical verb ?d\\\\'eshabiller*?,\\nwhere ?peler? (?to peel?) comes closer, with a syntagmatic axis that comes from\\na corpus where ?peler une pomme? (to peel an apple) is semantically and\\nsyntactically regular. We test this model on DicoSyn, which is a ?small world?\\nnetwork of synonyms, to compute the paradigmatic axis and on Frantext.20, a\\nFrench corpus, to compute the syntagmatic axis. Further, we evaluate the model\\nwith a sample of an experimental corpus of the database of Flexsem\\n\",\n",
              " \"  Hidden Markov models (HMMs) have been successfully applied to automatic\\nspeech recognition for more than 35 years in spite of the fact that a key HMM\\nassumption -- the statistical independence of frames -- is obviously violated\\nby speech data. In fact, this data/model mismatch has inspired many attempts to\\nmodify or replace HMMs with alternative models that are better able to take\\ninto account the statistical dependence of frames. However it is fair to say\\nthat in 2010 the HMM is the consensus model of choice for speech recognition\\nand that HMMs are at the heart of both commercially available products and\\ncontemporary research systems. In this paper we present a preliminary\\nexploration aimed at understanding how speech data depart from HMMs and what\\neffect this departure has on the accuracy of HMM-based speech recognition. Our\\nanalysis uses standard diagnostic tools from the field of statistics --\\nhypothesis testing, simulation and resampling -- which are rarely used in the\\nfield of speech recognition. Our main result, obtained by novel manipulations\\nof real and resampled data, demonstrates that real data have statistical\\ndependency and that this dependency is responsible for significant numbers of\\nrecognition errors. We also demonstrate, using simulation and resampling, that\\nif we `remove' the statistical dependency from data, then the resulting\\nrecognition error rates become negligible. Taken together, these results\\nsuggest that a better understanding of the structure of the statistical\\ndependency in speech data is a crucial first step towards improving HMM-based\\nspeech recognition.\\n\",\n",
              " \"  The article provides lexical statistical analysis of K. Vonnegut's two novels\\nand their Russian translations. It is found out that there happen some changes\\nbetween the speed of word types and word tokens ratio change in the source and\\ntarget texts. The author hypothesizes that these changes are typical for\\nEnglish-Russian translations, and moreover, they represent an example of\\nBaker's translation feature of levelling out.\\n\",\n",
              " '  Text documents are complex high dimensional objects. To effectively visualize\\nsuch data it is important to reduce its dimensionality and visualize the low\\ndimensional embedding as a 2-D or 3-D scatter plot. In this paper we explore\\ndimensionality reduction methods that draw upon domain knowledge in order to\\nachieve a better low dimensional embedding and visualization of documents. We\\nconsider the use of geometries specified manually by an expert, geometries\\nderived automatically from corpus statistics, and geometries computed from\\nlinguistic resources.\\n',\n",
              " '  Computers understand very little of the meaning of human language. This\\nprofoundly limits our ability to give instructions to computers, the ability of\\ncomputers to explain their actions to us, and the ability of computers to\\nanalyse and process text. Vector space models (VSMs) of semantics are beginning\\nto address these limits. This paper surveys the use of VSMs for semantic\\nprocessing of text. We organize the literature on VSMs according to the\\nstructure of the matrix in a VSM. There are currently three broad classes of\\nVSMs, based on term-document, word-context, and pair-pattern matrices, yielding\\nthree classes of applications. We survey a broad range of applications in these\\nthree categories and we take a detailed look at a specific open source project\\nin each category. Our goal in this survey is to show the breadth of\\napplications of VSMs for semantics, to provide a new perspective on VSMs for\\nthose who are already familiar with the area, and to provide pointers into the\\nliterature for those who are less familiar with the field.\\n',\n",
              " '  Developers express the meaning of the domain ideas in specifically selected\\nidentifiers and comments that form the target implemented code. Software\\nmaintenance requires knowledge and understanding of the encoded ideas. This\\npaper presents a way how to create automatically domain vocabulary. Knowledge\\nof domain vocabulary supports the comprehension of a specific domain for later\\ncode maintenance or evolution. We present experiments conducted in two selected\\ndomains: application servers and web frameworks. Knowledge of domain terms\\nenables easy localization of chunks of code that belong to a certain term. We\\nconsider these chunks of code as \"concepts\" and their placement in the code as\\n\"concept location\". Application developers may also benefit from the obtained\\ndomain terms. These terms are parts of speech that characterize a certain\\nconcept. Concepts are encoded in \"classes\" (OO paradigm) and the obtained\\nvocabulary of terms supports the selection and the comprehension of the class\\'\\nappropriate identifiers. We measured the following software products with our\\ntool: JBoss, JOnAS, GlassFish, Tapestry, Google Web Toolkit and Echo2.\\n',\n",
              " '  Poetry-writing in Sanskrit is riddled with problems for even those who know\\nthe language well. This is so because the rules that govern Sanskrit prosody\\nare numerous and stringent. We propose a computational algorithm that converts\\nprose given as E-text into poetry in accordance with the metrical rules of\\nSanskrit prosody, simultaneously taking care to ensure that sandhi or euphonic\\nconjunction, which is compulsory in verse, is handled. The algorithm is\\nconsiderably speeded up by a novel method of reducing the target search\\ndatabase. The algorithm further gives suggestions to the poet in case what\\nhe/she has given as the input prose is impossible to fit into any allowed\\nmetrical format. There is also an interactive component of the algorithm by\\nwhich the algorithm interacts with the poet to resolve ambiguities. In\\naddition, this unique work, which provides a solution to a problem that has\\nnever been addressed before, provides a simple yet effective speech recognition\\ninterface that would help the visually impaired dictate words in E-text, which\\nis in turn versified by our Poetry Composer Engine.\\n',\n",
              " '  The recognition and classification of Named Entities (NER) are regarded as an\\nimportant component for many Natural Language Processing (NLP) applications.\\nThe classification is usually made by taking into account the immediate context\\nin which the NE appears. In some cases, this immediate context does not allow\\ngetting the right classification. We show in this paper that the use of an\\nextended syntactic context and large-scale resources could be very useful in\\nthe NER task.\\n',\n",
              " \"  We propose a mathematical framework for a unification of the distributional\\ntheory of meaning in terms of vector space models, and a compositional theory\\nfor grammatical types, for which we rely on the algebra of Pregroups,\\nintroduced by Lambek. This mathematical framework enables us to compute the\\nmeaning of a well-typed sentence from the meanings of its constituents.\\nConcretely, the type reductions of Pregroups are `lifted' to morphisms in a\\ncategory, a procedure that transforms meanings of constituents into a meaning\\nof the (well-typed) whole. Importantly, meanings of whole sentences live in a\\nsingle space, independent of the grammatical structure of the sentence. Hence\\nthe inner-product can be used to compare meanings of arbitrary sentences, as it\\nis for comparing the meanings of words in the distributional model. The\\nmathematical structure we employ admits a purely diagrammatic calculus which\\nexposes how the information flows between the words in a sentence in order to\\nmake up the meaning of the whole sentence. A variation of our `categorical\\nmodel' which involves constraining the scalars of the vector spaces to the\\nsemiring of Booleans results in a Montague-style Boolean-valued semantics.\\n\",\n",
              " \"  In this chapter, we assume that systematically studying spatial markers\\nsemantics in language provides a means to reveal fundamental properties and\\nconcepts characterizing conceptual representations of space. We propose a\\nformal system accounting for the properties highlighted by the linguistic\\nanalysis, and we use these tools for representing the semantic content of\\nseveral spatial relations of French. The first part presents a semantic\\nanalysis of the expression of space in French aiming at describing the\\nconstraints that formal representations have to take into account. In the\\nsecond part, after presenting the structure of our formal system, we set out\\nits components. A commonsense geometry is sketched out and several functional\\nand pragmatic spatial concepts are formalized. We take a special attention in\\nshowing that these concepts are well suited to representing the semantic\\ncontent of several prepositions of French ('sur' (on), 'dans' (in), 'devant'\\n(in front of), 'au-dessus' (above)), and in illustrating the inferential\\nadequacy of these representations.\\n\",\n",
              " \"  While previous linguistic and psycholinguistic research on space has mainly\\nanalyzed spatial relations, the studies reported in this paper focus on how\\nlanguage distinguishes among spatial entities. Descriptive and experimental\\nstudies first propose a classification of entities, which accounts for both\\nstatic and dynamic space, has some cross-linguistic validity, and underlies\\nadults' cognitive processing. Formal and computational analyses then introduce\\ntheoretical elements aiming at modelling these categories, while fulfilling\\nvarious properties of formal ontologies (generality, parsimony, coherence...).\\nThis formal framework accounts, in particular, for functional dependences among\\nentities underlying some part-whole descriptions. Finally, developmental\\nresearch shows that language-specific properties have a clear impact on how\\nchildren talk about space. The results suggest some cross-linguistic\\nvariability in children's spatial representations from an early age onwards,\\nbringing into question models in which general cognitive capacities are the\\nonly determinants of spatial cognition during the course of development.\\n\",\n",
              " '  Automatically detecting discourse segments is an important preliminary step\\ntowards full discourse parsing. Previous research on discourse segmentation\\nhave relied on the assumption that elementary discourse units (EDUs) in a\\ndocument always form a linear sequence (i.e., they can never be nested).\\nUnfortunately, this assumption turns out to be too strong, for some theories of\\ndiscourse like SDRT allows for nested discourse units. In this paper, we\\npresent a simple approach to discourse segmentation that is able to produce\\nnested EDUs. Our approach builds on standard multi-class classification\\ntechniques combined with a simple repairing heuristic that enforces global\\ncoherence. Our system was developed and evaluated on the first round of\\nannotations provided by the French Annodis project (an ongoing effort to create\\na discourse bank for French). Cross-validated on only 47 documents (1,445\\nEDUs), our system achieves encouraging performance results with an F-score of\\n73% for finding EDUs.\\n',\n",
              " '  This paper has been withdrawn by the author.\\n',\n",
              " '  The Lambek calculus provides a foundation for categorial grammar in the form\\nof a logic of concatenation. But natural language is characterized by\\ndependencies which may also be discontinuous. In this paper we introduce the\\ndisplacement calculus, a generalization of Lambek calculus, which preserves its\\ngood proof-theoretic properties while embracing discontinuiity and subsuming\\nit. We illustrate linguistic applications and prove Cut-elimination, the\\nsubformula property, and decidability\\n',\n",
              " '  A statistical physics study of punctuation effects on sentence lengths is\\npresented for written texts: {\\\\it Alice in wonderland} and {\\\\it Through a\\nlooking glass}. The translation of the first text into esperanto is also\\nconsidered as a test for the role of punctuation in defining a style, and for\\ncontrasting natural and artificial, but written, languages. Several log-log\\nplots of the sentence length-rank relationship are presented for the major\\npunctuation marks. Different power laws are observed with characteristic\\nexponents. The exponent can take a value much less than unity ($ca.$ 0.50 or\\n0.30) depending on how a sentence is defined. The texts are also mapped into\\ntime series based on the word frequencies. The quantitative differences between\\nthe original and translated texts are very minutes, at the exponent level. It\\nis argued that sentences seem to be more reliable than word distributions in\\ndiscussing an author style.\\n',\n",
              " '  This paper describes in details the first version of Morphonette, a new\\nFrench morphological resource and a new radically lexeme-based method of\\nmorphological analysis. This research is grounded in a paradigmatic conception\\nof derivational morphology where the morphological structure is a structure of\\nthe entire lexicon and not one of the individual words it contains. The\\ndiscovery of this structure relies on a measure of morphological similarity\\nbetween words, on formal analogy and on the properties of two morphological\\nparadigms:\\n',\n",
              " '  The Lambek-Grishin calculus LG is the symmetric extension of the\\nnon-associative Lambek calculus NL. In this paper we prove that the\\nderivability problem for LG is NP-complete.\\n',\n",
              " '  Archaeological excavations in the sites of the Indus Valley civilization\\n(2500-1900 BCE) in Pakistan and northwestern India have unearthed a large\\nnumber of artifacts with inscriptions made up of hundreds of distinct signs. To\\ndate there is no generally accepted decipherment of these sign sequences and\\nthere have been suggestions that the signs could be non-linguistic. Here we\\napply complex network analysis techniques to a database of available Indus\\ninscriptions, with the aim of detecting patterns indicative of syntactic\\norganization. Our results show the presence of patterns, e.g., recursive\\nstructures in the segmentation trees of the sequences, that suggest the\\nexistence of a grammar underlying these inscriptions.\\n',\n",
              " \"  The contribution of this paper is to provide a semantic model (using soft\\nconstraints) of the words used by web-users to describe objects in a language\\ngame; a game in which one user describes a selected object of those composing\\nthe scene, and another user has to guess which object has been described. The\\ngiven description needs to be non ambiguous and accurate enough to allow other\\nusers to guess the described shape correctly.\\n  To build these semantic models the descriptions need to be analyzed to\\nextract the syntax and words' classes used. We have modeled the meaning of\\nthese descriptions using soft constraints as a way for grounding the meaning.\\n  The descriptions generated by the system took into account the context of the\\nobject to avoid ambiguous descriptions, and allowed users to guess the\\ndescribed object correctly 72% of the times.\\n\",\n",
              " \"  In the article, the project of quantitative parametrization of all texts by\\nIvan Franko is manifested. It can be made only by using modern computer\\ntechniques after the frequency dictionaries for all Franko's works are\\ncompiled. The paper describes the application spheres, methodology, stages,\\nprinciples and peculiarities in the compilation of the frequency dictionary of\\nthe second half of the 19th century - the beginning of the 20th century. The\\nrelation between the Ivan Franko frequency dictionary, explanatory dictionary\\nof writer's language and text corpus is discussed.\\n\",\n",
              " '  Lexicon-Grammar tables constitute a large-coverage syntactic lexicon but they\\ncannot be directly used in Natural Language Processing (NLP) applications\\nbecause they sometimes rely on implicit information. In this paper, we\\nintroduce LGExtract, a generic tool for generating a syntactic lexicon for NLP\\nfrom the Lexicon-Grammar tables. It is based on a global table that contains\\nundefined information and on a unique extraction script including all\\noperations to be performed for all tables. We also present an experiment that\\nhas been conducted to generate a new lexicon of French verbs and predicative\\nnouns.\\n',\n",
              " \"  In the article, the methodology and the principles of the compilation of the\\nFrequency dictionary for Ivan Franko's novel Dlja domashnjoho ohnyshcha (For\\nthe Hearth) are described. The following statistical parameters of the novel\\nvocabulary are obtained: variety, exclusiveness, concentration indexes,\\ncorrelation between word rank and text coverage, etc. The main quantitative\\ncharacteristics of Franko's novels Perekhresni stezhky (The Cross-Paths) and\\nDlja domashnjoho ohnyshcha are compared on the basis of their frequency\\ndictionaries.\\n\",\n",
              " '  The Lady Maisry ballads afford us a framework within which to segment a\\nstoryline into its major components. Segments and as a consequence nodal points\\nare discussed for nine different variants of the Lady Maisry story of a (young)\\nwoman being burnt to death by her family, on account of her becoming pregnant\\nby a foreign personage. We motivate the importance of nodal points in textual\\nand literary analysis. We show too how the openings of the nine variants can be\\nanalyzed comparatively, and also the conclusions of the ballads.\\n',\n",
              " \"  The ambition of a character recognition system is to transform a text\\ndocument typed on paper into a digital format that can be manipulated by word\\nprocessor software Unlike other languages, Arabic has unique features, while\\nother language doesn't have, from this language these are seven or eight\\nlanguage such as ordo, jewie and Persian writing, Arabic has twenty eight\\nletters, each of which can be linked in three different ways or separated\\ndepending on the case. The difficulty of the Arabic handwriting recognition is\\nthat, the accuracy of the character recognition which affects on the accuracy\\nof the word recognition, in additional there is also two or three from for each\\ncharacter, the suggested solution by using artificial neural network can solve\\nthe problem and overcome the difficulty of Arabic handwriting recognition.\\n\",\n",
              " '  Indian languages have long history in World Natural languages. Panini was the\\nfirst to define Grammar for Sanskrit language with about 4000 rules in fifth\\ncentury. These rules contain uncertainty information. It is not possible to\\nComputer processing of Sanskrit language with uncertain information. In this\\npaper, fuzzy logic and fuzzy reasoning are proposed to deal to eliminate\\nuncertain information for reasoning with Sanskrit grammar. The Sanskrit\\nlanguage processing is also discussed in this paper.\\n',\n",
              " '  There is much debate over the degree to which language learning is governed\\nby innate language-specific biases, or acquired through cognition-general\\nprinciples. Here we examine the probabilistic language acquisition hypothesis\\non three levels: We outline a novel theoretical result showing that it is\\npossible to learn the exact generative model underlying a wide class of\\nlanguages, purely from observing samples of the language. We then describe a\\nrecently proposed practical framework, which quantifies natural language\\nlearnability, allowing specific learnability predictions to be made for the\\nfirst time. In previous work, this framework was used to make learnability\\npredictions for a wide variety of linguistic constructions, for which\\nlearnability has been much debated. Here, we present a new experiment which\\ntests these learnability predictions. We find that our experimental results\\nsupport the possibility that these linguistic constructions are acquired\\nprobabilistically from cognition-general principles.\\n',\n",
              " \"  This companion paper complements the main DEFT'10 article describing the MARF\\napproach (arXiv:0905.1235) to the DEFT'10 NLP challenge (described at\\nhttp://www.groupes.polymtl.ca/taln2010/deft.php in French). This paper is aimed\\nto present the complete result sets of all the conducted experiments and their\\nsettings in the resulting tables highlighting the approach and the best\\nresults, but also showing the worse and the worst and their subsequent\\nanalysis. This particular work focuses on application of the MARF's classical\\nand NLP pipelines to identification tasks within various francophone corpora to\\nidentify decades when certain articles were published for the first track\\n(Piste 1) and place of origin of a publication (Piste 2), such as the journal\\nand location (France vs. Quebec). This is the sixth iteration of the release of\\nthe results.\\n\",\n",
              " \"  The Right Frontier Constraint (RFC), as a constraint on the attachment of new\\nconstituents to an existing discourse structure, has important implications for\\nthe interpretation of anaphoric elements in discourse and for Machine Learning\\n(ML) approaches to learning discourse structures. In this paper we provide\\nstrong empirical support for SDRT's version of RFC. The analysis of about 100\\ndoubly annotated documents by five different naive annotators shows that SDRT's\\nRFC is respected about 95% of the time. The qualitative analysis of presumed\\nviolations that we have performed shows that they are either click-errors or\\nstructural misconceptions.\\n\",\n",
              " '  We analyze the rank-frequency distributions of words in selected English and\\nPolish texts. We show that for the lemmatized (basic) word forms the\\nscale-invariant regime breaks after about two decades, while it might be\\nconsistent for the whole range of ranks for the inflected word forms. We also\\nfind that for a corpus consisting of texts written by different authors the\\nbasic scale-invariant regime is broken more strongly than in the case of\\ncomparable corpus consisting of texts written by the same author. Similarly,\\nfor a corpus consisting of texts translated into Polish from other languages\\nthe scale-invariant regime is broken more strongly than for a comparable corpus\\nof native Polish texts. Moreover, we find that if the words are tagged with\\ntheir proper part of speech, only verbs show rank-frequency distribution that\\nis almost scale-invariant.\\n',\n",
              " '  We investigate inflection structure of a synthetic language using Latin as an\\nexample. We construct a bipartite graph in which one group of vertices\\ncorrespond to dictionary headwords and the other group to inflected forms\\nencountered in a given text. Each inflected form is connected to its\\ncorresponding headword, which in some cases in non-unique. The resulting sparse\\ngraph decomposes into a large number of connected components, to be called word\\ngroups. We then show how the concept of the word group can be used to construct\\ncoverage curves of selected Latin texts. We also investigate a version of the\\ninflection graph in which all theoretically possible inflected forms are\\nincluded. Distribution of sizes of connected components of this graphs\\nresembles cluster distribution in a lattice percolation near the critical\\npoint.\\n',\n",
              " '  We establish concrete mathematical criteria to distinguish between different\\nkinds of written storytelling, fictional and non-fictional. Specifically, we\\nconstructed a semantic network from both novels and news stories, with $N$\\nindependent words as vertices or nodes, and edges or links allotted to words\\noccurring within $m$ places of a given vertex; we call $m$ the word distance.\\nWe then used measures from complex network theory to distinguish between news\\nand fiction, studying the minimal text length needed as well as the optimized\\nword distance $m$. The literature samples were found to be most effectively\\nrepresented by their corresponding power laws over degree distribution $P(k)$\\nand clustering coefficient $C(k)$; we also studied the mean geodesic distance,\\nand found all our texts were small-world networks. We observed a natural\\nbreak-point at $k=\\\\sqrt{N}$ where the power law in the degree distribution\\nchanged, leading to separate power law fit for the bulk and the tail of $P(k)$.\\nOur linear discriminant analysis yielded a $73.8 \\\\pm 5.15%$ accuracy for the\\ncorrect classification of novels and $69.1 \\\\pm 1.22%$ for news stories. We\\nfound an optimal word distance of $m=4$ and a minimum text length of 100 to 200\\nwords $N$.\\n',\n",
              " \"  The Lambek-Grishin calculus is a symmetric extension of the Lambek calculus:\\nin addition to the residuated family of product, left and right division\\noperations of Lambek's original calculus, one also considers a family of\\ncoproduct, right and left difference operations, related to the former by an\\narrow-reversing duality. Communication between the two families is implemented\\nin terms of linear distributivity principles. The aim of this paper is to\\ncomplement the symmetry between (dual) residuated type-forming operations with\\nan orthogonal opposition that contrasts residuated and Galois connected\\noperations. Whereas the (dual) residuated operations are monotone, the Galois\\nconnected operations (and their duals) are antitone. We discuss the algebraic\\nproperties of the (dual) Galois connected operations, and generalize the\\n(co)product distributivity principles to include the negative operations. We\\ngive a continuation-passing-style translation for the new type-forming\\noperations, and discuss some linguistic applications.\\n\",\n",
              " '  Space is a circuit oriented, spatial programming language designed to exploit\\nthe massive parallelism available in a novel formal model of computation called\\nthe Synchronic A-Ram, and physically related FPGA and reconfigurable\\narchitectures. Space expresses variable grained MIMD parallelism, is modular,\\nstrictly typed, and deterministic. Barring operations associated with memory\\nallocation and compilation, modules cannot access global variables, and are\\nreferentially transparent. At a high level of abstraction, modules exhibit a\\nsmall, sequential state transition system, aiding verification. Space deals\\nwith communication, scheduling, and resource contention issues in parallel\\ncomputing, by resolving them explicitly in an incremental manner, module by\\nmodule, whilst ascending the ladder of abstraction. Whilst the Synchronic A-Ram\\nmodel was inspired by linguistic considerations, it is also put forward as a\\nformal model for reconfigurable digital circuits. A programming environment has\\nbeen developed, that incorporates a simulator and compiler that transform Space\\nprograms into Synchronic A-Ram machine code, consisting of only three bit-level\\ninstructions, and a marking instruction. Space and the Synchronic A-Ram point\\nto novel routes out of the parallel computing crisis.\\n',\n",
              " '  We report on work in progress on extracting lexical simplifications (e.g.,\\n\"collaborate\" -> \"work together\"), focusing on utilizing edit histories in\\nSimple English Wikipedia for this task. We consider two main approaches: (1)\\nderiving simplification probabilities via an edit model that accounts for a\\nmixture of different operations, and (2) using metadata to focus on edits that\\nare more likely to be simplification operations. We find our methods to\\noutperform a reasonable baseline and yield many high-quality lexical\\nsimplifications not included in an independently-created manually prepared\\nlist.\\n',\n",
              " \"  Researchers in textual entailment have begun to consider inferences involving\\n'downward-entailing operators', an interesting and important class of lexical\\nitems that change the way inferences are made. Recent work proposed a method\\nfor learning English downward-entailing operators that requires access to a\\nhigh-quality collection of 'negative polarity items' (NPIs). However, English\\nis one of the very few languages for which such a list exists. We propose the\\nfirst approach that can be applied to the many languages for which there is no\\npre-existing high-precision database of NPIs. As a case study, we apply our\\nmethod to Romanian and show that our method yields good results. Also, we\\nperform a cross-linguistic analysis that suggests interesting connections to\\nsome findings in linguistic typology.\\n\",\n",
              " '  Lexical co-occurrence is an important cue for detecting word associations. We\\npresent a theoretical framework for discovering statistically significant\\nlexical co-occurrences from a given corpus. In contrast with the prevalent\\npractice of giving weightage to unigram frequencies, we focus only on the\\ndocuments containing both the terms (of a candidate bigram). We detect biases\\nin span distributions of associated words, while being agnostic to variations\\nin global unigram frequencies. Our framework has the fidelity to distinguish\\ndifferent classes of lexical co-occurrences, based on strengths of the document\\nand corpuslevel cues of co-occurrence in the data. We perform extensive\\nexperiments on benchmark data sets to study the performance of various\\nco-occurrence measures that are currently known in literature. We find that a\\nrelatively obscure measure called Ochiai, and a newly introduced measure CSA\\ncapture the notion of lexical co-occurrence best, followed next by LLR, Dice,\\nand TTest, while another popular measure, PMI, suprisingly, performs poorly in\\nthe context of lexical co-occurrence.\\n',\n",
              " '  This paper presents our investigations on emotional state categorization from\\nspeech signals with a psychologically inspired computational model against\\nhuman performance under the same experimental setup. Based on psychological\\nstudies, we propose a multistage categorization strategy which allows\\nestablishing an automatic categorization model flexibly for a given emotional\\nspeech categorization task. We apply the strategy to the Serbian Emotional\\nSpeech Corpus (GEES) and the Danish Emotional Speech Corpus (DES), where human\\nperformance was reported in previous psychological studies. Our work is the\\nfirst attempt to apply machine learning to the GEES corpus where the human\\nrecognition rates were only available prior to our study. Unlike the previous\\nwork on the DES corpus, our work focuses on a comparison to human performance\\nunder the same experimental settings. Our studies suggest that\\npsychology-inspired systems yield behaviours that, to a great extent, resemble\\nwhat humans perceived and their performance is close to that of humans under\\nthe same experimental setup. Furthermore, our work also uncovers some\\ndifferences between machine and humans in terms of emotional state recognition\\nfrom speech.\\n',\n",
              " '  Lexicon-Grammar tables are a very rich syntactic lexicon for the French\\nlanguage. This linguistic database is nevertheless not directly suitable for\\nuse by computer programs, as it is incomplete and lacks consistency. Tables are\\ndefined on the basis of features which are not explicitly recorded in the\\nlexicon. These features are only described in literature. Our aim is to define\\nfor each tables these essential properties to make them usable in various\\nNatural Language Processing (NLP) applications, such as parsing.\\n',\n",
              " '  Categorial type logics, pioneered by Lambek, seek a proof-theoretic\\nunderstanding of natural language syntax by identifying categories with\\nformulas and derivations with proofs. We typically observe an intuitionistic\\nbias: a structural configuration of hypotheses (a constituent) derives a single\\nconclusion (the category assigned to it). Acting upon suggestions of Grishin to\\ndualize the logical vocabulary, Moortgat proposed the Lambek-Grishin calculus\\n(LG) with the aim of restoring symmetry between hypotheses and conclusions. We\\ndevelop a theory of labeled modal tableaux for LG, inspired by the\\ninterpretation of its connectives as binary modal operators in the relational\\nsemantics of Kurtonina and Moortgat. As a linguistic application of our method,\\nwe show that grammars based on LG are context-free through use of an\\ninterpolation lemma. This result complements that of Melissen, who proved that\\nLG augmented by mixed associativity and -commutativity was exceeds LTAG in\\nexpressive power.\\n',\n",
              " '  Patterns of word use both reflect and influence a myriad of human activities\\nand interactions. Like other entities that are reproduced and evolve, words\\nrise or decline depending upon a complex interplay between {their intrinsic\\nproperties and the environments in which they function}. Using Internet\\ndiscussion communities as model systems, we define the concept of a word niche\\nas the relationship between the word and the characteristic features of the\\nenvironments in which it is used. We develop a method to quantify two important\\naspects of the size of the word niche: the range of individuals using the word\\nand the range of topics it is used to discuss. Controlling for word frequency,\\nwe show that these aspects of the word niche are strong determinants of changes\\nin word frequency. Previous studies have already indicated that word frequency\\nitself is a correlate of word success at historical time scales. Our analysis\\nof changes in word frequencies over time reveals that the relative sizes of\\nword niches are far more important than word frequencies in the dynamics of the\\nentire vocabulary at shorter time scales, as the language adapts to new\\nconcepts and social groupings. We also distinguish endogenous versus exogenous\\nfactors as additional contributors to the fates of words, and demonstrate the\\nforce of this distinction in the rise of novel words. Our results indicate that\\nshort-term nonstationarity in word statistics is strongly driven by individual\\nproclivities, including inclinations to provide novel information and to\\nproject a distinctive social identity.\\n',\n",
              " '  This paper describes a probabilistic top-down parser for minimalist grammars.\\nTop-down parsers have the great advantage of having a certain predictive power\\nduring the parsing, which takes place in a left-to-right reading of the\\nsentence. Such parsers have already been well-implemented and studied in the\\ncase of Context-Free Grammars, which are already top-down, but these are\\ndifficult to adapt to Minimalist Grammars, which generate sentences bottom-up.\\nI propose here a way of rewriting Minimalist Grammars as Linear Context-Free\\nRewriting Systems, allowing to easily create a top-down parser. This rewriting\\nallows also to put a probabilistic field on these grammars, which can be used\\nto accelerate the parser. Finally, I propose a method of refining the\\nprobabilistic field by using algorithms used in data compression.\\n',\n",
              " '  In this paper the problems of deriving a taxonomy from a text and\\nconcept-oriented text segmentation are approached. Formal Concept Analysis\\n(FCA) method is applied to solve both of these linguistic problems. The\\nproposed segmentation method offers a conceptual view for text segmentation,\\nusing a context-driven clustering of sentences. The Concept-oriented Clustering\\nSegmentation algorithm (COCS) is based on k-means linear clustering of the\\nsentences. Experimental results obtained using COCS algorithm are presented.\\n',\n",
              " '  It is usual to consider that standards generate mixed feelings among\\nscientists. They are often seen as not really reflecting the state of the art\\nin a given domain and a hindrance to scientific creativity. Still, scientists\\nshould theoretically be at the best place to bring their expertise into\\nstandard developments, being even more neutral on issues that may typically be\\nrelated to competing industrial interests. Even if it could be thought of as\\neven more complex to think about developping standards in the humanities, we\\nwill show how this can be made feasible through the experience gained both\\nwithin the Text Encoding Initiative consortium and the International\\nOrganisation for Standardisation. By taking the specific case of lexical\\nresources, we will try to show how this brings about new ideas for designing\\nfuture research infrastructures in the human and social sciences.\\n',\n",
              " '  We have developed a full discourse parser in the Penn Discourse Treebank\\n(PDTB) style. Our trained parser first identifies all discourse and\\nnon-discourse relations, locates and labels their arguments, and then\\nclassifies their relation types. When appropriate, the attribution spans to\\nthese relations are also determined. We present a comprehensive evaluation from\\nboth component-wise and error-cascading perspectives.\\n',\n",
              " '  A temporal analysis of emoticon use in Swedish, Italian, German and English\\nasynchronous electronic communication is reported. Emoticons are classified as\\npositive, negative and neutral. Postings to newsgroups over a 66 week period\\nare considered. The aggregate analysis of emoticon use in newsgroups for\\nscience and politics tend on the whole to be consistent over the entire time\\nperiod. Where possible, events that coincide with divergences from trends in\\nlanguage-subject pairs are noted. Political discourse in Italian over the\\nperiod shows marked use of negative emoticons, and in Swedish, positive\\nemoticons.\\n',\n",
              " \"  In this research paper we address the importance of Product Data Management\\n(PDM) with respect to its contributions in industry. Moreover we also present\\nsome currently available major challenges to PDM communities and targeting some\\nof these challenges we present an approach i.e. I-SOAS, and briefly discuss how\\nthis approach can be helpful in solving the PDM community's faced problems.\\nFurthermore, limiting the scope of this research to one challenge, we focus on\\nthe implementation of a semantic based search mechanism in PDM Systems. Going\\ninto the details, at first we describe the respective field i.e. Language\\nTechnology (LT), contributing towards natural language processing, to take\\nadvantage in implementing a search engine capable of understanding the semantic\\nout of natural language based search queries. Then we discuss how can we\\npractically take advantage of LT by implementing its concepts in the form of\\nsoftware application with the use of semantic web technology i.e. Ontology.\\nLater, in the end of this research paper, we briefly present a prototype\\napplication developed with the use of concepts of LT towards semantic based\\nsearch.\\n\",\n",
              " '  This article describes a method to build syntactical dependencies starting\\nfrom the phrase structure parsing process. The goal is to obtain all the\\ninformation needed for a detailled semantical analysis. Interaction Grammars\\nare used for parsing; the saturation of polarities which is the core of this\\nformalism can be mapped to dependency relation. Formally, graph patterns are\\nused to express the set of constraints which control dependency creations.\\n',\n",
              " '  \"What other people think\" has always been an important piece of information\\nduring various decision-making processes. Today people frequently make their\\nopinions available via the Internet, and as a result, the Web has become an\\nexcellent source for gathering consumer opinions. There are now numerous Web\\nresources containing such opinions, e.g., product reviews forums, discussion\\ngroups, and Blogs. But, due to the large amount of information and the wide\\nrange of sources, it is essentially impossible for a customer to read all of\\nthe reviews and make an informed decision on whether to purchase the product.\\nIt is also difficult for the manufacturer or seller of a product to accurately\\nmonitor customer opinions. For this reason, mining customer reviews, or opinion\\nmining, has become an important issue for research in Web information\\nextraction. One of the important topics in this research area is the\\nidentification of opinion polarity. The opinion polarity of a review is usually\\nexpressed with values \\'positive\\', \\'negative\\' or \\'neutral\\'. We propose a\\ntechnique for identifying polarity of reviews by identifying the polarity of\\nthe adjectives that appear in them. Our evaluation shows the technique can\\nprovide accuracy in the area of 73%, which is well above the 58%-64% provided\\nby naive Bayesian classifiers.\\n',\n",
              " \"  Our study applies statistical methods to French and Italian corpora to\\nexamine the phenomenon of multi-word term reduction in specialty languages.\\nThere are two kinds of reduction: anaphoric and lexical. We show that anaphoric\\nreduction depends on the discourse type (vulgarization, pedagogical,\\nspecialized) but is independent of both domain and language; that lexical\\nreduction depends on domain and is more frequent in technical, rapidly evolving\\ndomains; and that anaphoric reductions tend to follow full terms rather than\\nprecede them. We define the notion of the anaphoric tree of the term and study\\nits properties. Concerning lexical reduction, we attempt to prove statistically\\nthat there is a notion of term lifecycle, where the full form is progressively\\nreplaced by a lexical reduction. ----- Nous \\\\'etudions par des m\\\\'ethodes\\nstatistiques sur des corpus fran\\\\c{c}ais et italiens, le ph\\\\'enom\\\\`ene de\\nr\\\\'eduction des termes complexes dans les langues de sp\\\\'ecialit\\\\'e. Il existe\\ndeux types de r\\\\'eductions : anaphorique et lexicale. Nous montrons que la\\nr\\\\'eduction anaphorique d\\\\'epend du type de discours (de vulgarisation,\\np\\\\'edagogique, sp\\\\'ecialis\\\\'e) mais ne d\\\\'epend ni du domaine, ni de la langue,\\nalors que la r\\\\'eduction lexicale d\\\\'epend du domaine et est plus fr\\\\'equente\\ndans les domaines techniques \\\\`a \\\\'evolution rapide. D'autre part, nous\\nmontrons que la r\\\\'eduction anaphorique a tendance \\\\`a suivre la forme pleine\\ndu terme, nous d\\\\'efinissons une notion d'arbre anaphorique de terme et nous\\n\\\\'etudions ses propri\\\\'et\\\\'es. Concernant la r\\\\'eduction lexicale, nous tentons\\nde d\\\\'emontrer statistiquement qu'il existe une notion de cycle de vie de\\nterme, o\\\\`u la forme pleine est progressivement remplac\\\\'ee par une r\\\\'eduction\\nlexicale.\\n\",\n",
              " '  Meaning can be generated when information is related at a systemic level.\\nSuch a system can be an observer, but also a discourse, for example,\\noperationalized as a set of documents. The measurement of semantics as\\nsimilarity in patterns (correlations) and latent variables (factor analysis)\\nhas been enhanced by computer techniques and the use of statistics; for\\nexample, in \"Latent Semantic Analysis\". This communication provides an\\nintroduction, an example, pointers to relevant software, and summarizes the\\nchoices that can be made by the analyst. Visualization (\"semantic mapping\") is\\nthus made more accessible.\\n',\n",
              " '  This report describes the MUDOS-NG summarization system, which applies a set\\nof language-independent and generic methods for generating extractive\\nsummaries. The proposed methods are mostly combinations of simple operators on\\na generic character n-gram graph representation of texts. This work defines the\\nset of used operators upon n-gram graphs and proposes using these operators\\nwithin the multi-document summarization process in such subtasks as document\\nanalysis, salient sentence selection, query expansion and redundancy control.\\nFurthermore, a novel chunking methodology is used, together with a novel way to\\nassign concepts to sentences for query expansion. The experimental results of\\nthe summarization system, performed upon widely used corpora from the Document\\nUnderstanding and the Text Analysis Conferences, are promising and provide\\nevidence for the potential of the generic methods introduced. This work aims to\\ndesignate core methods exploiting the n-gram graph representation, providing\\nthe basis for more advanced summarization systems.\\n',\n",
              " '  We first recall some basic notions on minimalist grammars and on categorial\\ngrammars. Next we shortly introduce partially commutative linear logic, and our\\nrepresentation of minimalist grammars within this categorial system, the\\nso-called categorial minimalist grammars. Thereafter we briefly present\\n\\\\lambda\\\\mu-DRT (Discourse Representation Theory) an extension of \\\\lambda-DRT\\n(compositional DRT) in the framework of \\\\lambda\\\\mu calculus: it avoids type\\nraising and derives different readings from a single semantic representation,\\nin a setting which follows discourse structure. We run a complete example which\\nillustrates the various structures and rules that are needed to derive a\\nsemantic representation from the categorial view of a transformational\\nsyntactic analysis.\\n',\n",
              " '  This document presents Annotated English, a system of diacritical symbols\\nwhich turns English pronunciation into a precise and unambiguous process. The\\nannotations are defined and located in such a way that the original English\\ntext is not altered (not even a letter), thus allowing for a consistent reading\\nand learning of the English language with and without annotations. The\\nannotations are based on a set of general rules that make the frequency of\\nannotations not dramatically high. This makes the reader easily associate\\nannotations with exceptions, and makes it possible to shape, internalise and\\nconsolidate some rules for the English language which otherwise are weakened by\\nthe enormous amount of exceptions in English pronunciation. The advantages of\\nthis annotation system are manifold. Any existing text can be annotated without\\na significant increase in size. This means that we can get an annotated version\\nof any document or book with the same number of pages and fontsize. Since no\\nletter is affected, the text can be perfectly read by a person who does not\\nknow the annotation rules, since annotations can be simply ignored. The\\nannotations are based on a set of rules which can be progressively learned and\\nrecognised, even in cases where the reader has no access or time to read the\\nrules. This means that a reader can understand most of the annotations after\\nreading a few pages of Annotated English, and can take advantage from that\\nknowledge for any other annotated document she may read in the future.\\n',\n",
              " '  Coecke, Sadrzadeh, and Clark (arXiv:1003.4394v1 [cs.CL]) developed a\\ncompositional model of meaning for distributional semantics, in which each word\\nin a sentence has a meaning vector and the distributional meaning of the\\nsentence is a function of the tensor products of the word vectors. Abstractly\\nspeaking, this function is the morphism corresponding to the grammatical\\nstructure of the sentence in the category of finite dimensional vector spaces.\\nIn this paper, we provide a concrete method for implementing this linear\\nmeaning map, by constructing a corpus-based vector space for the type of\\nsentence. Our construction method is based on structured vector spaces whereby\\nmeaning vectors of all sentences, regardless of their grammatical structure,\\nlive in the same vector space. Our proposed sentence space is the tensor\\nproduct of two noun spaces, in which the basis vectors are pairs of words each\\naugmented with a grammatical role. This enables us to compare meanings of\\nsentences by simply taking the inner product of their vectors.\\n',\n",
              " '  Techniques in which words are represented as vectors have proved useful in\\nmany applications in computational linguistics, however there is currently no\\ngeneral semantic formalism for representing meaning in terms of vectors. We\\npresent a framework for natural language semantics in which words, phrases and\\nsentences are all represented as vectors, based on a theoretical analysis which\\nassumes that meaning is determined by context.\\n  In the theoretical analysis, we define a corpus model as a mathematical\\nabstraction of a text corpus. The meaning of a string of words is assumed to be\\na vector representing the contexts in which it occurs in the corpus model.\\nBased on this assumption, we can show that the vector representations of words\\ncan be considered as elements of an algebra over a field. We note that in\\napplications of vector spaces to representing meanings of words there is an\\nunderlying lattice structure; we interpret the partial ordering of the lattice\\nas describing entailment between meanings. We also define the context-theoretic\\nprobability of a string, and, based on this and the lattice structure, a degree\\nof entailment between strings.\\n  We relate the framework to existing methods of composing vector-based\\nrepresentations of meaning, and show that our approach generalises many of\\nthese, including vector addition, component-wise multiplication, and the tensor\\nproduct.\\n',\n",
              " '  We reformulate minimalist grammars as partial functions on term algebras for\\nstrings and trees. Using filler/role bindings and tensor product\\nrepresentations, we construct homomorphisms for these data structures into\\ngeometric vector spaces. We prove that the structure-building functions as well\\nas simple processors for minimalist languages can be realized by piecewise\\nlinear operators in representation space. We also propose harmony, i.e. the\\ndistance of an intermediate processing step from the final well-formed state in\\nrepresentation space, as a measure of processing complexity. Finally, we\\nillustrate our findings by means of two particular arithmetic and fractal\\nrepresentations.\\n',\n",
              " '  Arabic morphological analysis is one of the essential stages in Arabic\\nNatural Language Processing. In this paper we present an approach for Arabic\\nmorphological analysis. This approach is based on Arabic morphological\\nautomaton (AMAUT). The proposed technique uses a morphological database\\nrealized using XMODEL language. Arabic morphology represents a special type of\\nmorphological systems because it is based on the concept of scheme to represent\\nArabic words. We use this concept to develop the Arabic morphological automata.\\nThe proposed approach has development standardization aspect. It can be\\nexploited by NLP applications such as syntactic and semantic analysis,\\ninformation retrieval, machine translation and orthographical correction. The\\nproposed approach is compared with Xerox Arabic Analyzer and Smrz Arabic\\nAnalyzer.\\n',\n",
              " \"  Grishin proposed enriching the Lambek calculus with multiplicative\\ndisjunction (par) and coresiduals. Applications to linguistics were discussed\\nby Moortgat, who spoke of the Lambek-Grishin calculus (LG). In this paper, we\\nadapt Girard's polarity-sensitive double negation embedding for classical logic\\nto extract a compositional Montagovian semantics from a display calculus for\\nfocused proof search in LG. We seize the opportunity to illustrate our approach\\nalongside an analysis of extraction, providing linguistic motivation for linear\\ndistributivity of tensor over par, thus answering a question of\\nKurtonina&Moortgat. We conclude by comparing our proposal to the continuation\\nsemantics of Bernardi&Moortgat, corresponding to call-by- name and\\ncall-by-value evaluation strategies.\\n\",\n",
              " \"  The origin of Malagasy DNA is half African and half Indonesian, nevertheless\\nthe Malagasy language, spoken by the entire population, belongs to the\\nAustronesian family. The language most closely related to Malagasy is Maanyan\\n(Greater Barito East group of the Austronesian family), but related languages\\nare also in Sulawesi, Malaysia and Sumatra. For this reason, and because\\nMaanyan is spoken by a population which lives along the Barito river in\\nKalimantan and which does not possess the necessary skill for long maritime\\nnavigation, the ethnic composition of the Indonesian colonizers is still\\nunclear.\\n  There is a general consensus that Indonesian sailors reached Madagascar by a\\nmaritime trek, but the time, the path and the landing area of the first\\ncolonization are all disputed. In this research we try to answer these problems\\ntogether with other ones, such as the historical configuration of Malagasy\\ndialects, by types of analysis related to lexicostatistics and glottochronology\\nwhich draw upon the automated method recently proposed by the authors\\n\\\\cite{Serva:2008, Holman:2008, Petroni:2008, Bakker:2009}. The data were\\ncollected by the first author at the beginning of 2010 with the invaluable help\\nof Joselin\\\\`a Soafara N\\\\'er\\\\'e and consist of Swadesh lists of 200 items for 23\\ndialects covering all areas of the Island.\\n\",\n",
              " '  This paper studies the effect of linguistic constraints on the large scale\\norganization of language. It describes the properties of linguistic networks\\nbuilt using texts of written language with the words randomized. These\\nproperties are compared to those obtained for a network built over the text in\\nnatural order. It is observed that the \"random\" networks too exhibit\\nsmall-world and scale-free characteristics. They also show a high degree of\\nclustering. This is indeed a surprising result - one that has not been\\naddressed adequately in the literature. We hypothesize that many of the network\\nstatistics reported here studied are in fact functions of the distribution of\\nthe underlying data from which the network is built and may not be indicative\\nof the nature of the concerned network.\\n',\n",
              " '  We examine the class of languages that can be defined entirely in terms of\\nprovability in an extension of the sorted type theory (Ty_n) by embedding the\\nlogic of phonologies, without introduction of special types for syntactic\\nentities. This class is proven to precisely coincide with the class of\\nlogically closed languages that may be thought of as functions from expressions\\nto sets of logically equivalent Ty_n terms. For a specific sub-class of\\nlogically closed languages that are described by finite sets of rules or rule\\nschemata, we find effective procedures for building a compact Ty_n\\nrepresentation, involving a finite number of axioms or axiom schemata. The\\nproposed formalism is characterized by some useful features unavailable in a\\ntwo-component architecture of a language model. A further specialization and\\nextension of the formalism with a context type enable effective account of\\nintensional and dynamic semantics.\\n',\n",
              " \"  We address the problem of inferring a speaker's level of certainty based on\\nprosodic information in the speech signal, which has application in\\nspeech-based dialogue systems. We show that using phrase-level prosodic\\nfeatures centered around the phrases causing uncertainty, in addition to\\nutterance-level prosodic features, improves our model's level of certainty\\nclassification. In addition, our models can be used to predict which phrase a\\nperson is uncertain about. These results rely on a novel method for eliciting\\nutterances of varying levels of certainty that allows us to compare the utility\\nof contextually-based feature sets. We elicit level of certainty ratings from\\nboth the speakers themselves and a panel of listeners, finding that there is\\noften a mismatch between speakers' internal states and their perceived states,\\nand highlighting the importance of this distinction.\\n\",\n",
              " '  Dictionaries are inherently circular in nature. A given word is linked to a\\nset of alternative words (the definition) which in turn point to further\\ndescendants. Iterating through definitions in this way, one typically finds\\nthat definitions loop back upon themselves. The graph formed by such\\ndefinitional relations is our object of study. By eliminating those links which\\nare not in loops, we arrive at a core subgraph of highly connected nodes.\\n  We observe that definitional loops are conveniently classified by length,\\nwith longer loops usually emerging from semantic misinterpretation. By breaking\\nthe long loops in the graph of the dictionary, we arrive at a set of\\ndisconnected clusters. We find that the words in these clusters constitute\\nsemantic units, and moreover tend to have been introduced into the English\\nlanguage at similar times, suggesting a possible mechanism for language\\nevolution.\\n',\n",
              " '  The limited range in its abscissa of ranked letter frequency distributions\\ncauses multiple functions to fit the observed distribution reasonably well. In\\norder to critically compare various functions, we apply the statistical model\\nselections on ten functions, using the texts of U.S. and Mexican presidential\\nspeeches in the last 1-2 centuries. Dispite minor switching of ranking order of\\ncertain letters during the temporal evolution for both datasets, the letter\\nusage is generally stable. The best fitting function, judged by either\\nleast-square-error or by AIC/BIC model selection, is the Cocho/Beta function.\\nWe also use a novel method to discover clusters of letters by their\\nobserved-over-expected frequency ratios.\\n',\n",
              " '  Existing grammar frameworks do not work out particularly well for controlled\\nnatural languages (CNL), especially if they are to be used in predictive\\neditors. I introduce in this paper a new grammar notation, called Codeco, which\\nis designed specifically for CNLs and predictive editors. Two different parsers\\nhave been implemented and a large subset of Attempto Controlled English (ACE)\\nhas been represented in Codeco. The results show that Codeco is practical,\\nadequate and efficient.\\n',\n",
              " '  This article presents a fragment of a new comparative dictionary \"A\\ncomparative dictionary of names of expansive action in Russian and Bulgarian\\nlanguages\". Main features of the new web-based comparative dictionary are\\nplaced, the principles of its formation are shown, primary links between the\\nword-matches are classified. The principal difference between translation\\ndictionaries and the model of double comparison is also shown. The\\nclassification scheme of the pages is proposed. New concepts and keywords have\\nbeen introduced. The real prototype of the dictionary with a few key pages is\\npublished. The broad debate about the possibility of this prototype to become a\\nversion of Russian-Bulgarian comparative dictionary of a new generation is\\navailable.\\n',\n",
              " '  To facilitate future research in unsupervised induction of syntactic\\nstructure and to standardize best-practices, we propose a tagset that consists\\nof twelve universal part-of-speech categories. In addition to the tagset, we\\ndevelop a mapping from 25 different treebank tagsets to this universal set. As\\na result, when combined with the original treebank data, this universal tagset\\nand mapping produce a dataset consisting of common parts-of-speech for 22\\ndifferent languages. We highlight the use of this resource via two experiments,\\nincluding one that reports competitive accuracies for unsupervised grammar\\ninduction without gold standard part-of-speech tags.\\n',\n",
              " '  Chinese characters can be compared to a molecular structure: a character is\\nanalogous to a molecule, radicals are like atoms, calligraphic strokes\\ncorrespond to elementary particles, and when characters form compounds, they\\nare like molecular structures. In chemistry the conjunction of all of these\\nstructural levels produces what we perceive as matter. In language, the\\nconjunction of strokes, radicals, characters, and compounds produces meaning.\\nBut when does meaning arise? We all know that radicals are, in some sense, the\\nbasic semantic components of Chinese script, but what about strokes?\\nConsidering the fact that many characters are made by adding individual strokes\\nto (combinations of) radicals, we can legitimately ask the question whether\\nstrokes carry meaning, or not. In this talk I will present my project of\\nextending traditional NLP techniques to radicals and strokes, aiming to obtain\\na deeper understanding of the way ideographic languages model the world.\\n',\n",
              " \"  The idea that the distance among pairs of languages can be evaluated from\\nlexical differences seems to have its roots in the work of the French explorer\\nDumont D'Urville. He collected comparative words lists of various languages\\nduring his voyages aboard the Astrolabe from 1826 to 1829 and, in his work\\nabout the geographical division of the Pacific, he proposed a method to measure\\nthe degree of relation between languages.\\n  The method used by the modern lexicostatistics, developed by Morris Swadesh\\nin the 1950s, measures distances from the percentage of shared cognates, which\\nare words with a common historical origin. The weak point of this method is\\nthat subjective judgment plays a relevant role.\\n  Recently, we have proposed a new automated method which is motivated by the\\nanalogy with genetics. The new approach avoids any subjectivity and results can\\nbe easily replicated by other scholars. The distance between two languages is\\ndefined by considering a renormalized Levenshtein distance between pair of\\nwords with the same meaning and averaging on the words contained in a list. The\\nrenormalization, which takes into account the length of the words, plays a\\ncrucial role, and no sensible results can be found without it.\\n  In this paper we give a short review of our automated method and we\\nillustrate it by considering the cluster of Malagasy dialects. We show that it\\nsheds new light on their kinship relation and also that it furnishes a lot of\\nnew information concerning the modalities of the settlement of Madagascar.\\n\",\n",
              " '  This paper introduces the performance evaluation of statistical approaches\\nfor TextIndependent speaker recognition system using source feature. Linear\\nprediction LP residual is used as a representation of excitation information in\\nspeech. The speaker-specific information in the excitation of voiced speech is\\ncaptured using statistical approaches such as Gaussian Mixture Models GMMs and\\nHidden Markov Models HMMs. The decrease in the error during training and\\nrecognizing speakers during testing phase close to 100 percent accuracy\\ndemonstrates that the excitation component of speech contains speaker-specific\\ninformation and is indeed being effectively captured by continuous Ergodic HMM\\nthan GMM. The performance of the speaker recognition system is evaluated on GMM\\nand 2 state ergodic HMM with different mixture components and test speech\\nduration. We demonstrate the speaker recognition studies on TIMIT database for\\nboth GMM and Ergodic HMM.\\n',\n",
              " \"  The psycholinguistic theory of communication accommodation accounts for the\\ngeneral observation that participants in conversations tend to converge to one\\nanother's communicative behavior: they coordinate in a variety of dimensions\\nincluding choice of words, syntax, utterance length, pitch and gestures. In its\\nalmost forty years of existence, this theory has been empirically supported\\nexclusively through small-scale or controlled laboratory studies. Here we\\naddress this phenomenon in the context of Twitter conversations. Undoubtedly,\\nthis setting is unlike any other in which accommodation was observed and, thus,\\nchallenging to the theory. Its novelty comes not only from its size, but also\\nfrom the non real-time nature of conversations, from the 140 character length\\nrestriction, from the wide variety of social relation types, and from a design\\nthat was initially not geared towards conversation at all. Given such\\nconstraints, it is not clear a priori whether accommodation is robust enough to\\noccur given the constraints of this new environment. To investigate this, we\\ndevelop a probabilistic framework that can model accommodation and measure its\\neffects. We apply it to a large Twitter conversational dataset specifically\\ndeveloped for this task. This is the first time the hypothesis of linguistic\\nstyle accommodation has been examined (and verified) in a large scale, real\\nworld setting. Furthermore, when investigating concepts such as stylistic\\ninfluence and symmetry of accommodation, we discover a complexity of the\\nphenomenon which was never observed before. We also explore the potential\\nrelation between stylistic influence and network features commonly associated\\nwith social status.\\n\",\n",
              " '  This article overviews the current state of the English-Lithuanian-English\\nmachine translation system. The first part of the article describes the\\nproblems that system poses today and what actions will be taken to solve them\\nin the future. The second part of the article tackles the main issue of the\\ntranslation process. Article briefly overviews the word sense disambiguation\\nfor MT technique using Google.\\n',\n",
              " '  The paper presents the design and development of English-Lithuanian-English\\ndictionarylexicon tool and lexicon database management system for MT. The\\nsystem is oriented to support two main requirements: to be open to the user and\\nto describe much more attributes of speech parts as a regular dictionary that\\nare required for the MT. Programming language Java and database management\\nsystem MySql is used to implement the designing tool and lexicon database\\nrespectively. This solution allows easily deploying this system in the\\nInternet. The system is able to run on various OS such as: Windows, Linux, Mac\\nand other OS where Java Virtual Machine is supported. Since the modern lexicon\\ndatabase managing system is used, it is not a problem accessing the same\\ndatabase for several users.\\n',\n",
              " '  We provide an overview of the hybrid compositional distributional model of\\nmeaning, developed in Coecke et al. (arXiv:1003.4394v1 [cs.CL]), which is based\\non the categorical methods also applied to the analysis of information flow in\\nquantum protocols. The mathematical setting stipulates that the meaning of a\\nsentence is a linear function of the tensor products of the meanings of its\\nwords. We provide concrete constructions for this definition and present\\ntechniques to build vector spaces for meaning vectors of words, as well as that\\nof sentences. The applicability of these methods is demonstrated via a toy\\nvector space as well as real data from the British National Corpus and two\\ndisambiguation experiments.\\n',\n",
              " \"  Linguistic markers of personality traits have been studied extensively, but\\nfew cross-cultural studies exist. In this paper, we evaluate how native\\nspeakers of American English and Arabic perceive personality traits and\\nnaturalness of English utterances that vary along the dimensions of verbosity,\\nhedging, lexical and syntactic alignment, and formality. The utterances are the\\nturns within dialogue fragments that are presented as text transcripts to the\\nworkers of Amazon's Mechanical Turk. The results of the study suggest that all\\nfour dimensions can be used as linguistic markers of all personality traits by\\nboth language communities. A further comparative analysis shows cross-cultural\\ndifferences for some combinations of measures of personality traits and\\nnaturalness, the dimensions of linguistic variability and dialogue acts.\\n\",\n",
              " '  In natural speech, the speaker does not pause between words, yet a human\\nlistener somehow perceives this continuous stream of phonemes as a series of\\ndistinct words. The detection of boundaries between spoken words is an instance\\nof a general capability of the human neocortex to remember and to recognize\\nrecurring sequences. This paper describes a computer algorithm that is designed\\nto solve the problem of locating word boundaries in blocks of English text from\\nwhich the spaces have been removed. This problem avoids the complexities of\\nspeech processing but requires similar capabilities for detecting recurring\\nsequences. The algorithm relies entirely on statistical relationships between\\nletters in the input stream to infer the locations of word boundaries. A\\nViterbi trellis is used to simultaneously evaluate a set of hypothetical\\nsegmentations of a block of adjacent words. This technique improves accuracy\\nbut incurs a small latency between the arrival of letters in the input stream\\nand the sending of words to the output stream. The source code for a C++\\nversion of this algorithm is presented in an appendix.\\n',\n",
              " '  Simple representations of documents based on the occurrences of terms are\\nubiquitous in areas like Information Retrieval, and also frequent in Natural\\nLanguage Processing. In this work we propose a logical-probabilistic approach\\nto the analysis of natural language text based in the concept of Uncertain\\nConditional, on top of a formulation of lexical measurements inspired in the\\ntheoretical concept of ideal quantum measurements. The proposed concept can be\\nused for generating topic-specific representations of text, aiming to match in\\na simple way the perception of a user with a pre-established idea of what the\\nusage of terms in the text should be. A simple example is developed with two\\nversions of a text in two languages, showing how regularities in the use of\\nterms are detected and easily represented.\\n',\n",
              " '  This paper presents an algorithm for identifying noun-phrase antecedents of\\npronouns and adjectival anaphors in Spanish dialogues. We believe that anaphora\\nresolution requires numerous sources of information in order to find the\\ncorrect antecedent of the anaphor. These sources can be of different kinds,\\ne.g., linguistic information, discourse/dialogue structure information, or\\ntopic information. For this reason, our algorithm uses various different kinds\\nof information (hybrid information). The algorithm is based on linguistic\\nconstraints and preferences and uses an anaphoric accessibility space within\\nwhich the algorithm finds the noun phrase. We present some experiments related\\nto this algorithm and this space using a corpus of 204 dialogues. The algorithm\\nis implemented in Prolog. According to this study, 95.9% of antecedents were\\nlocated in the proposed space, a precision of 81.3% was obtained for pronominal\\nanaphora resolution, and 81.5% for adjectival anaphora.\\n',\n",
              " \"  Conversational participants tend to immediately and unconsciously adapt to\\neach other's language styles: a speaker will even adjust the number of articles\\nand other function words in their next utterance in response to the number in\\ntheir partner's immediately preceding utterance. This striking level of\\ncoordination is thought to have arisen as a way to achieve social goals, such\\nas gaining approval or emphasizing difference in status. But has the adaptation\\nmechanism become so deeply embedded in the language-generation process as to\\nbecome a reflex? We argue that fictional dialogs offer a way to study this\\nquestion, since authors create the conversations but don't receive the social\\nbenefits (rather, the imagined characters do). Indeed, we find significant\\ncoordination across many families of function words in our large movie-script\\ncorpus. We also report suggestive preliminary findings on the effects of gender\\nand other features; e.g., surprisingly, for articles, on average, characters\\nadapt more to females than to males.\\n\",\n",
              " '  Modelling compositional meaning for sentences using empirical distributional\\nmethods has been a challenge for computational linguists. We implement the\\nabstract categorical model of Coecke et al. (arXiv:1003.4394v1 [cs.CL]) using\\ndata from the BNC and evaluate it. The implementation is based on unsupervised\\nlearning of matrices for relational words and applying them to the vectors of\\ntheir arguments. The evaluation is based on the word disambiguation task\\ndeveloped by Mitchell and Lapata (2008) for intransitive sentences, and on a\\nsimilar new experiment designed for transitive sentences. Our model matches the\\nresults of its competitors in the first experiment, and betters them in the\\nsecond. The general improvement in results with increase in syntactic\\ncomplexity showcases the compositional power of our model.\\n',\n",
              " \"  This paper focuses on a system, WOLFIE (WOrd Learning From Interpreted\\nExamples), that acquires a semantic lexicon from a corpus of sentences paired\\nwith semantic representations. The lexicon learned consists of phrases paired\\nwith meaning representations. WOLFIE is part of an integrated system that\\nlearns to transform sentences into representations such as logical database\\nqueries. Experimental results are presented demonstrating WOLFIE's ability to\\nlearn useful lexicons for a database interface in four different natural\\nlanguages. The usefulness of the lexicons learned by WOLFIE are compared to\\nthose acquired by a similar system, with results favorable to WOLFIE. A second\\nset of experiments demonstrates WOLFIE's ability to scale to larger and more\\ndifficult, albeit artificially generated, corpora. In natural language\\nacquisition, it is difficult to gather the annotated data needed for supervised\\nlearning; however, unannotated data is fairly plentiful. Active learning\\nmethods attempt to select for annotation and training only the most informative\\nexamples, and therefore are potentially very useful in natural language\\napplications. However, most results to date for active learning have only\\nconsidered standard classification tasks. To reduce annotation effort while\\nmaintaining accuracy, we apply active learning to semantic lexicons. We show\\nthat active learning can significantly reduce the number of annotated examples\\nrequired to achieve a given level of performance.\\n\",\n",
              " '  This paper evaluates the different tasks carried out in the translation of\\npronominal anaphora in a machine translation (MT) system. The MT interlingua\\napproach named AGIR (Anaphora Generation with an Interlingua Representation)\\nimproves upon other proposals presented to date because it is able to translate\\nintersentential anaphors, detect co-reference chains, and translate Spanish\\nzero pronouns into English---issues hardly considered by other systems. The\\npaper presents the resolution and evaluation of these anaphora problems in AGIR\\nwith the use of different kinds of knowledge (lexical, morphological,\\nsyntactic, and semantic). The translation of English and Spanish anaphoric\\nthird-person personal pronouns (including Spanish zero pronouns) into the\\ntarget language has been evaluated on unrestricted corpora. We have obtained a\\nprecision of 80.4% and 84.8% in the translation of Spanish and English\\npronouns, respectively. Although we have only studied the Spanish and English\\nlanguages, our approach can be easily extended to other languages such as\\nPortuguese, Italian, or Japanese.\\n',\n",
              " '  Natural language generation (NLG) systems are computer software systems that\\nproduce texts in English and other human languages, often from non-linguistic\\ninput data. NLG systems, like most AI systems, need substantial amounts of\\nknowledge. However, our experience in two NLG projects suggests that it is\\ndifficult to acquire correct knowledge for NLG systems; indeed, every knowledge\\nacquisition (KA) technique we tried had significant problems. In general terms,\\nthese problems were due to the complexity, novelty, and poorly understood\\nnature of the tasks our systems attempted, and were worsened by the fact that\\npeople write so differently. This meant in particular that corpus-based KA\\napproaches suffered because it was impossible to assemble a sizable corpus of\\nhigh-quality consistent manually written texts in our domains; and structured\\nexpert-oriented KA techniques suffered because experts disagreed and because we\\ncould not get enough information about special and unusual cases to build\\nrobust systems. We believe that such problems are likely to affect many other\\nNLG systems as well. In the long term, we hope that new KA techniques may\\nemerge to help NLG system builders. In the shorter term, we believe that\\nunderstanding how individual KA techniques can fail, and using a mixture of\\ndifferent KA techniques with different strengths and weaknesses, can help\\ndevelopers acquire NLG knowledge that is mostly correct.\\n',\n",
              " '  This paper presents an investigation of the entropy of the Telugu script.\\nSince this script is syllabic, and not alphabetic, the computation of entropy\\nis somewhat complicated.\\n',\n",
              " \"  This article studies the emergence of ambiguity in communication through the\\nconcept of logical irreversibility and within the framework of Shannon's\\ninformation theory. This leads us to a precise and general expression of the\\nintuition behind Zipf's vocabulary balance in terms of a symmetry equation\\nbetween the complexities of the coding and the decoding processes that imposes\\nan unavoidable amount of logical uncertainty in natural communication.\\nAccordingly, the emergence of irreversible computations is required if the\\ncomplexities of the coding and the decoding processes are balanced in a\\nsymmetric scenario, which means that the emergence of ambiguous codes is a\\nnecessary condition for natural communication to succeed.\\n\",\n",
              " '  These notes are a continuation of topics covered by V. Selegej in his article\\n\"Electronic Dictionaries and Computational lexicography\". How can an electronic\\ndictionary have as its object the description of closely related languages?\\nObviously, such a question allows multiple answers.\\n',\n",
              " '  Formal and distributional semantic models offer complementary benefits in\\nmodeling meaning. The categorical compositional distributional (DisCoCat) model\\nof meaning of Coecke et al. (arXiv:1003.4394v1 [cs.CL]) combines aspected of\\nboth to provide a general framework in which meanings of words, obtained\\ndistributionally, are composed using methods from the logical setting to form\\nsentence meaning. Concrete consequences of this general abstract setting and\\napplications to empirical data are under active study (Grefenstette et al.,\\narxiv:1101.0309; Grefenstette and Sadrzadeh, arXiv:1106.4058v1 [cs.CL]). . In\\nthis paper, we extend this study by examining transitive verbs, represented as\\nmatrices in a DisCoCat. We discuss three ways of constructing such matrices,\\nand evaluate each method in a disambiguation task developed by Grefenstette and\\nSadrzadeh (arXiv:1106.4058v1 [cs.CL]).\\n',\n",
              " \"  The dialects of Madagascar belong to the Greater Barito East group of the\\nAustronesian family and it is widely accepted that the Island was colonized by\\nIndonesian sailors after a maritime trek which probably took place around 650\\nCE. The language most closely related to Malagasy dialects is Maanyan but also\\nMalay is strongly related especially for what concerns navigation terms. Since\\nthe Maanyan Dayaks live along the Barito river in Kalimantan (Borneo) and they\\ndo not possess the necessary skill for long maritime navigation, probably they\\nwere brought as subordinates by Malay sailors.\\n  In a recent paper we compared 23 different Malagasy dialects in order to\\ndetermine the time and the landing area of the first colonization. In this\\nresearch we use new data and new methods to confirm that the landing took place\\non the south-east coast of the Island. Furthermore, we are able to state here\\nthat it is unlikely that there were multiple settlements and, therefore,\\ncolonization consisted in a single founding event.\\n  To reach our goal we find out the internal kinship relations among all the 23\\nMalagasy dialects and we also find out the different kinship degrees of the 23\\ndialects versus Malay and Maanyan. The method used is an automated version of\\nthe lexicostatistic approach. The data concerning Madagascar were collected by\\nthe author at the beginning of 2010 and consist of Swadesh lists of 200 items\\nfor 23 dialects covering all areas of the Island. The lists for Maanyan and\\nMalay were obtained from published datasets integrated by author's interviews.\\n\",\n",
              " '  Consumers increasingly rate, review and research products online.\\nConsequently, websites containing consumer reviews are becoming targets of\\nopinion spam. While recent work has focused primarily on manually identifiable\\ninstances of opinion spam, in this work we study deceptive opinion\\nspam---fictitious opinions that have been deliberately written to sound\\nauthentic. Integrating work from psychology and computational linguistics, we\\ndevelop and compare three approaches to detecting deceptive opinion spam, and\\nultimately develop a classifier that is nearly 90% accurate on our\\ngold-standard opinion spam dataset. Based on feature analysis of our learned\\nmodels, we additionally make several theoretical contributions, including\\nrevealing a relationship between deceptive opinions and imaginative writing.\\n',\n",
              " '  Model-based language specification has applications in the implementation of\\nlanguage processors, the design of domain-specific languages, model-driven\\nsoftware development, data integration, text mining, natural language\\nprocessing, and corpus-based induction of models. Model-based language\\nspecification decouples language design from language processing and, unlike\\ntraditional grammar-driven approaches, which constrain language designers to\\nspecific kinds of grammars, it needs general parser generators able to deal\\nwith ambiguities. In this paper, we propose Fence, an efficient bottom-up\\nparsing algorithm with lexical and syntactic ambiguity support that enables the\\nuse of model-based language specification in practice.\\n',\n",
              " '  We describe a new semantic relatedness measure combining the Wikipedia-based\\nExplicit Semantic Analysis measure, the WordNet path measure and the mixed\\ncollocation index. Our measure achieves the currently highest results on the\\nWS-353 test: a Spearman rho coefficient of 0.79 (vs. 0.75 in (Gabrilovich and\\nMarkovitch, 2007)) when applying the measure directly, and a value of 0.87 (vs.\\n0.78 in (Agirre et al., 2009)) when using the prediction of a polynomial SVM\\nclassifier trained on our measure.\\n  In the appendix we discuss the adaptation of ESA to 2011 Wikipedia data, as\\nwell as various unsuccessful attempts to enhance ESA by filtering at word,\\nsentence, and section level.\\n',\n",
              " \"  Diacritical marks play a crucial role in meeting the criteria of usability of\\ntypographic text, such as: homogeneity, clarity and legibility. To change the\\ndiacritic of a letter in a word could completely change its semantic. The\\nsituation is very complicated with multilingual text. Indeed, the problem of\\ndesign becomes more difficult by the presence of diacritics that come from\\nvarious scripts; they are used for different purposes, and are controlled by\\nvarious typographic rules. It is quite challenging to adapt rules from one\\nscript to another. This paper aims to study the placement and sizing of\\ndiacritical marks in Arabic script, with a comparison with the Latin's case.\\nThe Arabic script is cursive and runs from right-to-left; its criteria and\\nrules are quite distinct from those of the Latin script. In the beginning, we\\ncompare the difficulty of processing diacritics in both scripts. After, we will\\nstudy the limits of Latin resolution strategies when applied to Arabic. At the\\nend, we propose an approach to resolve the problem for positioning and resizing\\ndiacritics. This strategy includes creating an Arabic font, designed in\\nOpenType format, along with suitable justification in TEX.\\n\",\n",
              " '  The interest in text to speech synthesis increased in the world .text to\\nspeech have been developed formany popular languages such as English, Spanish\\nand French and many researches and developmentshave been applied to those\\nlanguages. Persian on the other hand, has been given little attentioncompared\\nto other languages of similar importance and the research in Persian is still\\nin its infancy.Persian language possess many difficulty and exceptions that\\nincrease complexity of text to speechsystems. For example: short vowels is\\nabsent in written text or existence of homograph words. in thispaper we propose\\na new method for persian text to phonetic that base on pronunciations by\\nanalogy inwords, semantic relations and grammatical rules for finding proper\\nphonetic. Keywords:PbA, text to speech, Persian language, FPbA\\n',\n",
              " \"  We propose NEMO, a system for extracting organization names in the\\naffiliation and normalizing them to a canonical organization name. Our parsing\\nprocess involves multi-layered rule matching with multiple dictionaries. The\\nsystem achieves more than 98% f-score in extracting organization names. Our\\nprocess of normalization that involves clustering based on local sequence\\nalignment metrics and local learning based on finding connected components. A\\nhigh precision was also observed in normalization. NEMO is the missing link in\\nassociating each biomedical paper and its authors to an organization name in\\nits canonical form and the Geopolitical location of the organization. This\\nresearch could potentially help in analyzing large social networks of\\norganizations for landscaping a particular topic, improving performance of\\nauthor disambiguation, adding weak links in the co-author network of authors,\\naugmenting NLM's MARS system for correcting errors in OCR output of affiliation\\nfield, and automatically indexing the PubMed citations with the normalized\\norganization name and country. Our system is available as a graphical user\\ninterface available for download along with this paper.\\n\",\n",
              " '  BioSimplify is an open source tool written in Java that introduces and\\nfacilitates the use of a novel model for sentence simplification tuned for\\nautomatic discourse analysis and information extraction (as opposed to sentence\\nsimplification for improving human readability). The model is based on a\\n\"shot-gun\" approach that produces many different (simpler) versions of the\\noriginal sentence by combining variants of its constituent elements. This tool\\nis optimized for processing biomedical scientific literature such as the\\nabstracts indexed in PubMed. We tested our tool on its impact to the task of\\nPPI extraction and it improved the f-score of the PPI tool by around 7%, with\\nan improvement in recall of around 20%. The BioSimplify tool and test corpus\\ncan be downloaded from https://biosimplify.sourceforge.net.\\n',\n",
              " '  Overall, the two main contributions of this work include the application of\\nsentence simplification to association extraction as described above, and the\\nuse of distributional semantics for concept extraction. The proposed work on\\nconcept extraction amalgamates for the first time two diverse research areas\\n-distributional semantics and information extraction. This approach renders all\\nthe advantages offered in other semi-supervised machine learning systems, and,\\nunlike other proposed semi-supervised approaches, it can be used on top of\\ndifferent basic frameworks and algorithms.\\nhttp://gradworks.umi.com/34/49/3449837.html\\n',\n",
              " '  In this paper we consider the problem of efficient computation of\\ncross-moments of a vector random variable represented by a stochastic\\ncontext-free grammar. Two types of cross-moments are discussed. The sample\\nspace for the first one is the set of all derivations of the context-free\\ngrammar, and the sample space for the second one is the set of all derivations\\nwhich generate a string belonging to the language of the grammar. In the past,\\nthis problem was widely studied, but mainly for the cross-moments of scalar\\nvariables and up to the second order. This paper presents new algorithms for\\ncomputing the cross-moments of an arbitrary order, and the previously developed\\nones are derived as special cases.\\n',\n",
              " '  This paper introduces, an XML format developed to serialise the object model\\ndefined by the ISO Syntactic Annotation Framework SynAF. Based on widespread\\nbest practices we adapt a popular XML format for syntactic annotation,\\nTigerXML, with additional features to support a variety of syntactic phenomena\\nincluding constituent and dependency structures, binding, and different node\\ntypes such as compounds or empty elements. We also define interfaces to other\\nformats and standards including the Morpho-syntactic Annotation Framework MAF\\nand the ISOCat Data Category Registry. Finally a case study of the German\\nTreebank TueBa-D/Z is presented, showcasing the handling of constituent\\nstructures, topological fields and coreference annotation in tandem.\\n',\n",
              " \"  The usefulness of annotated corpora is greatly increased if there is an\\nassociated tool that can allow various kinds of operations to be performed in a\\nsimple way. Different kinds of annotation frameworks and many query languages\\nfor them have been proposed, including some to deal with multiple layers of\\nannotation. We present here an easy to learn query language for a particular\\nkind of annotation framework based on 'threaded trees', which are somewhere\\nbetween the complete order of a tree and the anarchy of a graph. Through\\n'typed' threads, they can allow multiple levels of annotation in the same\\ndocument. Our language has a simple, intuitive and concise syntax and high\\nexpressive power. It allows not only to search for complicated patterns with\\nshort queries but also allows data manipulation and specification of arbitrary\\nreturn values. Many of the commonly used tasks that otherwise require writing\\nprograms, can be performed with one or more queries. We compare the language\\nwith some others and try to evaluate it.\\n\",\n",
              " '  We present a system to translate natural language sentences to formulas in a\\nformal or a knowledge representation language. Our system uses two inverse\\nlambda-calculus operators and using them can take as input the semantic\\nrepresentation of some words, phrases and sentences and from that derive the\\nsemantic representation of other words and phrases. Our inverse lambda operator\\nworks on many formal languages including first order logic, database query\\nlanguages and answer set programming. Our system uses a syntactic combinatorial\\ncategorial parser to parse natural language sentences and also to construct the\\nsemantic meaning of the sentences as directed by their parsing. The same parser\\nis used for both. In addition to the inverse lambda-calculus operators, our\\nsystem uses a notion of generalization to learn semantic representation of\\nwords from the semantic representation of other words that are of the same\\ncategory. Together with this, we use an existing statistical learning approach\\nto assign weights to deal with multiple meanings of words. Our system produces\\nimproved results on standard corpora on natural language interfaces for robot\\ncommand and control and database queries.\\n',\n",
              " '  For a system to understand natural language, it needs to be able to take\\nnatural language text and answer questions given in natural language with\\nrespect to that text; it also needs to be able to follow instructions given in\\nnatural language. To achieve this, a system must be able to process natural\\nlanguage and be able to capture the knowledge within that text. Thus it needs\\nto be able to translate natural language text into a formal language. We\\ndiscuss our approach to do this, where the translation is achieved by composing\\nthe meaning of words in a sentence. Our initial approach uses an inverse lambda\\nmethod that we developed (and other methods) to learn meaning of words from\\nmeaning of sentences and an initial lexicon. We then present an improved method\\nwhere the initial lexicon is also learned by analyzing the training sentence\\nand meaning pairs. We evaluate our methods and compare them with other existing\\nmethods on a corpora of database querying and robot command and control.\\n',\n",
              " '  We present a system capable of automatically solving combinatorial logic\\npuzzles given in (simplified) English. It involves translating the English\\ndescriptions of the puzzles into answer set programming(ASP) and using ASP\\nsolvers to provide solutions of the puzzles. To translate the descriptions, we\\nuse a lambda-calculus based approach using Probabilistic Combinatorial\\nCategorial Grammars (PCCG) where the meanings of words are associated with\\nparameters to be able to distinguish between multiple meanings of the same\\nword. Meaning of many words and the parameters are learned. The puzzles are\\nrepresented in ASP using an ontology which is applicable to a large set of\\nlogic puzzles.\\n',\n",
              " '  This paper investigates the efficiency of the EWC semantic relatedness\\nmeasure in an ad-hoc retrieval task. This measure combines the Wikipedia-based\\nExplicit Semantic Analysis measure, the WordNet path measure and the mixed\\ncollocation index. In the experiments, the open source search engine Terrier\\nwas utilised as a tool to index and retrieve data. The proposed technique was\\ntested on the NTCIR data collection. The experiments demonstrated promising\\nresults.\\n',\n",
              " '  Selection through iterated learning explains no more than other\\nnon-functional accounts, such as universal grammar, why language is so\\nwell-designed for communicative efficiency. It does not predict several\\ndistinctive features of language like central embedding, large lexicons or the\\nlack of iconicity, that seem to serve communication purposes at the expense of\\nlearnability.\\n',\n",
              " '  In this article, we present a corpus of dialogues between a schizophrenic\\nspeaker and an interlocutor who drives the dialogue. We had identified specific\\ndiscontinuities for paranoid schizophrenics. We propose a modeling of these\\ndiscontinuities with S-DRT (its pragmatic part)\\n',\n",
              " '  We present a framework which constructs an event-style dis- course semantics.\\nThe discourse dynamics are encoded in continuation semantics and various\\nrhetorical relations are embedded in the resulting interpretation of the\\nframework. We assume discourse and sentence are distinct semantic objects, that\\nplay different roles in meaning evalua- tion. Moreover, two sets of composition\\nfunctions, for handling different discourse relations, are introduced. The\\npaper first gives the necessary background and motivation for event and dynamic\\nsemantics, then the framework with detailed examples will be introduced.\\n',\n",
              " \"  This article presents an extension of Minimalist Categorial Gram- mars (MCG)\\nto encode Chomsky's phases. These grammars are based on Par- tially Commutative\\nLogic (PCL) and encode properties of Minimalist Grammars (MG) of Stabler. The\\nfirst implementation of MCG were using both non- commutative properties (to\\nrespect the linear word order in an utterance) and commutative ones (to model\\nfeatures of different constituents). Here, we pro- pose to adding Chomsky's\\nphases with the non-commutative tensor product of the logic. Then we could give\\naccount of the PIC just by using logical prop- erties of the framework.\\n\",\n",
              " '  Stabler proposes an implementation of the Chomskyan Minimalist Program,\\nChomsky 95 with Minimalist Grammars - MG, Stabler 97. This framework inherits a\\nlong linguistic tradition. But the semantic calculus is more easily added if\\none uses the Curry-Howard isomorphism. Minimalist Categorial Grammars - MCG,\\nbased on an extension of the Lambek calculus, the mixed logic, were introduced\\nto provide a theoretically-motivated syntax-semantics interface, Amblard 07. In\\nthis article, we give full definitions of MG with algebraic tree descriptions\\nand of MCG, and take the first steps towards giving a proof of inclusion of\\ntheir generated languages.\\n',\n",
              " '  We perform a statistical analysis of emotionally annotated comments in two\\nlarge online datasets, examining chains of consecutive posts in the\\ndiscussions. Using comparisons with randomised data we show that there is a\\nhigh level of correlation for the emotional content of messages.\\n',\n",
              " '  Formality is one of the most important dimensions of writing style variation.\\nIn this study we conducted an inter-rater reliability experiment for assessing\\nsentence formality on a five-point Likert scale, and obtained good agreement\\nresults as well as different rating distributions for different sentence\\ncategories. We also performed a difficulty analysis to identify the bottlenecks\\nof our rating procedure. Our main objective is to design an automatic scoring\\nmechanism for sentence-level formality, and this study is important for that\\npurpose.\\n',\n",
              " '  This paper presents a method to understand spoken Tunisian dialect based on\\nlexical semantic. This method takes into account the specificity of the\\nTunisian dialect which has no linguistic processing tools. This method is\\nontology-based which allows exploiting the ontological concepts for semantic\\nannotation and ontological relations for speech interpretation. This\\ncombination increases the rate of comprehension and limits the dependence on\\nlinguistic resources. This paper also details the process of building the\\nontology used for annotation and interpretation of Tunisian dialect in the\\ncontext of speech understanding in dialogue systems for restricted domain.\\n',\n",
              " '  We introduce a stochastic graph-based method for computing relative\\nimportance of textual units for Natural Language Processing. We test the\\ntechnique on the problem of Text Summarization (TS). Extractive TS relies on\\nthe concept of sentence salience to identify the most important sentences in a\\ndocument or set of documents. Salience is typically defined in terms of the\\npresence of particular important words or in terms of similarity to a centroid\\npseudo-sentence. We consider a new approach, LexRank, for computing sentence\\nimportance based on the concept of eigenvector centrality in a graph\\nrepresentation of sentences. In this model, a connectivity matrix based on\\nintra-sentence cosine similarity is used as the adjacency matrix of the graph\\nrepresentation of sentences. Our system, based on LexRank ranked in first place\\nin more than one task in the recent DUC 2004 evaluation. In this paper we\\npresent a detailed analysis of our approach and apply it to a larger data set\\nincluding data from earlier DUC evaluations. We discuss several methods to\\ncompute centrality using the similarity graph. The results show that\\ndegree-based methods (including LexRank) outperform both centroid-based methods\\nand other systems participating in DUC in most of the cases. Furthermore, the\\nLexRank with threshold method outperforms the other degree-based techniques\\nincluding continuous LexRank. We also show that our approach is quite\\ninsensitive to the noise in the data that may result from an imperfect topical\\nclustering of documents.\\n',\n",
              " '  In this paper we concentrate on the resolution of the lexical ambiguity that\\narises when a given word has several different meanings. This specific task is\\ncommonly referred to as word sense disambiguation (WSD). The task of WSD\\nconsists of assigning the correct sense to words using an electronic dictionary\\nas the source of word definitions. We present two WSD methods based on two main\\nmethodological approaches in this research area: a knowledge-based method and a\\ncorpus-based method. Our hypothesis is that word-sense disambiguation requires\\nseveral knowledge sources in order to solve the semantic ambiguity of the\\nwords. These sources can be of different kinds--- for example, syntagmatic,\\nparadigmatic or statistical information. Our approach combines various sources\\nof knowledge, through combinations of the two WSD methods mentioned above.\\nMainly, the paper concentrates on how to combine these methods and sources of\\ninformation in order to achieve good results in the disambiguation. Finally,\\nthis paper presents a comprehensive study and experimental work on evaluation\\nof the methods and their combinations.\\n',\n",
              " '  A fundamental requirement of any task-oriented dialogue system is the ability\\nto generate object descriptions that refer to objects in the task domain. The\\nsubproblem of content selection for object descriptions in task-oriented\\ndialogue has been the focus of much previous work and a large number of models\\nhave been proposed. In this paper, we use the annotated COCONUT corpus of\\ntask-oriented design dialogues to develop feature sets based on Dale and\\nReiters (1995) incremental model, Brennan and Clarks (1996) conceptual pact\\nmodel, and Jordans (2000b) intentional influences model, and use these feature\\nsets in a machine learning experiment to automatically learn a model of content\\nselection for object descriptions. Since Dale and Reiters model requires a\\nrepresentation of discourse structure, the corpus annotations are used to\\nderive a representation based on Grosz and Sidners (1986) theory of the\\nintentional structure of discourse, as well as two very simple representations\\nof discourse structure based purely on recency. We then apply the\\nrule-induction program RIPPER to train and test the content selection component\\nof an object description generator on a set of 393 object descriptions from the\\ncorpus. To our knowledge, this is the first reported experiment of a trainable\\ncontent selection component for object description generation in dialogue.\\nThree separate content selection models that are based on the three theoretical\\nmodels, all independently achieve accuracies significantly above the majority\\nclass baseline (17%) on unseen test data, with the intentional influences model\\n(42.4%) performing significantly better than either the incremental model\\n(30.4%) or the conceptual pact model (28.9%). But the best performing models\\ncombine all the feature sets, achieving accuracies near 60%. Surprisingly, a\\nsimple recency-based representation of discourse structure does as well as one\\nbased on intentional structure. To our knowledge, this is also the first\\nempirical comparison of a representation of Grosz and Sidners model of\\ndiscourse structure with a simpler model for any generation task.\\n',\n",
              " '  In this paper we present a framework to analyze conflicts of contracts\\nwritten in structured English. A contract that has manually been rewritten in a\\nstructured English is automatically translated into a formal language using the\\nGrammatical Framework (GF). In particular we use the contract language CL as a\\ntarget formal language for this translation. In our framework CL specifications\\ncould then be input into the tool CLAN to detect the presence of conflicts\\n(whether there are contradictory obligations, permissions, and prohibitions. We\\nalso use GF to get a version in (restricted) English of CL formulae. We discuss\\nthe implementation of such a framework.\\n',\n",
              " '  The relationship between written and spoken words is convoluted in languages\\nwith a deep orthography such as English and therefore it is difficult to devise\\nexplicit rules for generating the pronunciations for unseen words.\\nPronunciation by analogy (PbA) is a data-driven method of constructing\\npronunciations for novel words from concatenated segments of known words and\\ntheir pronunciations. PbA performs relatively well with English and outperforms\\nseveral other proposed methods. However, the best published word accuracy of\\n65.5% (for the 20,000 word NETtalk corpus) suggests there is much room for\\nimprovement in it.\\n  Previous PbA algorithms have used several different scoring strategies such\\nas the product of the frequencies of the component pronunciations of the\\nsegments, or the number of different segmentations that yield the same\\npronunciation, and different combinations of these methods, to evaluate the\\ncandidate pronunciations. In this article, we instead propose to use a\\nprobabilistically justified scoring rule. We show that this principled approach\\nalone yields better accuracy (66.21% for the NETtalk corpus) than any\\npreviously published PbA algorithm. Furthermore, combined with certain ad hoc\\nmodifications motivated by earlier algorithms, the performance climbs up to\\n66.6%, and further improvements are possible by combining this method with\\nother methods.\\n',\n",
              " \"  Since 2006 we have undertaken to describe the differences between 17th\\ncentury English and contemporary English thanks to NLP software. Studying a\\ncorpus spanning the whole century (tales of English travellers in the Ottoman\\nEmpire in the 17th century, Mary Astell's essay A Serious Proposal to the\\nLadies and other literary texts) has enabled us to highlight various lexical,\\nmorphological or grammatical singularities. Thanks to the NooJ linguistic\\nplatform, we created dictionaries indexing the lexical variants and their\\ntranscription in CE. The latter is often the result of the validation of forms\\nrecognized dynamically by morphological graphs. We also built syntactical\\ngraphs aimed at transcribing certain archaic forms in contemporary English. Our\\nprevious research implied a succession of elementary steps alternating textual\\nanalysis and result validation. We managed to provide examples of\\ntranscriptions, but we have not created a global tool for automatic\\ntranscription. Therefore we need to focus on the results we have obtained so\\nfar, study the conditions for creating such a tool, and analyze possible\\ndifficulties. In this paper, we will be discussing the technical and linguistic\\naspects we have not yet covered in our previous work. We are using the results\\nof previous research and proposing a transcription method for words or\\nsequences identified as archaic.\\n\",\n",
              " '  A new approach to the problem of natural language understanding is proposed.\\nThe knowledge domain under consideration is the social behavior of people.\\nEnglish sentences are translated into set of predicates of a semantic database,\\nwhich describe persons, occupations, organizations, projects, actions, events,\\nmessages, machines, things, animals, location and time of actions, relations\\nbetween objects, thoughts, cause-and-effect relations, abstract objects. There\\nis a knowledge base containing the description of semantics of objects\\n(functions and structure), actions (motives and causes), and operations.\\n',\n",
              " '  We show that information about social relationships can be used to improve\\nuser-level sentiment analysis. The main motivation behind our approach is that\\nusers that are somehow \"connected\" may be more likely to hold similar opinions;\\ntherefore, relationship information can complement what we can extract about a\\nuser\\'s viewpoints from their utterances. Employing Twitter as a source for our\\nexperimental data, and working within a semi-supervised framework, we propose\\nmodels that are induced either from the Twitter follower/followee network or\\nfrom the network in Twitter formed by users referring to each other using \"@\"\\nmentions. Our transductive learning results reveal that incorporating\\nsocial-network information can indeed lead to statistically significant\\nsentiment-classification improvements over the performance of an approach based\\non Support Vector Machines having access only to textual features.\\n',\n",
              " '  Machine transliteration is a method for automatically converting words in one\\nlanguage into phonetically equivalent ones in another language. Machine\\ntransliteration plays an important role in natural language applications such\\nas information retrieval and machine translation, especially for handling\\nproper nouns and technical terms. Four machine transliteration models --\\ngrapheme-based transliteration model, phoneme-based transliteration model,\\nhybrid transliteration model, and correspondence-based transliteration model --\\nhave been proposed by several researchers. To date, however, there has been\\nlittle research on a framework in which multiple transliteration models can\\noperate simultaneously. Furthermore, there has been no comparison of the four\\nmodels within the same framework and using the same data. We addressed these\\nproblems by 1) modeling the four models within the same framework, 2) comparing\\nthem under the same conditions, and 3) developing a way to improve machine\\ntransliteration through this comparison. Our comparison showed that the hybrid\\nand correspondence-based models were the most effective and that the four\\nmodels can be used in a complementary manner to improve machine transliteration\\nperformance.\\n',\n",
              " '  In this paper we propose a data intensive approach for inferring\\nsentence-internal temporal relations. Temporal inference is relevant for\\npractical NLP applications which either extract or synthesize temporal\\ninformation (e.g., summarisation, question answering). Our method bypasses the\\nneed for manual coding by exploiting the presence of markers like after\", which\\novertly signal a temporal relation. We first show that models trained on main\\nand subordinate clauses connected with a temporal marker achieve good\\nperformance on a pseudo-disambiguation task simulating temporal inference\\n(during testing the temporal marker is treated as unseen and the models must\\nselect the right marker from a set of possible candidates). Secondly, we assess\\nwhether the proposed approach holds promise for the semi-automatic creation of\\ntemporal annotations. Specifically, we use a model trained on noisy and\\napproximate data (i.e., main and subordinate clauses) to predict\\nintra-sentential relations present in TimeBank, a corpus annotated rich\\ntemporal information. Our experiments compare and contrast several\\nprobabilistic models differing in their feature space, linguistic assumptions\\nand data requirements. We evaluate performance against gold standard corpora\\nand also against human subjects.\\n',\n",
              " \"  Product review nowadays has become an important source of information, not\\nonly for customers to find opinions about products easily and share their\\nreviews with peers, but also for product manufacturers to get feedback on their\\nproducts. As the number of product reviews grows, it becomes difficult for\\nusers to search and utilize these resources in an efficient way. In this work,\\nwe build a product review summarization system that can automatically process a\\nlarge collection of reviews and aggregate them to generate a concise summary.\\nMore importantly, the drawback of existing product summarization systems is\\nthat they cannot provide the underlying reasons to justify users' opinions. In\\nour method, we solve this problem by applying clustering, prior to selecting\\nrepresentative candidates for summarization.\\n\",\n",
              " '  Traditional language processing tools constrain language designers to\\nspecific kinds of grammars. In contrast, model-based language specification\\ndecouples language design from language processing. As a consequence,\\nmodel-based language specification tools need general parsers able to parse\\nunrestricted context-free grammars. As languages specified following this\\napproach may be ambiguous, parsers must deal with ambiguities. Model-based\\nlanguage specification also allows the definition of associativity, precedence,\\nand custom constraints. Therefore parsers generated by model-driven language\\nspecification tools need to enforce constraints. In this paper, we propose\\nFence, an efficient bottom-up chart parser with lexical and syntactic ambiguity\\nsupport that allows the specification of constraints and, therefore, enables\\nthe use of model-based language specification in practice.\\n',\n",
              " '  The goal of the present chapter is to explore the possibility of providing\\nthe research (but also the industrial) community that commonly uses spoken\\ncorpora with a stable portfolio of well-documented standardised formats that\\nallow a high re-use rate of annotated spoken resources and, as a consequence,\\nbetter interoperability across tools used to produce or exploit such resources.\\n',\n",
              " '  In anaphora resolution for English, animacy identification can play an\\nintegral role in the application of agreement restrictions between pronouns and\\ncandidates, and as a result, can improve the accuracy of anaphora resolution\\nsystems. In this paper, two methods for animacy identification are proposed and\\nevaluated using intrinsic and extrinsic measures. The first method is a\\nrule-based one which uses information about the unique beginners in WordNet to\\nclassify NPs on the basis of their animacy. The second method relies on a\\nmachine learning algorithm which exploits a WordNet enriched with animacy\\ninformation for each sense. The effect of word sense disambiguation on the two\\nmethods is also assessed. The intrinsic evaluation reveals that the machine\\nlearning method reaches human levels of performance. The extrinsic evaluation\\ndemonstrates that animacy identification can be beneficial in anaphora\\nresolution, especially in the cases where animate entities are identified with\\nhigh precision.\\n',\n",
              " '  Background: Online news reports are increasingly becoming a source for event\\nbased early warning systems that detect natural disasters. Harnessing the\\nmassive volume of information available from multilingual newswire presents as\\nmany challenges as opportunities due to the patterns of reporting complex\\nspatiotemporal events. Results: In this article we study the problem of\\nutilising correlated event reports across languages. We track the evolution of\\n16 disease outbreaks using 5 temporal aberration detection algorithms on\\ntext-mined events classified according to disease and outbreak country. Using\\nProMED reports as a silver standard, comparative analysis of news data for 13\\nlanguages over a 129 day trial period showed improved sensitivity, F1 and\\ntimeliness across most models using cross-lingual events. We report a detailed\\ncase study analysis for Cholera in Angola 2010 which highlights the challenges\\nfaced in correlating news events with the silver standard. Conclusions: The\\nresults show that automated health surveillance using multilingual text mining\\nhas the potential to turn low value news into high value alerts if informed\\nchoices are used to govern the selection of models and data sources. An\\nimplementation of the C2 alerting algorithm using multilingual news is\\navailable at the BioCaster portal http://born.nii.ac.jp/?page=globalroundup.\\n',\n",
              " \"  Background: Micro-blogging services such as Twitter offer the potential to\\ncrowdsource epidemics in real-time. However, Twitter posts ('tweets') are often\\nambiguous and reactive to media trends. In order to ground user messages in\\nepidemic response we focused on tracking reports of self-protective behaviour\\nsuch as avoiding public gatherings or increased sanitation as the basis for\\nfurther risk analysis. Results: We created guidelines for tagging self\\nprotective behaviour based on Jones and Salath\\\\'e (2009)'s behaviour response\\nsurvey. Applying the guidelines to a corpus of 5283 Twitter messages related to\\ninfluenza like illness showed a high level of inter-annotator agreement (kappa\\n0.86). We employed supervised learning using unigrams, bigrams and regular\\nexpressions as features with two supervised classifiers (SVM and Naive Bayes)\\nto classify tweets into 4 self-reported protective behaviour categories plus a\\nself-reported diagnosis. In addition to classification performance we report\\nmoderately strong Spearman's Rho correlation by comparing classifier output\\nagainst WHO/NREVSS laboratory data for A(H1N1) in the USA during the 2009-2010\\ninfluenza season. Conclusions: The study adds to evidence supporting a high\\ndegree of correlation between pre-diagnostic social media signals and\\ndiagnostic influenza case data, pointing the way towards low cost sensor\\nnetworks. We believe that the signals we have modelled may be applicable to a\\nwide range of diseases.\\n\",\n",
              " '  Background: Accurate and timely detection of public health events of\\ninternational concern is necessary to help support risk assessment and response\\nand save lives. Novel event-based methods that use the World Wide Web as a\\nsignal source offer potential to extend health surveillance into areas where\\ntraditional indicator networks are lacking. In this paper we address the issue\\nof systematically evaluating online health news to support automatic alerting\\nusing daily disease-country counts text mined from real world data using\\nBioCaster. For 18 data sets produced by BioCaster, we compare 5 aberration\\ndetection algorithms (EARS C2, C3, W2, F-statistic and EWMA) for performance\\nagainst expert moderated ProMED-mail postings. Results: We report sensitivity,\\nspecificity, positive predictive value (PPV), negative predictive value (NPV),\\nmean alerts/100 days and F1, at 95% confidence interval (CI) for 287\\nProMED-mail postings on 18 outbreaks across 14 countries over a 366 day period.\\nResults indicate that W2 had the best F1 with a slight benefit for day of week\\neffect over C2. In drill down analysis we indicate issues arising from the\\ngranular choice of country-level modeling, sudden drops in reporting due to day\\nof week effects and reporting bias. Automatic alerting has been implemented in\\nBioCaster available from http://born.nii.ac.jp. Conclusions: Online health news\\nalerts have the potential to enhance manual analytical methods by increasing\\nthroughput, timeliness and detection rates. Systematic evaluation of health\\nnews aberrations is necessary to push forward our understanding of the complex\\nrelationship between news report volumes and case numbers and to select the\\nbest performing features and algorithms.\\n',\n",
              " '  Recent studies have shown strong correlation between social networking data\\nand national influenza rates. We expanded upon this success to develop an\\nautomated text mining system that classifies Twitter messages in real time into\\nsix syndromic categories based on key terms from a public health ontology.\\n10-fold cross validation tests were used to compare Naive Bayes (NB) and\\nSupport Vector Machine (SVM) models on a corpus of 7431 Twitter messages. SVM\\nperformed better than NB on 4 out of 6 syndromes. The best performing\\nclassifiers showed moderately strong F1 scores: respiratory = 86.2 (NB);\\ngastrointestinal = 85.4 (SVM polynomial kernel degree 2); neurological = 88.6\\n(SVM polynomial kernel degree 1); rash = 86.0 (SVM polynomial kernel degree 1);\\nconstitutional = 89.3 (SVM polynomial kernel degree 1); hemorrhagic = 89.9\\n(NB). The resulting classifiers were deployed together with an EARS C2\\naberration detection algorithm in an experimental online system.\\n',\n",
              " '  We show that the frequency of word use is not only determined by the word\\nlength \\\\cite{Zipf1935} and the average information content\\n\\\\cite{Piantadosi2011}, but also by its emotional content. We have analyzed\\nthree established lexica of affective word usage in English, German, and\\nSpanish, to verify that these lexica have a neutral, unbiased, emotional\\ncontent. Taking into account the frequency of word usage, we find that words\\nwith a positive emotional content are more frequently used. This lends support\\nto Pollyanna hypothesis \\\\cite{Boucher1969} that there should be a positive bias\\nin human expression. We also find that negative words contain more information\\nthan positive words, as the informativeness of a word increases uniformly with\\nits valence decrease. Our findings support earlier conjectures about (i) the\\nrelation between word frequency and information content, and (ii) the impact of\\npositive emotions on communication and social links.\\n',\n",
              " \"  This paper presents a novel algorithm to compute sentiment orientation of\\nChinese sentiment word. The algorithm uses ideograms which are a distinguishing\\nfeature of Chinese language. The proposed algorithm can be applied to any\\nsentiment classification scheme. To compute a word's sentiment orientation\\nusing the proposed algorithm, only the word itself and a precomputed character\\nontology is required, rather than a corpus. The influence of three parameters\\nover the algorithm performance is analyzed and verified by experiment.\\nExperiment also shows that proposed algorithm achieves an F Measure of 85.02%\\noutperforming existing ideogram based algorithm.\\n\",\n",
              " '  One of the biggest challenges in the development and deployment of spoken\\ndialogue systems is the design of the spoken language generation module. This\\nchallenge arises from the need for the generator to adapt to many features of\\nthe dialogue domain, user population, and dialogue context. A promising\\napproach is trainable generation, which uses general-purpose linguistic\\nknowledge that is automatically adapted to the features of interest, such as\\nthe application domain, individual user, or user group. In this paper we\\npresent and evaluate a trainable sentence planner for providing restaurant\\ninformation in the MATCH dialogue system. We show that trainable sentence\\nplanning can produce complex information presentations whose quality is\\ncomparable to the output of a template-based generator tuned to this domain. We\\nalso show that our method easily supports adapting the sentence planner to\\nindividuals, and that the individualized sentence planners generally perform\\nbetter than models trained and tested on a population of individuals. Previous\\nwork has documented and utilized individual preferences for content selection,\\nbut to our knowledge, these results provide the first demonstration of\\nindividual preferences for sentence planning operations, affecting the content\\norder, discourse structure and sentence structure of system responses. Finally,\\nwe evaluate the contribution of different feature sets, and show that, in our\\napplication, n-gram features often do as well as features based on higher-level\\nlinguistic representations.\\n',\n",
              " '  This paper introduces context algebras and demonstrates their application to\\ncombining logical and vector-based representations of meaning. Other approaches\\nto this problem attempt to reproduce aspects of logical semantics within new\\nframeworks. The approach we present here is different: We show how logical\\nsemantics can be embedded within a vector space framework, and use this to\\ncombine distributional semantics, in which the meanings of words are\\nrepresented as vectors, with logical semantics, in which the meaning of a\\nsentence is represented as a logical form.\\n',\n",
              " '  This paper deals with the identification of Multiword Expressions (MWEs) in\\nManipuri, a highly agglutinative Indian Language. Manipuri is listed in the\\nEight Schedule of Indian Constitution. MWE plays an important role in the\\napplications of Natural Language Processing(NLP) like Machine Translation, Part\\nof Speech tagging, Information Retrieval, Question Answering etc. Feature\\nselection is an important factor in the recognition of Manipuri MWEs using\\nConditional Random Field (CRF). The disadvantage of manual selection and\\nchoosing of the appropriate features for running CRF motivates us to think of\\nGenetic Algorithm (GA). Using GA we are able to find the optimal features to\\nrun the CRF. We have tried with fifty generations in feature selection along\\nwith three fold cross validation as fitness function. This model demonstrated\\nthe Recall (R) of 64.08%, Precision (P) of 86.84% and F-measure (F) of 73.74%,\\nshowing an improvement over the CRF based Manipuri MWE identification without\\nGA application.\\n',\n",
              " '  This paper presents the preliminary works to put online a French oral corpus\\nand its transcription. This corpus is the Socio-Linguistic Survey in Orleans,\\nrealized in 1968. First, we numerized the corpus, then we handwritten\\ntranscribed it with the Transcriber software adding different tags about\\nspeakers, time, noise, etc. Each document (audio file and XML file of the\\ntranscription) was described by a set of metadata stored in an XML format to\\nallow an easy consultation. Second, we added different levels of annotations,\\nrecognition of named entities and annotation of personal information about\\nspeakers. This two annotation tasks used the CasSys system of transducer\\ncascades. We used and modified a first cascade to recognize named entities.\\nThen we built a second cascade to annote the designating entities, i.e.\\ninformation about the speaker. These second cascade parsed the named entity\\nannotated corpus. The objective is to locate information about the speaker and,\\nalso, what kind of information can designate him/her. These two cascades was\\nevaluated with precision and recall measures.\\n',\n",
              " '  In this paper, we evaluate various French lexica with the parser FRMG: the\\nLefff, LGLex, the lexicon built from the tables of the French Lexicon-Grammar,\\nthe lexicon DICOVALENCE and a new version of the verbal entries of the Lefff,\\nobtained by merging with DICOVALENCE and partial manual validation. For this,\\nall these lexica have been converted to the format of the Lefff, Alexina\\nformat. The evaluation was made on the part of the EASy corpus used in the\\nfirst evaluation campaign Passage.\\n',\n",
              " '  In this paper, we summerize the work done on the resources of Modern Greek on\\nthe Lexicon-Grammar of verbs. We detail the definitional features of each\\ntable, and all changes made to the names of features to make them consistent.\\nThrough the development of the table of classes, including all the features, we\\nhave considered the conversion of tables in a syntactic lexicon: LGLex. The\\nlexicon, in plain text format or XML, is generated by the LGExtract tool\\n(Constant & Tolone, 2010). This format is directly usable in applications of\\nNatural Language Processing (NLP).\\n',\n",
              " \"  This paper presents a work on extending the adverbial entries of LGLex: a NLP\\noriented syntactic resource for French. Adverbs were extracted from the\\nLexicon-Grammar tables of both simple adverbs ending in -ment '-ly' (Molinier\\nand Levrier, 2000) and compound adverbs (Gross, 1986; 1990). This work relies\\non the exploitation of fine-grained linguistic information provided in existing\\nresources. Various features are encoded in both LG tables and they haven't been\\nexploited yet. They describe the relations of deleting, permuting, intensifying\\nand paraphrasing that associate, on the one hand, the simple and compound\\nadverbs and, on the other hand, different types of compound adverbs. The\\nresulting syntactic resource is manually evaluated and freely available under\\nthe LGPL-LR license.\\n\",\n",
              " '  Algorithms of question answering in a computer system oriented on input and\\nlogical processing of text information are presented. A knowledge domain under\\nconsideration is social behavior of a person. A database of the system includes\\nan internal representation of natural language sentences and supplemental\\ninformation. The answer {\\\\it Yes} or {\\\\it No} is formed for a general question.\\nA special question containing an interrogative word or group of interrogative\\nwords permits to find a subject, object, place, time, cause, purpose and way of\\naction or event. Answer generation is based on identification algorithms of\\npersons, organizations, machines, things, places, and times. Proposed\\nalgorithms of question answering can be realized in information systems closely\\nconnected with text processing (criminology, operation of business, medicine,\\ndocument systems).\\n',\n",
              " '  A tagger is a mandatory segment of most text scrutiny systems, as it\\nconsigned a s yntax class (e.g., noun, verb, adjective, and adverb) to every\\nword in a sentence. In this paper, we present a simple part of speech tagger\\nfor homoeopathy clinical language. This paper reports about the anticipated\\npart of speech tagger for homoeopathy clinical language. It exploit standard\\npattern for evaluating sentences, untagged clinical corpus of 20085 words is\\nused, from which we had selected 125 sentences (2322 tokens). The problem of\\ntagging in natural language processing is to find a way to tag every word in a\\ntext as a meticulous part of speech. The basic idea is to apply a set of rules\\non clinical sentences and on each word, Accuracy is the leading factor in\\nevaluating any POS tagger so the accuracy of proposed tagger is also conversed.\\n',\n",
              " '  Twitter messages often contain so-called hashtags to denote keywords related\\nto them. Using a dataset of 29 million messages, I explore relations among\\nthese hashtags with respect to co-occurrences. Furthermore, I present an\\nattempt to classify hashtags into five intuitive classes, using a\\nmachine-learning approach. The overall outcome is an interactive Web\\napplication to explore Twitter hashtags.\\n',\n",
              " '  This works aims to design a statistical machine translation from English text\\nto American Sign Language (ASL). The system is based on Moses tool with some\\nmodifications and the results are synthesized through a 3D avatar for\\ninterpretation. First, we translate the input text to gloss, a written form of\\nASL. Second, we pass the output to the WebSign Plug-in to play the sign.\\nContributions of this work are the use of a new couple of language English/ASL\\nand an improvement of statistical machine translation based on string matching\\nthanks to Jaro-distance.\\n',\n",
              " '  In this paper we describe function tagging using Transformation Based\\nLearning (TBL) for Myanmar that is a method of extensions to the previous\\nstatistics-based function tagger. Contextual and lexical rules (developed using\\nTBL) were critical in achieving good results. First, we describe a method for\\nexpressing lexical relations in function tagging that statistical function\\ntagging are currently unable to express. Function tagging is the preprocessing\\nstep to show grammatical relations of the sentences. Then we use the context\\nfree grammar technique to clarify the grammatical relations in Myanmar\\nsentences or to output the parse trees. The grammatical relations are the\\nfunctional structure of a language. They rely very much on the function tag of\\nthe tokens. We augment the grammatical relations of Myanmar sentences with\\ntransformation-based learning of function tagging.\\n',\n",
              " \"  Short Message Service (SMS) messages are largely sent directly from one\\nperson to another from their mobile phones. They represent a means of personal\\ncommunication that is an important communicative artifact in our current\\ndigital era. As most existing studies have used private access to SMS corpora,\\ncomparative studies using the same raw SMS data has not been possible up to\\nnow. We describe our efforts to collect a public SMS corpus to address this\\nproblem. We use a battery of methodologies to collect the corpus, paying\\nparticular attention to privacy issues to address contributors' concerns. Our\\nlive project collects new SMS message submissions, checks their quality and\\nadds the valid messages, releasing the resultant corpus as XML and as SQL\\ndumps, along with corpus statistics, every month. We opportunistically collect\\nas much metadata about the messages and their sender as possible, so as to\\nenable different types of analyses. To date, we have collected about 60,000\\nmessages, focusing on English and Mandarin Chinese.\\n\",\n",
              " '  A step-to-step introduction is provided on how to generate a semantic map\\nfrom a collection of messages (full texts, paragraphs or statements) using\\nfreely available software and/or SPSS for the relevant statistics and the\\nvisualization. The techniques are discussed in the various theoretical contexts\\nof (i) linguistics (e.g., Latent Semantic Analysis), (ii) sociocybernetics and\\nsocial systems theory (e.g., the communication of meaning), and (iii)\\ncommunication studies (e.g., framing and agenda-setting). We distinguish\\nbetween the communication of information in the network space (social network\\nanalysis) and the communication of meaning in the vector space. The vector\\nspace can be considered a generated as an architecture by the network of\\nrelations in the network space; words are then not only related, but also\\npositioned. These positions are expected rather than observed and therefore one\\ncan communicate meaning. Knowledge can be generated when these meanings can\\nrecursively be communicated and therefore also further codified.\\n',\n",
              " \"  Grishin's generalization of Lambek's Syntactic Calculus combines a\\nnon-commutative multiplicative conjunction and its residuals (product, left and\\nright division) with a dual family: multiplicative disjunction, right and left\\ndifference. Interaction between these two families takes the form of linear\\ndistributivity principles. We study proof nets for the Lambek-Grishin calculus\\nand the correspondence between these nets and unfocused and focused versions of\\nits sequent calculus.\\n\",\n",
              " '  A formal theory based on a binary operator of directional associative\\nrelation is constructed in the article and an understanding of an associative\\nnormal form of image constructions is introduced. A model of a commutative\\nsemigroup, which provides a presentation of a sentence as three components of\\nan interrogative linguistic image construction, is considered.\\n',\n",
              " '  Researches on signed languages still strongly dissociate lin- guistic issues\\nrelated on phonological and phonetic aspects, and gesture studies for\\nrecognition and synthesis purposes. This paper focuses on the imbrication of\\nmotion and meaning for the analysis, synthesis and evaluation of sign language\\ngestures. We discuss the relevance and interest of a motor theory of perception\\nin sign language communication. According to this theory, we consider that\\nlinguistic knowledge is mapped on sensory-motor processes, and propose a\\nmethodology based on the principle of a synthesis-by-analysis approach, guided\\nby an evaluation process that aims to validate some hypothesis and concepts of\\nthis theory. Examples from existing studies illustrate the di erent concepts\\nand provide avenues for future work.\\n',\n",
              " '  We describe a Context Free Grammar (CFG) for Bangla language and hence we\\npropose a Bangla parser based on the grammar. Our approach is very much general\\nto apply in Bangla Sentences and the method is well accepted for parsing a\\nlanguage of a grammar. The proposed parser is a predictive parser and we\\nconstruct the parse table for recognizing Bangla grammar. Using the parse table\\nwe recognize syntactical mistakes of Bangla sentences when there is no entry\\nfor a terminal in the parse table. If a natural language can be successfully\\nparsed then grammar checking from this language becomes possible. The proposed\\nscheme is based on Top down parsing method and we have avoided the left\\nrecursion of the CFG using the idea of left factoring.\\n',\n",
              " \"  Historically two types of NLP have been investigated: fully automated\\nprocessing of language by machines (NLP) and autonomous processing of natural\\nlanguage by people, i.e. the human brain (psycholinguistics). We believe that\\nthere is room and need for another kind, INLP: interactive natural language\\nprocessing. This intermediate approach starts from peoples' needs, trying to\\nbridge the gap between their actual knowledge and a given goal. Given the fact\\nthat peoples' knowledge is variable and often incomplete, the aim is to build\\nbridges linking a given knowledge state to a given goal. We present some\\nexamples, trying to show that this goal is worth pursuing, achievable and at a\\nreasonable cost.\\n\",\n",
              " '  [This is the translation of paper \"Arborification de Wikip\\\\\\'edia et analyse\\ns\\\\\\'emantique explicite stratifi\\\\\\'ee\" submitted to TALN 2012.]\\n  We present an extension of the Explicit Semantic Analysis method by\\nGabrilovich and Markovitch. Using their semantic relatedness measure, we weight\\nthe Wikipedia categories graph. Then, we extract a minimal spanning tree, using\\nChu-Liu & Edmonds\\' algorithm. We define a notion of stratified tfidf where the\\nstratas, for a given Wikipedia page and a given term, are the classical tfidf\\nand categorical tfidfs of the term in the ancestor categories of the page\\n(ancestors in the sense of the minimal spanning tree). Our method is based on\\nthis stratified tfidf, which adds extra weight to terms that \"survive\" when\\nclimbing up the category tree. We evaluate our method by a text classification\\non the WikiNews corpus: it increases precision by 18%. Finally, we provide\\nhints for future research\\n',\n",
              " '  Algorithms of inference in a computer system oriented to input and semantic\\nprocessing of text information are presented. Such inference is necessary for\\nlogical questions when the direct comparison of objects from a question and\\ndatabase can not give a result. The following classes of problems are\\nconsidered: a check of hypotheses for persons and non-typical actions, the\\ndetermination of persons and circumstances for non-typical actions, planning\\nactions, the determination of event cause and state of persons. To form an\\nanswer both deduction and plausible reasoning are used. As a knowledge domain\\nunder consideration is social behavior of persons, plausible reasoning is based\\non laws of social psychology. Proposed algorithms of inference and plausible\\nreasoning can be realized in computer systems closely connected with text\\nprocessing (criminology, operation of business, medicine, document systems).\\n',\n",
              " '  Here we describe work on learning the subcategories of verbs in a\\nmorphologically rich language using only minimal linguistic resources. Our goal\\nis to learn verb subcategorizations for Quechua, an under-resourced\\nmorphologically rich language, from an unannotated corpus. We compare results\\nfrom applying this approach to an unannotated Arabic corpus with those achieved\\nby processing the same text in treebank form. The original plan was to use only\\na morphological analyzer and an unannotated corpus, but experiments suggest\\nthat this approach by itself will not be effective for learning the\\ncombinatorial potential of Arabic verbs in general. The lower bound on\\nresources for acquiring this information is somewhat higher, apparently\\nrequiring a a part-of-speech tagger and chunker for most languages, and a\\nmorphological disambiguater for Arabic.\\n',\n",
              " '  Sentiment analysis predicts the presence of positive or negative emotions in\\na text document. In this paper we consider higher dimensional extensions of the\\nsentiment concept, which represent a richer set of human emotions. Our approach\\ngoes beyond previous work in that our model contains a continuous manifold\\nrather than a finite set of human emotions. We investigate the resulting model,\\ncompare it to psychological observations, and explore its predictive\\ncapabilities. Besides obtaining significant improvements over a baseline\\nwithout manifold, we are also able to visualize different notions of positive\\nsentiment in different domains.\\n',\n",
              " '  This paper presents the continuation of the work completed by Satori and all.\\n[SCH07] by the realization of an automatic speech recognition system (ASR) for\\nArabic language based SPHINX 4 system. The previous work was limited to the\\nrecognition of the first ten digits, whereas the present work is a remarkable\\nprojection consisting in continuous Arabic speech recognition with a rate of\\nrecognition of surroundings 96%.\\n',\n",
              " '  Lexical ambiguities naturally arise in languages. We present Lamb, a lexical\\nanalyzer that produces a lexical analysis graph describing all the possible\\nsequences of tokens that can be found within the input string. Parsers can\\nprocess such lexical analysis graphs and discard any sequence of tokens that\\ndoes not produce a valid syntactic sentence, therefore performing, together\\nwith Lamb, a context-sensitive lexical analysis in lexically-ambiguous language\\nspecifications.\\n',\n",
              " \"  I pinpoint an interesting similarity between a recent account to rational\\nparsing and the treatment of sequential decisions problems in a dynamical\\nsystems approach. I argue that expectation-driven search heuristics aiming at\\nfast computation resembles a high-risk decision strategy in favor of large\\ntransition velocities. Hale's rational parser, combining generalized\\nleft-corner parsing with informed $\\\\mathrm{A}^*$ search to resolve processing\\nconflicts, explains gardenpath effects in natural sentence processing by\\nmisleading estimates of future processing costs that are to be minimized. On\\nthe other hand, minimizing the duration of cognitive computations in\\ntime-continuous dynamical systems can be described by combining vector space\\nrepresentations of cognitive states by means of filler/role decompositions and\\nsubsequent tensor product representations with the paradigm of stable\\nheteroclinic sequences. Maximizing transition velocities according to a\\nhigh-risk decision strategy could account for a fast race even between states\\nthat are apparently remote in representation space.\\n\",\n",
              " '  Language evolution might have preferred certain prior social configurations\\nover others. Experiments conducted with models of different social structures\\n(varying subgroup interactions and the role of a dominant interlocutor) suggest\\nthat having isolated agent groups rather than an interconnected agent is more\\nadvantageous for the emergence of a social communication system. Distinctive\\ngroups that are closely connected by communication yield systems less like\\nnatural language than fully isolated groups inhabiting the same world.\\nFurthermore, the addition of a dominant male who is asymmetrically favoured as\\na hearer, and equally likely to be a speaker has no positive influence on the\\ndisjoint groups.\\n',\n",
              " '  In this paper, we claim that language is likely to have emerged as a\\nmechanism for coordinating the solution of complex tasks. To confirm this\\nthesis, computer simulations are performed based on the coordination task\\npresented by Garrod & Anderson (1987). The role of success in task-oriented\\ndialogue is analytically evaluated with the help of performance measurements\\nand a thorough lexical analysis of the emergent communication system.\\nSimulation results confirm a strong effect of success mattering on both\\nreliability and dispersion of linguistic conventions.\\n',\n",
              " '  This paper describes a context free grammar (CFG) based grammatical relations\\nfor Myanmar sentences which combine corpus-based function tagging system. Part\\nof the challenge of statistical function tagging for Myanmar sentences comes\\nfrom the fact that Myanmar has free-phrase-order and a complex morphological\\nsystem. Function tagging is a pre-processing step to show grammatical relations\\nof Myanmar sentences. In the task of function tagging, which tags the function\\nof Myanmar sentences with correct segmentation, POS (part-of-speech) tagging\\nand chunking information, we use Naive Bayesian theory to disambiguate the\\npossible function tags of a word. We apply context free grammar (CFG) to find\\nout the grammatical relations of the function tags. We also create a functional\\nannotated tagged corpus for Myanmar and propose the grammar rules for Myanmar\\nsentences. Experiments show that our analysis achieves a good result with\\nsimple sentences and complex sentences.\\n',\n",
              " '  The ability to mimic human notions of semantic distance has widespread\\napplications. Some measures rely only on raw text (distributional measures) and\\nsome rely on knowledge sources such as WordNet. Although extensive studies have\\nbeen performed to compare WordNet-based measures with human judgment, the use\\nof distributional measures as proxies to estimate semantic distance has\\nreceived little attention. Even though they have traditionally performed poorly\\nwhen compared to WordNet-based measures, they lay claim to certain uniquely\\nattractive features, such as their applicability in resource-poor languages and\\ntheir ability to mimic both semantic similarity and semantic relatedness.\\nTherefore, this paper presents a detailed study of distributional measures.\\nParticular attention is paid to flesh out the strengths and limitations of both\\nWordNet-based and distributional measures, and how distributional measures of\\ndistance can be brought more in line with human notions of semantic distance.\\nWe conclude with a brief discussion of recent work on hybrid measures.\\n',\n",
              " '  The automatic ranking of word pairs as per their semantic relatedness and\\nability to mimic human notions of semantic relatedness has widespread\\napplications. Measures that rely on raw data (distributional measures) and\\nthose that use knowledge-rich ontologies both exist. Although extensive studies\\nhave been performed to compare ontological measures with human judgment, the\\ndistributional measures have primarily been evaluated by indirect means. This\\npaper is a detailed study of some of the major distributional measures; it\\nlists their respective merits and limitations. New measures that overcome these\\ndrawbacks, that are more in line with the human notions of semantic\\nrelatedness, are suggested. The paper concludes with an exhaustive comparison\\nof the distributional and ontology-based measures. Along the way, significant\\nresearch problems are identified. Work on these problems may lead to a better\\nunderstanding of how semantic relatedness is to be measured.\\n',\n",
              " \"  The categorization of emotion names, i.e., the grouping of emotion words that\\nhave similar emotional connotations together, is a key tool of Social\\nPsychology used to explore people's knowledge about emotions. Without\\nexception, the studies following that research line were based on the gauging\\nof the perceived similarity between emotion names by the participants of the\\nexperiments. Here we propose and examine a new approach to study the categories\\nof emotion names - the similarities between target emotion names are obtained\\nby comparing the contexts in which they appear in texts retrieved from the\\nWorld Wide Web. This comparison does not account for any explicit semantic\\ninformation; it simply counts the number of common words or lexical items used\\nin the contexts. This procedure allows us to write the entries of the\\nsimilarity matrix as dot products in a linear vector space of contexts. The\\nproperties of this matrix were then explored using Multidimensional Scaling\\nAnalysis and Hierarchical Clustering. Our main findings, namely, the underlying\\ndimension of the emotion space and the categories of emotion names, were\\nconsistent with those based on people's judgments of emotion names\\nsimilarities.\\n\",\n",
              " '  We present the first annotated corpus of nonverbal behaviors in receptionist\\ninteractions, and the first nonverbal corpus (excluding the original video and\\naudio data) of service encounters freely available online. Native speakers of\\nAmerican English and Arabic participated in a naturalistic role play at\\nreception desks of university buildings in Doha, Qatar and Pittsburgh, USA.\\nTheir manually annotated nonverbal behaviors include gaze direction, hand and\\nhead gestures, torso positions, and facial expressions. We discuss possible\\nuses of the corpus and envision it to become a useful tool for the human-robot\\ninteraction community.\\n',\n",
              " '  The study of natural language, especially Arabic, and mechanisms for the\\nimplementation of automatic processing is a fascinating field of study, with\\nvarious potential applications. The importance of tools for natural language\\nprocessing is materialized by the need to have applications that can\\neffectively treat the vast mass of information available nowadays on electronic\\nforms. Among these tools, mainly driven by the necessity of a fast writing in\\nalignment to the actual daily life speed, our interest is on the writing\\nauditors. The morphological and syntactic properties of Arabic make it a\\ndifficult language to master, and explain the lack in the processing tools for\\nthat language. Among these properties, we can mention: the complex structure of\\nthe Arabic word, the agglutinative nature, lack of vocalization, the\\nsegmentation of the text, the linguistic richness, etc.\\n',\n",
              " '  Modern computational linguistic software cannot produce important aspects of\\nsign language translation. Using some researches we deduce that the majority of\\nautomatic sign language translation systems ignore many aspects when they\\ngenerate animation; therefore the interpretation lost the truth information\\nmeaning. Our goals are: to translate written text from any language to ASL\\nanimation; to model maximum raw information using machine learning and\\ncomputational techniques; and to produce a more adapted and expressive form to\\nnatural looking and understandable ASL animations. Our methods include\\nlinguistic annotation of initial text and semantic orientation to generate the\\nfacial expression. We use the genetic algorithms coupled to learning/recognized\\nsystems to produce the most natural form. To detect emotion we are based on\\nfuzzy logic to produce the degree of interpolation between facial expressions.\\nRoughly, we present a new expressive language Text Adapted Sign Modeling\\nLanguage TASML that describes all maximum aspects related to a natural sign\\nlanguage interpretation. This paper is organized as follow: the next section is\\ndevoted to present the comprehension effect of using Space/Time/SVO form in ASL\\nanimation based on experimentation. In section 3, we describe our technical\\nconsiderations. We present the general approach we adopted to develop our tool\\nin section 4. Finally, we give some perspectives and future works.\\n',\n",
              " '  In spite of its robust syntax, semantic cohesion, and less ambiguity, lemma\\nlevel analysis and generation does not yet focused in Arabic NLP literatures.\\nIn the current research, we propose the first non-statistical accurate Arabic\\nlemmatizer algorithm that is suitable for information retrieval (IR) systems.\\nThe proposed lemmatizer makes use of different Arabic language knowledge\\nresources to generate accurate lemma form and its relevant features that\\nsupport IR purposes. As a POS tagger, the experimental results show that, the\\nproposed algorithm achieves a maximum accuracy of 94.8%. For first seen\\ndocuments, an accuracy of 89.15% is achieved, compared to 76.7% of up to date\\nStanford accurate Arabic model, for the same, dataset.\\n',\n",
              " '  Automatic speech processing systems are employed more and more often in real\\nenvironments. Although the underlying speech technology is mostly language\\nindependent, differences between languages with respect to their structure and\\ngrammar have substantial effect on the recognition systems performance. In this\\npaper, we present a review of the latest developments in the sign language\\nrecognition research in general and in the Arabic sign language (ArSL) in\\nspecific. This paper also presents a general framework for improving the deaf\\ncommunity communication with the hearing people that is called SignsWorld. The\\noverall goal of the SignsWorld project is to develop a vision-based technology\\nfor recognizing and translating continuous Arabic sign language ArSL.\\n',\n",
              " '  In this paper, a supervised learning technique for extracting keyphrases of\\nArabic documents is presented. The extractor is supplied with linguistic\\nknowledge to enhance its efficiency instead of relying only on statistical\\ninformation such as term frequency and distance. During analysis, an annotated\\nArabic corpus is used to extract the required lexical features of the document\\nwords. The knowledge also includes syntactic rules based on part of speech tags\\nand allowed word sequences to extract the candidate keyphrases. In this work,\\nthe abstract form of Arabic words is used instead of its stem form to represent\\nthe candidate terms. The Abstract form hides most of the inflections found in\\nArabic words. The paper introduces new features of keyphrases based on\\nlinguistic knowledge, to capture titles and subtitles of a document. A simple\\nANOVA test is used to evaluate the validity of selected features. Then, the\\nlearning model is built using the LDA - Linear Discriminant Analysis - and\\ntraining documents. Although, the presented system is trained using documents\\nin the IT domain, experiments carried out show that it has a significantly\\nbetter performance than the existing Arabic extractor systems, where precision\\nand recall values reach double their corresponding values in the other systems\\nespecially for lengthy and non-scientific articles.\\n',\n",
              " '  This paper gives a detail overview about the modified features selection in\\nCRF (Conditional Random Field) based Manipuri POS (Part of Speech) tagging.\\nSelection of features is so important in CRF that the better are the features\\nthen the better are the outputs. This work is an attempt or an experiment to\\nmake the previous work more efficient. Multiple new features are tried to run\\nthe CRF and again tried with the Reduplicated Multiword Expression (RMWE) as\\nanother feature. The CRF run with RMWE because Manipuri is rich of RMWE and\\nidentification of RMWE becomes one of the necessities to bring up the result of\\nPOS tagging. The new CRF system shows a Recall of 78.22%, Precision of 73.15%\\nand F-measure of 75.60%. With the identification of RMWE and considering it as\\na feature makes an improvement to a Recall of 80.20%, Precision of 74.31% and\\nF-measure of 77.14%.\\n',\n",
              " '  We present CAVaT, a tool that performs Corpus Analysis and Validation for\\nTimeML. CAVaT is an open source, modular checking utility for statistical\\nanalysis of features specific to temporally-annotated natural language corpora.\\nIt provides reporting, highlights salient links between a variety of general\\nand time-specific linguistic features, and also validates a temporal annotation\\nto ensure that it is logically consistent and sufficiently annotated. Uniquely,\\nCAVaT provides analysis specific to TimeML-annotated temporal information.\\nTimeML is a standard for annotating temporal information in natural language\\ntext. In this paper, we present the reporting part of CAVaT, and then its\\nerror-checking ability, including the workings of several novel TimeML document\\nverification methods. This is followed by the execution of some example tasks\\nusing the tool to show relations between times, events, signals and links. We\\nalso demonstrate inconsistencies in a TimeML corpus (TimeBank) that have been\\ndetected with CAVaT.\\n',\n",
              " '  Temporal information conveyed by language describes how the world around us\\nchanges through time. Events, durations and times are all temporal elements\\nthat can be viewed as intervals. These intervals are sometimes temporally\\nrelated in text. Automatically determining the nature of such relations is a\\ncomplex and unsolved problem. Some words can act as \"signals\" which suggest a\\ntemporal ordering between intervals. In this paper, we use these signal words\\nto improve the accuracy of a recent approach to classification of temporal\\nlinks.\\n',\n",
              " '  We describe the University of Sheffield system used in the TempEval-2\\nchallenge, USFD2. The challenge requires the automatic identification of\\ntemporal entities and relations in text. USFD2 identifies and anchors temporal\\nexpressions, and also attempts two of the four temporal relation assignment\\ntasks. A rule-based system picks out and anchors temporal expressions, and a\\nmaximum entropy classifier assigns temporal link labels, based on features that\\ninclude descriptions of associated temporal signal words. USFD2 identified\\ntemporal expressions successfully, and correctly classified their type in 90%\\nof cases. Determining the relation between an event and time expression in the\\nsame sentence was performed at 63% accuracy, the second highest score in this\\npart of the challenge.\\n',\n",
              " '  In this paper we present RTMML, a markup language for the tenses of verbs and\\ntemporal relations between verbs. There is a richness to tense in language that\\nis not fully captured by existing temporal annotation schemata. Following\\nReichenbach we present an analysis of tense in terms of abstract time points,\\nwith the aim of supporting automated processing of tense and temporal relations\\nin language. This allows for precise reasoning about tense in documents, and\\nthe deduction of temporal relations between the times and verbal events in a\\ndiscourse. We define the syntax of RTMML, and demonstrate the markup in a range\\nof situations.\\n',\n",
              " '  Automatic temporal ordering of events described in discourse has been of\\ngreat interest in recent years. Event orderings are conveyed in text via va\\nrious linguistic mechanisms including the use of expressions such as \"before\",\\n\"after\" or \"during\" that explicitly assert a temporal relation -- temporal\\nsignals. In this paper, we investigate the role of temporal signals in temporal\\nrelation extraction and provide a quantitative analysis of these expres sions\\nin the TimeBank annotated corpus.\\n',\n",
              " \"  This paper describes the University of Sheffield's entry in the 2011 TAC KBP\\nentity linking and slot filling tasks. We chose to participate in the\\nmonolingual entity linking task, the monolingual slot filling task and the\\ntemporal slot filling tasks. We set out to build a framework for\\nexperimentation with knowledge base population. This framework was created, and\\napplied to multiple KBP tasks. We demonstrated that our proposed framework is\\neffective and suitable for collaborative development efforts, as well as useful\\nin a teaching environment. Finally we present results that, while very modest,\\nprovide improvements an order of magnitude greater than our 2010 attempt.\\n\",\n",
              " '  Automatic annotation of temporal expressions is a research challenge of great\\ninterest in the field of information extraction. Gold standard\\ntemporally-annotated resources are limited in size, which makes research using\\nthem difficult. Standards have also evolved over the past decade, so not all\\ntemporally annotated data is in the same format. We vastly increase available\\nhuman-annotated temporal expression resources by converting older format\\nresources to TimeML/TIMEX3. This task is difficult due to differing annotation\\nmethods. We present a robust conversion tool and a new, large temporal\\nexpression resource. Using this, we evaluate our conversion process by using it\\nas training data for an existing TimeML annotation tool, achieving a 0.87 F1\\nmeasure -- better than any system in the TempEval-2 timex recognition exercise.\\n',\n",
              " '  Automated answering of natural language questions is an interesting and\\nuseful problem to solve. Question answering (QA) systems often perform\\ninformation retrieval at an initial stage. Information retrieval (IR)\\nperformance, provided by engines such as Lucene, places a bound on overall\\nsystem performance. For example, no answer bearing documents are retrieved at\\nlow ranks for almost 40% of questions.\\n  In this paper, answer texts from previous QA evaluations held as part of the\\nText REtrieval Conferences (TREC) are paired with queries and analysed in an\\nattempt to identify performance-enhancing words. These words are then used to\\nevaluate the performance of a query expansion method.\\n  Data driven extension words were found to help in over 70% of difficult\\nquestions. These words can be used to improve and evaluate query expansion\\nmethods. Simple blind relevance feedback (RF) was correctly predicted as\\nunlikely to help overall performance, and an possible explanation is provided\\nfor its low value in IR for QA.\\n',\n",
              " \"  ASR short for Automatic Speech Recognition is the process of converting a\\nspoken speech into text that can be manipulated by a computer. Although ASR has\\nseveral applications, it is still erroneous and imprecise especially if used in\\na harsh surrounding wherein the input speech is of low quality. This paper\\nproposes a post-editing ASR error correction method and algorithm based on\\nBing's online spelling suggestion. In this approach, the ASR recognized output\\ntext is spell-checked using Bing's spelling suggestion technology to detect and\\ncorrect misrecognized words. More specifically, the proposed algorithm breaks\\ndown the ASR output text into several word-tokens that are submitted as search\\nqueries to Bing search engine. A returned spelling suggestion implies that a\\nquery is misspelled; and thus it is replaced by the suggested correction;\\notherwise, no correction is performed and the algorithm continues with the next\\ntoken until all tokens get validated. Experiments carried out on various\\nspeeches in different languages indicated a successful decrease in the number\\nof ASR errors and an improvement in the overall error correction rate. Future\\nresearch can improve upon the proposed algorithm so much so that it can be\\nparallelized to take advantage of multiprocessor computers.\\n\",\n",
              " '  At the present time, computers are employed to solve complex tasks and\\nproblems ranging from simple calculations to intensive digital image processing\\nand intricate algorithmic optimization problems to computationally-demanding\\nweather forecasting problems. ASR short for Automatic Speech Recognition is yet\\nanother type of computational problem whose purpose is to recognize human\\nspoken speech and convert it into text that can be processed by a computer.\\nDespite that ASR has many versatile and pervasive real-world applications,it is\\nstill relatively erroneous and not perfectly solved as it is prone to produce\\nspelling errors in the recognized text, especially if the ASR system is\\noperating in a noisy environment, its vocabulary size is limited, and its input\\nspeech is of bad or low quality. This paper proposes a post-editing ASR error\\ncorrection method based on MicrosoftN-Gram dataset for detecting and correcting\\nspelling errors generated by ASR systems. The proposed method comprises an\\nerror detection algorithm for detecting word errors; a candidate corrections\\ngeneration algorithm for generating correction suggestions for the detected\\nword errors; and a context-sensitive error correction algorithm for selecting\\nthe best candidate for correction. The virtue of using the Microsoft N-Gram\\ndataset is that it contains real-world data and word sequences extracted from\\nthe web which canmimica comprehensive dictionary of words having a large and\\nall-inclusive vocabulary. Experiments conducted on numerous speeches, performed\\nby different speakers, showed a remarkable reduction in ASR errors. Future\\nresearch can improve upon the proposed algorithm so much so that it can be\\nparallelized to take advantage of multiprocessor and distributed systems.\\n',\n",
              " '  This paper aims to shed some light on the concept of virality - especially in\\nsocial networks - and to provide new insights on its structure. We argue that:\\n(a) virality is a phenomenon strictly connected to the nature of the content\\nbeing spread, rather than to the influencers who spread it, (b) virality is a\\nphenomenon with many facets, i.e. under this generic term several different\\neffects of persuasive communication are comprised and they only partially\\noverlap. To give ground to our claims, we provide initial experiments in a\\nmachine learning framework to show how various aspects of virality can be\\nindependently predicted according to content features.\\n',\n",
              " \"  Tree transducers are formal automata that transform trees into other trees.\\nMany varieties of tree transducers have been explored in the automata theory\\nliterature, and more recently, in the machine translation literature. In this\\npaper I review T and xT transducers, situate them among related formalisms, and\\nshow how they can be used to implement rules for machine translation systems\\nthat cover all of the cross-language structural divergences described in Bonnie\\nDorr's influential article on the topic. I also present an implementation of xT\\ntransduction, suitable and convenient for experimenting with translation rules.\\n\",\n",
              " '  Understanding the ways in which information achieves widespread public\\nawareness is a research question of significant interest. We consider whether,\\nand how, the way in which the information is phrased --- the choice of words\\nand sentence structure --- can affect this process. To this end, we develop an\\nanalysis framework and build a corpus of movie quotes, annotated with\\nmemorability information, in which we are able to control for both the speaker\\nand the setting of the quotes. We find that there are significant differences\\nbetween memorable and non-memorable quotes in several key dimensions, even\\nafter controlling for situational and contextual factors. One is lexical\\ndistinctiveness: in aggregate, memorable quotes use less common word choices,\\nbut at the same time are built upon a scaffolding of common syntactic patterns.\\nAnother is that memorable quotes tend to be more general in ways that make them\\neasy to apply in new contexts --- that is, more portable. We also show how the\\nconcept of \"memorable language\" can be extended across domains.\\n',\n",
              " '  The terminology used in Biomedicine shows lexical peculiarities that have\\nrequired the elaboration of terminological resources and information retrieval\\nsystems with specific functionalities. The main characteristics are the high\\nrates of synonymy and homonymy, due to phenomena such as the proliferation of\\npolysemic acronyms and their interaction with common language. Information\\nretrieval systems in the biomedical domain use techniques oriented to the\\ntreatment of these lexical peculiarities. In this paper we review some of the\\ntechniques used in this domain, such as the application of Natural Language\\nProcessing (BioNLP), the incorporation of lexical-semantic resources, and the\\napplication of Named Entity Recognition (BioNER). Finally, we present the\\nevaluation methods adopted to assess the suitability of these techniques for\\nretrieving biomedical resources.\\n',\n",
              " \"  WordNet proved that it is possible to construct a large-scale electronic\\nlexical database on the principles of lexical semantics. It has been accepted\\nand used extensively by computational linguists ever since it was released.\\nInspired by WordNet's success, we propose as an alternative a similar resource,\\nbased on the 1987 Penguin edition of Roget's Thesaurus of English Words and\\nPhrases.\\n  Peter Mark Roget published his first Thesaurus over 150 years ago. Countless\\nwriters, orators and students of the English language have used it.\\nComputational linguists have employed Roget's for almost 50 years in Natural\\nLanguage Processing, however hesitated in accepting Roget's Thesaurus because a\\nproper machine tractable version was not available.\\n  This dissertation presents an implementation of a machine-tractable version\\nof the 1987 Penguin edition of Roget's Thesaurus - the first implementation of\\nits kind to use an entire current edition. It explains the steps necessary for\\ntaking a machine-readable file and transforming it into a tractable system.\\nThis involves converting the lexical material into a format that can be more\\neasily exploited, identifying data structures and designing classes to\\ncomputerize the Thesaurus. Roget's organization is studied in detail and\\ncontrasted with WordNet's.\\n  We show two applications of the computerized Thesaurus: computing semantic\\nsimilarity between words and phrases, and building lexical chains in a text.\\nThe experiments are performed using well-known benchmarks and the results are\\ncompared to those of other systems that use Roget's, WordNet and statistical\\ntechniques. Roget's has turned out to be an excellent resource for measuring\\nsemantic similarity; lexical chains are easily built but more difficult to\\nevaluate. We also explain ways in which Roget's Thesaurus and WordNet can be\\ncombined.\\n\",\n",
              " '  Spell-checking is the process of detecting and sometimes providing\\nsuggestions for incorrectly spelled words in a text. Basically, the larger the\\ndictionary of a spell-checker is, the higher is the error detection rate;\\notherwise, misspellings would pass undetected. Unfortunately, traditional\\ndictionaries suffer from out-of-vocabulary and data sparseness problems as they\\ndo not encompass large vocabulary of words indispensable to cover proper names,\\ndomain-specific terms, technical jargons, special acronyms, and terminologies.\\nAs a result, spell-checkers will incur low error detection and correction rate\\nand will fail to flag all errors in the text. This paper proposes a new\\nparallel shared-memory spell-checking algorithm that uses rich real-world word\\nstatistics from Yahoo! N-Grams Dataset to correct non-word and real-word errors\\nin computer text. Essentially, the proposed algorithm can be divided into three\\nsub-algorithms that run in a parallel fashion: The error detection algorithm\\nthat detects misspellings, the candidates generation algorithm that generates\\ncorrection suggestions, and the error correction algorithm that performs\\ncontextual error correction. Experiments conducted on a set of text articles\\ncontaining misspellings, showed a remarkable spelling error correction rate\\nthat resulted in a radical reduction of both non-word and real-word errors in\\nelectronic text. In a further study, the proposed algorithm is to be optimized\\nfor message-passing systems so as to become more flexible and less costly to\\nscale over distributed machines.\\n',\n",
              " '  Since the dawn of the computing era, information has been represented\\ndigitally so that it can be processed by electronic computers. Paper books and\\ndocuments were abundant and widely being published at that time; and hence,\\nthere was a need to convert them into digital format. OCR, short for Optical\\nCharacter Recognition was conceived to translate paper-based books into digital\\ne-books. Regrettably, OCR systems are still erroneous and inaccurate as they\\nproduce misspellings in the recognized text, especially when the source\\ndocument is of low printing quality. This paper proposes a post-processing OCR\\ncontext-sensitive error correction method for detecting and correcting non-word\\nand real-word OCR errors. The cornerstone of this proposed approach is the use\\nof Google Web 1T 5-gram data set as a dictionary of words to spell-check OCR\\ntext. The Google data set incorporates a very large vocabulary and word\\nstatistics entirely reaped from the Internet, making it a reliable source to\\nperform dictionary-based error correction. The core of the proposed solution is\\na combination of three algorithms: The error detection, candidate spellings\\ngenerator, and error correction algorithms, which all exploit information\\nextracted from Google Web 1T 5-gram data set. Experiments conducted on scanned\\nimages written in different languages showed a substantial improvement in the\\nOCR error correction rate. As future developments, the proposed algorithm is to\\nbe parallelised so as to support parallel and distributed computing\\narchitectures.\\n',\n",
              " \"  With the advent of digital optical scanners, a lot of paper-based books,\\ntextbooks, magazines, articles, and documents are being transformed into an\\nelectronic version that can be manipulated by a computer. For this purpose,\\nOCR, short for Optical Character Recognition was developed to translate scanned\\ngraphical text into editable computer text. Unfortunately, OCR is still\\nimperfect as it occasionally mis-recognizes letters and falsely identifies\\nscanned text, leading to misspellings and linguistics errors in the OCR output\\ntext. This paper proposes a post-processing context-based error correction\\nalgorithm for detecting and correcting OCR non-word and real-word errors. The\\nproposed algorithm is based on Google's online spelling suggestion which\\nharnesses an internal database containing a huge collection of terms and word\\nsequences gathered from all over the web, convenient to suggest possible\\nreplacements for words that have been misspelled during the OCR process.\\nExperiments carried out revealed a significant improvement in OCR error\\ncorrection rate. Future research can improve upon the proposed algorithm so\\nmuch so that it can be parallelized and executed over multiprocessing\\nplatforms.\\n\",\n",
              " \"  We have implemented a system that measures semantic similarity using a\\ncomputerized 1987 Roget's Thesaurus, and evaluated it by performing a few\\ntypical tests. We compare the results of these tests with those produced by\\nWordNet-based similarity measures. One of the benchmarks is Miller and Charles'\\nlist of 30 noun pairs to which human judges had assigned similarity measures.\\nWe correlate these measures with those computed by several NLP systems. The 30\\npairs can be traced back to Rubenstein and Goodenough's 65 pairs, which we have\\nalso studied. Our Roget's-based system gets correlations of .878 for the\\nsmaller and .818 for the larger list of noun pairs; this is quite close to the\\n.885 that Resnik obtained when he employed humans to replicate the Miller and\\nCharles experiment. We further evaluate our measure by using Roget's and\\nWordNet to answer 80 TOEFL, 50 ESL and 300 Reader's Digest questions: the\\ncorrect synonym must be selected amongst a group of four words. Our system gets\\n78.75%, 82.00% and 74.33% of the questions respectively.\\n\",\n",
              " '  This paper proposes some modest improvements to Extractor, a state-of-the-art\\nkeyphrase extraction system, by using a terabyte-sized corpus to estimate the\\ninformativeness and semantic similarity of keyphrases. We present two\\ntechniques to improve the organization and remove outliers of lists of\\nkeyphrases. The first is a simple ordering according to their occurrences in\\nthe corpus; the second is clustering according to semantic similarity.\\nEvaluation issues are discussed. We present a novel technique of comparing\\nextracted keyphrases to a gold standard which relies on semantic similarity\\nrather than string matching or an evaluation involving human judges.\\n',\n",
              " \"  Morris and Hirst present a method of linking significant words that are about\\nthe same topic. The resulting lexical chains are a means of identifying\\ncohesive regions in a text, with applications in many natural language\\nprocessing tasks, including text summarization. The first lexical chains were\\nconstructed manually using Roget's International Thesaurus. Morris and Hirst\\nwrote that automation would be straightforward given an electronic thesaurus.\\nAll applications so far have used WordNet to produce lexical chains, perhaps\\nbecause adequate electronic versions of Roget's were not available until\\nrecently. We discuss the building of lexical chains using an electronic version\\nof Roget's Thesaurus. We implement a variant of the original algorithm, and\\nexplain the necessary design decisions. We include a comparison with other\\nimplementations.\\n\",\n",
              " \"  This paper presents the steps involved in creating an electronic lexical\\nknowledge base from the 1987 Penguin edition of Roget's Thesaurus. Semantic\\nrelations are labelled with the help of WordNet. The two resources are compared\\nin a qualitative and quantitative manner. Differences in the organization of\\nthe lexical material are discussed, as well as the possibility of merging both\\nresources.\\n\",\n",
              " '  In this paper we present statistical analysis of English texts from\\nWikipedia. We try to address the issue of language complexity empirically by\\ncomparing the simple English Wikipedia (Simple) to comparable samples of the\\nmain English Wikipedia (Main). Simple is supposed to use a more simplified\\nlanguage with a limited vocabulary, and editors are explicitly requested to\\nfollow this guideline, yet in practice the vocabulary richness of both samples\\nare at the same level. Detailed analysis of longer units (n-grams of words and\\npart of speech tags) shows that the language of Simple is less complex than\\nthat of Main primarily due to the use of shorter sentences, as opposed to\\ndrastically simplified syntax or vocabulary. Comparing the two language\\nvarieties by the Gunning readability index supports this conclusion. We also\\nreport on the topical dependence of language complexity, e.g. that the language\\nis more advanced in conceptual articles compared to person-based (biographical)\\nand object-based articles. Finally, we investigate the relation between\\nconflict and language complexity by analyzing the content of the talk pages\\nassociated to controversial and peacefully developing articles, concluding that\\ncontroversy has the effect of reducing language complexity.\\n',\n",
              " '  We propose a new segmentation evaluation metric, called segmentation\\nsimilarity (S), that quantifies the similarity between two segmentations as the\\nproportion of boundaries that are not transformed when comparing them using\\nedit distance, essentially using edit distance as a penalty function and\\nscaling penalties by segmentation size. We propose several adapted\\ninter-annotator agreement coefficients which use S that are suitable for\\nsegmentation. We show that S is configurable enough to suit a wide variety of\\nsegmentation evaluations, and is an improvement upon the state of the art. We\\nalso propose using inter-annotator agreement coefficients to evaluate automatic\\nsegmenters in terms of human performance.\\n',\n",
              " \"  Jules Bloch's work on formation of the Marathi language has to be expanded\\nfurther to provide for a study of evolution and formation of Indian languages\\nin the Indian language union (sprachbund). The paper analyses the stages in the\\nevolution of early writing systems which began with the evolution of counting\\nin the ancient Near East. A stage anterior to the stage of syllabic\\nrepresentation of sounds of a language, is identified. Unique geometric shapes\\nrequired for tokens to categorize objects became too large to handle to\\nabstract hundreds of categories of goods and metallurgical processes during the\\nproduction of bronze-age goods. About 3500 BCE, Indus script as a writing\\nsystem was developed to use hieroglyphs to represent the 'spoken words'\\nidentifying each of the goods and processes. A rebus method of representing\\nsimilar sounding words of the lingua franca of the artisans was used in Indus\\nscript. This method is recognized and consistently applied for the lingua\\nfranca of the Indian sprachbund. That the ancient languages of India,\\nconstituted a sprachbund (or language union) is now recognized by many\\nlinguists. The sprachbund area is proximate to the area where most of the Indus\\nscript inscriptions were discovered, as documented in the corpora. That\\nhundreds of Indian hieroglyphs continued to be used in metallurgy is evidenced\\nby their use on early punch-marked coins. This explains the combined use of\\nsyllabic scripts such as Brahmi and Kharoshti together with the hieroglyphs on\\nRampurva copper bolt, and Sohgaura copper plate from about 6th century\\nBCE.Indian hieroglyphs constitute a writing system for meluhha language and are\\nrebus representations of archaeo-metallurgy lexemes. The rebus principle was\\nemployed by the early scripts and can legitimately be used to decipher the\\nIndus script, after secure pictorial identification.\\n\",\n",
              " '  We are interested in bridging the world of natural language and the world of\\nthe semantic web in particular to support natural multilingual access to the\\nweb of data. In this paper we introduce a new type of lexical ontology called\\ninterlingual lexical ontology (ILexicOn), which uses semantic web formalisms to\\nmake each interlingual lexical unit class (ILUc) support the projection of its\\nsemantic decomposition on itself. After a short overview of existing lexical\\nontologies, we briefly introduce the semantic web formalisms we use. We then\\npresent the three layered architecture of our approach: i) the interlingual\\nlexical meta-ontology (ILexiMOn); ii) the ILexicOn where ILUcs are formally\\ndefined; iii) the data layer. We illustrate our approach with a standalone\\nILexicOn, and introduce and explain a concise human-readable notation to\\nrepresent ILexicOns. Finally, we show how semantic web formalisms enable the\\nprojection of a semantic decomposition on the decomposed ILUc.\\n',\n",
              " '  In recent years there has been a growing interest in crowdsourcing\\nmethodologies to be used in experimental research for NLP tasks. In particular,\\nevaluation of systems and theories about persuasion is difficult to accommodate\\nwithin existing frameworks. In this paper we present a new cheap and fast\\nmethodology that allows fast experiment building and evaluation with\\nfully-automated analysis at a low cost. The central idea is exploiting existing\\ncommercial tools for advertising on the web, such as Google AdWords, to measure\\nmessage impact in an ecological setting. The paper includes a description of\\nthe approach, tips for how to use AdWords for scientific research, and results\\nof pilot experiments on the impact of affective text variations which confirm\\nthe effectiveness of the approach.\\n',\n",
              " '  In computing, spell checking is the process of detecting and sometimes\\nproviding spelling suggestions for incorrectly spelled words in a text.\\nBasically, a spell checker is a computer program that uses a dictionary of\\nwords to perform spell checking. The bigger the dictionary is, the higher is\\nthe error detection rate. The fact that spell checkers are based on regular\\ndictionaries, they suffer from data sparseness problem as they cannot capture\\nlarge vocabulary of words including proper names, domain-specific terms,\\ntechnical jargons, special acronyms, and terminologies. As a result, they\\nexhibit low error detection rate and often fail to catch major errors in the\\ntext. This paper proposes a new context-sensitive spelling correction method\\nfor detecting and correcting non-word and real-word errors in digital text\\ndocuments. The approach hinges around data statistics from Google Web 1T 5-gram\\ndata set which consists of a big volume of n-gram word sequences, extracted\\nfrom the World Wide Web. Fundamentally, the proposed method comprises an error\\ndetector that detects misspellings, a candidate spellings generator based on a\\ncharacter 2-gram model that generates correction suggestions, and an error\\ncorrector that performs contextual error correction. Experiments conducted on a\\nset of text documents from different domains and containing misspellings,\\nshowed an outstanding spelling error correction rate and a drastic reduction of\\nboth non-word and real-word errors. In a further study, the proposed algorithm\\nis to be parallelized so as to lower the computational cost of the error\\ndetection and correction processes.\\n',\n",
              " '  The aim of this paper is to evaluate a Text to Knowledge Mapping (TKM)\\nPrototype. The prototype is domain-specific, the purpose of which is to map\\ninstructional text onto a knowledge domain. The context of the knowledge domain\\nis DC electrical circuit. During development, the prototype has been tested\\nwith a limited data set from the domain. The prototype reached a stage where it\\nneeds to be evaluated with a representative linguistic data set called corpus.\\nA corpus is a collection of text drawn from typical sources which can be used\\nas a test data set to evaluate NLP systems. As there is no available corpus for\\nthe domain, we developed and annotated a representative corpus. The evaluation\\nof the prototype considers two of its major components- lexical components and\\nknowledge model. Evaluation on lexical components enriches the lexical\\nresources of the prototype like vocabulary and grammar structures. This leads\\nthe prototype to parse a reasonable amount of sentences in the corpus. While\\ndealing with the lexicon was straight forward, the identification and\\nextraction of appropriate semantic relations was much more involved. It was\\nnecessary, therefore, to manually develop a conceptual structure for the domain\\nto formulate a domain-specific framework of semantic relations. The framework\\nof semantic relationsthat has resulted from this study consisted of 55\\nrelations, out of which 42 have inverse relations. We also conducted rhetorical\\nanalysis on the corpus to prove its representativeness in conveying semantic.\\nFinally, we conducted a topical and discourse analysis on the corpus to analyze\\nthe coverage of discourse by the prototype.\\n',\n",
              " '  Two formalisms, both based on context-free grammars, have recently been\\nproposed as a basis for a non-uniform random generation of combinatorial\\nobjects. The former, introduced by Denise et al, associates weights with\\nletters, while the latter, recently explored by Weinberg et al in the context\\nof random generation, associates weights to transitions. In this short note, we\\nuse a simple modification of the Greibach Normal Form transformation algorithm,\\ndue to Blum and Koch, to show the equivalent expressivities, in term of their\\ninduced distributions, of these two formalisms.\\n',\n",
              " '  One important aspect of the relationship between spoken and written Chinese\\nis the ranked syllable-to-character mapping spectrum, which is the ranked list\\nof syllables by the number of characters that map to the syllable. Previously,\\nthis spectrum is analyzed for more than 400 syllables without distinguishing\\nthe four intonations. In the current study, the spectrum with 1280 toned\\nsyllables is analyzed by logarithmic function, Beta rank function, and\\npiecewise logarithmic function. Out of the three fitting functions, the\\ntwo-piece logarithmic function fits the data the best, both by the smallest sum\\nof squared errors (SSE) and by the lowest Akaike information criterion (AIC)\\nvalue. The Beta rank function is the close second. By sampling from a Poisson\\ndistribution whose parameter value is chosen from the observed data, we\\nempirically estimate the $p$-value for testing the\\ntwo-piece-logarithmic-function being better than the Beta rank function\\nhypothesis, to be 0.16. For practical purposes, the piecewise logarithmic\\nfunction and the Beta rank function can be considered a tie.\\n',\n",
              " '  This paper describes the use of Naive Bayes to address the task of assigning\\nfunction tags and context free grammar (CFG) to parse Myanmar sentences. Part\\nof the challenge of statistical function tagging for Myanmar sentences comes\\nfrom the fact that Myanmar has free-phrase-order and a complex morphological\\nsystem. Function tagging is a pre-processing step for parsing. In the task of\\nfunction tagging, we use the functional annotated corpus and tag Myanmar\\nsentences with correct segmentation, POS (part-of-speech) tagging and chunking\\ninformation. We propose Myanmar grammar rules and apply context free grammar\\n(CFG) to find out the parse tree of function tagged Myanmar sentences.\\nExperiments show that our analysis achieves a good result with parsing of\\nsimple sentences and three types of complex sentences.\\n',\n",
              " '  The success rates of Optical Character Recognition (OCR) systems for printed\\nMalayalam documents is quite impressive with the state of the art accuracy\\nlevels in the range of 85-95% for various. However for real applications,\\nfurther enhancement of this accuracy levels are required. One of the bottle\\nnecks in further enhancement of the accuracy is identified as close-matching\\ncharacters. In this paper, we delineate the close matching characters in\\nMalayalam and report the development of a specialised classifier for these\\nclose-matching characters. The output of a state of the art of OCR is taken and\\ncharacters falling into the close-matching character set is further fed into\\nthis specialised classifier for enhancing the accuracy. The classifier is based\\non support vector machine algorithm and uses feature vectors derived out of\\nspectral coefficients of projection histogram signals of close-matching\\ncharacters.\\n',\n",
              " '  We develop the multilingual topic model for unaligned text (MuTo), a\\nprobabilistic model of text that is designed to analyze corpora composed of\\ndocuments in two languages. From these documents, MuTo uses stochastic EM to\\nsimultaneously discover both a matching between the languages and multilingual\\nlatent topics. We demonstrate that MuTo is able to find shared topics on\\nreal-world multilingual corpora, successfully pairing related documents across\\nlanguages. MuTo provides a new framework for creating multilingual topic models\\nwithout needing carefully curated parallel corpora and allows applications\\nbuilt using the topic model formalism to be applied to a much wider class of\\ncorpora.\\n',\n",
              " '  Existing probabilistic scanners and parsers impose hard constraints on the\\nway lexical and syntactic ambiguities can be resolved. Furthermore, traditional\\ngrammar-based parsing tools are limited in the mechanisms they allow for taking\\ncontext into account. In this paper, we propose a model-driven tool that allows\\nfor statistical language models with arbitrary probability estimators. Our work\\non model-driven probabilistic parsing is built on top of ModelCC, a model-based\\nparser generator, and enables the probabilistic interpretation and resolution\\nof anaphoric, cataphoric, and recursive references in the disambiguation of\\nabstract syntax graphs. In order to prove the expression power of ModelCC, we\\ndescribe the design of a general-purpose natural language parser.\\n',\n",
              " '  This work consists of creating a system of the Computer Assisted Language\\nLearning (CALL) based on a system of Automatic Speech Recognition (ASR) for the\\nArabic language using the tool CMU Sphinx3 [1], based on the approach of HMM.\\nTo this work, we have constructed a corpus of six hours of speech recordings\\nwith a number of nine speakers. we find in the robustness to noise a grounds\\nfor the choice of the HMM approach [2]. the results achieved are encouraging\\nsince our corpus is made by only nine speakers, but they are always reasons\\nthat open the door for other improvement works.\\n',\n",
              " '  While the use of cluster features became ubiquitous in core NLP tasks, most\\ncluster features in NLP are based on distributional similarity. We propose a\\nnew type of clustering criteria, specific to the task of part-of-speech\\ntagging. Instead of distributional similarity, these clusters are based on the\\nbeha vior of a baseline tagger when applied to a large corpus. These cluster\\nfeatures provide similar gains in accuracy to those achieved by\\ndistributional-similarity derived clusters. Using both types of cluster\\nfeatures together further improve tagging accuracies. We show that the method\\nis effective for both the in-domain and out-of-domain scenarios for English,\\nand for French, German and Italian. The effect is larger for out-of-domain\\ntext.\\n',\n",
              " '  We introduce precision-biased parsing: a parsing task which favors precision\\nover recall by allowing the parser to abstain from decisions deemed uncertain.\\nWe focus on dependency-parsing and present an ensemble method which is capable\\nof assigning parents to 84% of the text tokens while being over 96% accurate on\\nthese tokens. We use the precision-biased parsing task to solve the related\\nhigh-quality parse-selection task: finding a subset of high-quality (accurate)\\ntrees in a large collection of parsed text. We present a method for choosing\\nover a third of the input trees while keeping unlabeled dependency parsing\\naccuracy of 97% on these trees. We also present a method which is not based on\\nan ensemble but rather on directly predicting the risk associated with\\nindividual parser decisions. In addition to its efficiency, this method\\ndemonstrates that a parsing system can provide reasonable estimates of\\nconfidence in its predictions without relying on ensembles or aggregate corpus\\ncounts.\\n',\n",
              " '  Lexical substitutes have found use in areas such as paraphrasing, text\\nsimplification, machine translation, word sense disambiguation, and part of\\nspeech induction. However the computational complexity of accurately\\nidentifying the most likely substitutes for a word has made large scale\\nexperiments difficult. In this paper I introduce a new search algorithm,\\nFASTSUBS, that is guaranteed to find the K most likely lexical substitutes for\\na given word in a sentence based on an n-gram language model. The computation\\nis sub-linear in both K and the vocabulary size V. An implementation of the\\nalgorithm and a dataset with the top 100 substitutes of each token in the WSJ\\nsection of the Penn Treebank are available at http://goo.gl/jzKH0.\\n',\n",
              " \"  The study of the Tip of the Tongue phenomenon (TOT) provides valuable clues\\nand insights concerning the organisation of the mental lexicon (meaning, number\\nof syllables, relation with other words, etc.). This paper describes a tool\\nbased on psycho-linguistic observations concerning the TOT phenomenon. We've\\nbuilt it to enable a speaker/writer to find the word he is looking for, word he\\nmay know, but which he is unable to access in time. We try to simulate the TOT\\nphenomenon by creating a situation where the system knows the target word, yet\\nis unable to access it. In order to find the target word we make use of the\\nparadigmatic and syntagmatic associations stored in the linguistic databases.\\nOur experiment allows the following conclusion: a tool like SVETLAN, capable to\\nstructure (automatically) a dictionary by domains can be used sucessfully to\\nhelp the speaker/writer to find the word he is looking for, if it is combined\\nwith a database rich in terms of paradigmatic links like EuroWordNet.\\n\",\n",
              " '  This project explores the nature of language acquisition in computers, guided\\nby techniques similar to those used in children. While existing natural\\nlanguage processing methods are limited in scope and understanding, our system\\naims to gain an understanding of language from first principles and hence\\nminimal initial input. The first portion of our system was implemented in Java\\nand is focused on understanding the morphology of language using bigrams. We\\nuse frequency distributions and differences between them to define and\\ndistinguish languages. English and French texts were analyzed to determine a\\ndifference threshold of 55 before the texts are considered to be in different\\nlanguages, and this threshold was verified using Spanish texts. The second\\nportion of our system focuses on gaining an understanding of the syntax of a\\nlanguage using a recursive method. The program uses one of two possible methods\\nto analyze given sentences based on either sentence patterns or surrounding\\nwords. Both methods have been implemented in C++. The program is able to\\nunderstand the structure of simple sentences and learn new words. In addition,\\nwe have provided some suggestions regarding future work and potential\\nextensions of the existing program.\\n',\n",
              " '  We propose a general method for automated word puzzle generation. Contrary to\\nprevious approaches in this novel field, the presented method does not rely on\\nhighly structured datasets obtained with serious human annotation effort: it\\nonly needs an unstructured and unannotated corpus (i.e., document collection)\\nas input. The method builds upon two additional pillars: (i) a topic model,\\nwhich induces a topic dictionary from the input corpus (examples include e.g.,\\nlatent semantic analysis, group-structured dictionaries or latent Dirichlet\\nallocation), and (ii) a semantic similarity measure of word pairs. Our method\\ncan (i) generate automatically a large number of proper word puzzles of\\ndifferent types, including the odd one out, choose the related word and\\nseparate the topics puzzle. (ii) It can easily create domain-specific puzzles\\nby replacing the corpus component. (iii) It is also capable of automatically\\ngenerating puzzles with parameterizable levels of difficulty suitable for,\\ne.g., beginners or intermediate learners.\\n',\n",
              " '  Universal Networking Language (UNL) is a declarative formal language that is\\nused to represent semantic data extracted from natural language texts. This\\npaper presents a novel approach to converting Bangla natural language text into\\nUNL using a method known as Predicate Preserving Parser (PPP) technique. PPP\\nperforms morphological, syntactic and semantic, and lexical analysis of text\\nsynchronously. This analysis produces a semantic-net like structure represented\\nusing UNL. We demonstrate how Bangla texts are analyzed following the PPP\\ntechnique to produce UNL documents which can then be translated into any other\\nsuitable natural language facilitating the opportunity to develop a universal\\nlanguage translation method via UNL.\\n',\n",
              " '  Understanding the ways in which participants in public discussions frame\\ntheir arguments is important in understanding how public opinion is formed. In\\nthis paper, we adopt the position that it is time for more\\ncomputationally-oriented research on problems involving framing. In the\\ninterests of furthering that goal, we propose the following specific,\\ninteresting and, we believe, relatively accessible question: In the controversy\\nregarding the use of genetically-modified organisms (GMOs) in agriculture, do\\npro- and anti-GMO articles differ in whether they choose to adopt a\\n\"scientific\" tone?\\n  Prior work on the rhetoric and sociology of science suggests that hedging may\\ndistinguish popular-science text from text written by professional scientists\\nfor their colleagues. We propose a detailed approach to studying whether hedge\\ndetection can be used to understanding scientific framing in the GMO debates,\\nand provide corpora to facilitate this study. Some of our preliminary analyses\\nsuggest that hedges occur less frequently in scientific discourse than in\\npopular text, a finding that contradicts prior assertions in the literature. We\\nhope that our initial work and data will encourage others to pursue this\\npromising line of inquiry.\\n',\n",
              " \"  In this memory we made the design of an indexing model for Arabic language\\nand adapting standards for describing learning resources used (the LOM and\\ntheir application profiles) with learning conditions such as levels education\\nof students, their levels of understanding...the pedagogical context with\\ntaking into account the repre-sentative elements of the text, text's\\nlength,...in particular, we highlight the specificity of the Arabic language\\nwhich is a complex language, characterized by its flexion, its voyellation and\\nits agglutination.\\n\",\n",
              " '  Automatic annotation of temporal expressions is a research challenge of great\\ninterest in the field of information extraction. In this report, I describe a\\nnovel rule-based architecture, built on top of a pre-existing system, which is\\nable to normalise temporal expressions detected in English texts. Gold standard\\ntemporally-annotated resources are limited in size and this makes research\\ndifficult. The proposed system outperforms the state-of-the-art systems with\\nrespect to TempEval-2 Shared Task (value attribute) and achieves substantially\\nbetter results with respect to the pre-existing system on top of which it has\\nbeen developed. I will also introduce a new free corpus consisting of 2822\\nunique annotated temporal expressions. Both the corpus and the system are\\nfreely available on-line.\\n',\n",
              " '  BADREX uses dynamically generated regular expressions to annotate term\\ndefinition-term abbreviation pairs, and corefers unpaired acronyms and\\nabbreviations back to their initial definition in the text. Against the\\nMedstract corpus BADREX achieves precision and recall of 98% and 97%, and\\nagainst a much larger corpus, 90% and 85%, respectively. BADREX yields improved\\nperformance over previous approaches, requires no training data and allows\\nruntime customisation of its input parameters. BADREX is freely available from\\nhttps://github.com/philgooch/BADREX-Biomedical-Abbreviation-Expander as a\\nplugin for the General Architecture for Text Engineering (GATE) framework and\\nis licensed under the GPLv3.\\n',\n",
              " '  We describe the TempEval-3 task which is currently in preparation for the\\nSemEval-2013 evaluation exercise. The aim of TempEval is to advance research on\\ntemporal information processing. TempEval-3 follows on from previous TempEval\\nevents, incorporating: a three-part task structure covering event, temporal\\nexpression and temporal relation extraction; a larger dataset; and single\\noverall task quality scores.\\n',\n",
              " '  This paper describes a computationally inexpensive and efficient generic\\nsummarization algorithm for Arabic texts. The algorithm belongs to extractive\\nsummarization family, which reduces the problem into representative sentences\\nidentification and extraction sub-problems. Important keyphrases of the\\ndocument to be summarized are identified employing combinations of statistical\\nand linguistic features. The sentence extraction algorithm exploits keyphrases\\nas the primary attributes to rank a sentence. The present experimental work,\\ndemonstrates different techniques for achieving various summarization goals\\nincluding: informative richness, coverage of both main and auxiliary topics,\\nand keeping redundancy to a minimum. A scoring scheme is then adopted that\\nbalances between these summarization goals. To evaluate the resulted Arabic\\nsummaries with well-established systems, aligned English/Arabic texts are used\\nthrough the experiments.\\n',\n",
              " '  Unlabeled data is often used to learn representations which can be used to\\nsupplement baseline features in a supervised learner. For example, for text\\napplications where the words lie in a very high dimensional space (the size of\\nthe vocabulary), one can learn a low rank \"dictionary\" by an\\neigen-decomposition of the word co-occurrence matrix (e.g. using PCA or CCA).\\nIn this paper, we present a new spectral method based on CCA to learn an\\neigenword dictionary. Our improved procedure computes two set of CCAs, the\\nfirst one between the left and right contexts of the given word and the second\\none between the projections resulting from this CCA and the word itself. We\\nprove theoretically that this two-step procedure has lower sample complexity\\nthan the simple single step procedure and also illustrate the empirical\\nefficacy of our approach and the richness of representations learned by our Two\\nStep CCA (TSCCA) procedure on the tasks of POS tagging and sentiment\\nclassification.\\n',\n",
              " '  As robots become more ubiquitous and capable, it becomes ever more important\\nto enable untrained users to easily interact with them. Recently, this has led\\nto study of the language grounding problem, where the goal is to extract\\nrepresentations of the meanings of natural language tied to perception and\\nactuation in the physical world. In this paper, we present an approach for\\njoint learning of language and perception models for grounded attribute\\ninduction. Our perception model includes attribute classifiers, for example to\\ndetect object color and shape, and the language model is based on a\\nprobabilistic categorial grammar that enables the construction of rich,\\ncompositional meaning representations. The approach is evaluated on the task of\\ninterpreting sentences that describe sets of objects in a physical workspace.\\nWe demonstrate accurate task performance and effective latent-variable concept\\ninduction in physical grounded scenes.\\n',\n",
              " '  In spite of their superior performance, neural probabilistic language models\\n(NPLMs) remain far less widely used than n-gram models due to their notoriously\\nlong training times, which are measured in weeks even for moderately-sized\\ndatasets. Training NPLMs is computationally expensive because they are\\nexplicitly normalized, which leads to having to consider all words in the\\nvocabulary when computing the log-likelihood gradients.\\n  We propose a fast and simple algorithm for training NPLMs based on\\nnoise-contrastive estimation, a newly introduced procedure for estimating\\nunnormalized continuous distributions. We investigate the behaviour of the\\nalgorithm on the Penn Treebank corpus and show that it reduces the training\\ntimes by more than an order of magnitude without affecting the quality of the\\nresulting models. The algorithm is also more efficient and much more stable\\nthan importance sampling because it requires far fewer noise samples to perform\\nwell.\\n  We demonstrate the scalability of the proposed approach by training several\\nneural language models on a 47M-word corpus with a 80K-word vocabulary,\\nobtaining state-of-the-art results on the Microsoft Research Sentence\\nCompletion Challenge dataset.\\n',\n",
              " '  In many multilingual text classification problems, the documents in different\\nlanguages often share the same set of categories. To reduce the labeling cost\\nof training a classification model for each individual language, it is\\nimportant to transfer the label knowledge gained from one language to another\\nlanguage by conducting cross language classification. In this paper we develop\\na novel subspace co-regularized multi-view learning method for cross language\\ntext classification. This method is built on parallel corpora produced by\\nmachine translation. It jointly minimizes the training error of each classifier\\nin each language while penalizing the distance between the subspace\\nrepresentations of parallel documents. Our empirical study on a large set of\\ncross language text classification tasks shows the proposed method consistently\\noutperforms a number of inductive methods, domain adaptation methods, and\\nmulti-view learning methods.\\n',\n",
              " '  We present a novel technique to remove spurious ambiguity from transition\\nsystems for dependency parsing. Our technique chooses a canonical sequence of\\ntransition operations (computation) for a given dependency tree. Our technique\\ncan be applied to a large class of bottom-up transition systems, including for\\ninstance Nivre (2004) and Attardi (2006).\\n',\n",
              " '  We now have a rich and growing set of modeling tools and algorithms for\\ninducing linguistic structure from text that is less than fully annotated. In\\nthis paper, we discuss some of the weaknesses of our current methodology. We\\npresent a new abstract framework for evaluating natural language processing\\n(NLP) models in general and unsupervised NLP models in particular. The central\\nidea is to make explicit certain adversarial roles among researchers, so that\\nthe different roles in an evaluation are more clearly defined and performers of\\nall roles are offered ways to make measurable contributions to the larger goal.\\nAdopting this approach may help to characterize model successes and failures by\\nencouraging earlier consideration of error analysis. The framework can be\\ninstantiated in a variety of ways, simulating some familiar intrinsic and\\nextrinsic evaluations as well as some new evaluations.\\n',\n",
              " '  In this paper, we applied a novel learning algorithm, namely, Deep Belief\\nNetworks (DBN) to word sense disambiguation (WSD). DBN is a probabilistic\\ngenerative model composed of multiple layers of hidden units. DBN uses\\nRestricted Boltzmann Machine (RBM) to greedily train layer by layer as a\\npretraining. Then, a separate fine tuning step is employed to improve the\\ndiscriminative power. We compared DBN with various state-of-the-art supervised\\nlearning algorithms in WSD such as Support Vector Machine (SVM), Maximum\\nEntropy model (MaxEnt), Naive Bayes classifier (NB) and Kernel Principal\\nComponent Analysis (KPCA). We used all words in the given paragraph,\\nsurrounding context words and part-of-speech of surrounding words as our\\nknowledge sources. We conducted our experiment on the SENSEVAL-2 data set. We\\nobserved that DBN outperformed all other learning algorithms.\\n',\n",
              " '  This paper addresses the problem of mapping natural language sentences to\\nlambda-calculus encodings of their meaning. We describe a learning algorithm\\nthat takes as input a training set of sentences labeled with expressions in the\\nlambda calculus. The algorithm induces a grammar for the problem, along with a\\nlog-linear model that represents a distribution over syntactic and semantic\\nanalyses conditioned on the input sentence. We apply the method to the task of\\nlearning natural language interfaces to databases and show that the learned\\nparsers outperform previous methods in two benchmark database domains.\\n',\n",
              " '  The statistical methods derived and described in this thesis provide new ways\\nto elucidate the structural properties of text and other symbolic sequences.\\nGenerically, these methods allow detection of a difference in the frequency of\\na single feature, the detection of a difference between the frequencies of an\\nensemble of features and the attribution of the source of a text. These three\\nabstract tasks suffice to solve problems in a wide variety of settings.\\nFurthermore, the techniques described in this thesis can be extended to provide\\na wide range of additional tests beyond the ones described here.\\n  A variety of applications for these methods are examined in detail. These\\napplications are drawn from the area of text analysis and genetic sequence\\nanalysis. The textually oriented tasks include finding interesting collocations\\nand cooccurent phrases, language identification, and information retrieval. The\\nbiologically oriented tasks include species identification and the discovery of\\npreviously unreported long range structure in genes. In the applications\\nreported here where direct comparison is possible, the performance of these new\\nmethods substantially exceeds the state of the art.\\n  Overall, the methods described here provide new and effective ways to analyse\\ntext and other symbolic sequences. Their particular strength is that they deal\\nwell with situations where relatively little data are available. Since these\\nmethods are abstract in nature, they can be applied in novel situations with\\nrelative ease.\\n',\n",
              " '  This paper summarises the current state-of-the art in the study of\\ncompositionality in distributional semantics, and major challenges for this\\narea. We single out generalised quantifiers and intensional semantics as areas\\non which to focus attention for the development of the theory. Once suitable\\ntheories have been developed, algorithms will be needed to apply the theory to\\ntasks. Evaluation is a major problem; we single out application to recognising\\ntextual entailment and machine translation for this purpose.\\n',\n",
              " \"  The distribution of frequency counts of distinct words by length in a\\nlanguage's vocabulary will be analyzed using two methods. The first, will look\\nat the empirical distributions of several languages and derive a distribution\\nthat reasonably explains the number of distinct words as a function of length.\\nWe will be able to derive the frequency count, mean word length, and variance\\nof word length based on the marginal probability of letters and spaces. The\\nsecond, based on information theory, will demonstrate that the conditional\\nentropies can also be used to estimate the frequency of distinct words of a\\ngiven length in a language. In addition, it will be shown how these techniques\\ncan also be applied to estimate higher order entropies using vocabulary word\\nlength.\\n\",\n",
              " '  The following study presents a collocation extraction approach based on\\nclustering technique. This study uses a combination of several classical\\nmeasures which cover all aspects of a given corpus then it suggests separating\\nbigrams found in the corpus in several disjoint groups according to the\\nprobability of presence of collocations. This will allow excluding groups where\\nthe presence of collocations is very unlikely and thus reducing in a meaningful\\nway the search space.\\n',\n",
              " '  The work of automatic segmentation of a Manipuri language (or Meiteilon) word\\ninto syllabic units is demonstrated in this paper. This language is a scheduled\\nIndian language of Tibeto-Burman origin, which is also a very highly\\nagglutinative language. This language usages two script: a Bengali script and\\nMeitei Mayek (Script). The present work is based on the second script. An\\nalgorithm is designed so as to identify mainly the syllables of Manipuri origin\\nword. The result of the algorithm shows a Recall of 74.77, Precision of 91.21\\nand F-Score of 82.18 which is a reasonable score with the first attempt of such\\nkind for this language.\\n',\n",
              " \"  Our goal in this paper is to establish a means for a dialogue platform to be\\nable to cope with open domains considering the possible interaction between the\\nembodied agent and humans. To this end we present an algorithm capable of\\nprocessing natural language utterances and validate them against knowledge\\nstructures of an intelligent agent's mind. Our algorithm leverages dialogue\\ntechniques in order to solve ambiguities and acquire knowledge about unknown\\nentities.\\n\",\n",
              " \"  The notion of appropriate sequence as introduced by Z. Harris provides a\\npowerful syntactic way of analysing the detailed meaning of various sentences,\\nincluding ambiguous ones. In an adjectival sentence like 'The leather was\\nyellow', the introduction of an appropriate noun, here 'colour', specifies\\nwhich quality the adjective describes. In some other adjectival sentences with\\nan appropriate noun, that noun plays the same part as 'colour' and seems to be\\nrelevant to the description of the adjective. These appropriate nouns can\\nusually be used in elementary sentences like 'The leather had some colour', but\\nin many cases they have a more or less obligatory modifier. For example, you\\ncan hardly mention that an object has a colour without qualifying that colour\\nat all. About 300 French nouns are appropriate in at least one adjectival\\nsentence and have an obligatory modifier. They enter in a number of sentence\\nstructures related by several syntactic transformations. The appropriateness of\\nthe noun and the fact that the modifier is obligatory are reflected in these\\ntransformations. The description of these syntactic phenomena provides a basis\\nfor a classification of these nouns. It also concerns the lexical properties of\\nthousands of predicative adjectives, and in particular the relations between\\nthe sentence without the noun : 'The leather was yellow' and the adjectival\\nsentence with the noun : 'The colour of the leather was yellow'.\\n\",\n",
              " '  The comparative evaluation of Arabic HPSG grammar lexica requires a deep\\nstudy of their linguistic coverage. The complexity of this task results mainly\\nfrom the heterogeneity of the descriptive components within those lexica\\n(underlying linguistic resources and different data categories, for example).\\nIt is therefore essential to define more homogeneous representations, which in\\nturn will enable us to compare them and eventually merge them. In this context,\\nwe present a method for comparing HPSG lexica based on a rule system. This\\nmethod is implemented within a prototype for the projection from Arabic HPSG to\\na normalised pivot language compliant with LMF (ISO 24613 - Lexical Markup\\nFramework) and serialised using a TEI (Text Encoding Initiative) based\\nrepresentation. The design of this system is based on an initial study of the\\nHPSG formalism looking at its adequacy for the representation of Arabic, and\\nfrom this, we identify the appropriate feature structures corresponding to each\\nArabic lexical category and their possible LMF counterparts.\\n',\n",
              " '  Hindi being a highly inflectional language, FST (Finite State Transducer)\\nbased approach is most efficient for developing a morphological analyzer for\\nthis language. The work presented in this paper uses the SFST (Stuttgart Finite\\nState Transducer) tool for generating the FST. A lexicon of root words is\\ncreated. Rules are then added for generating inflectional and derivational\\nwords from these root words. The Morph Analyzer developed was used in a Part Of\\nSpeech (POS) Tagger based on Stanford POS Tagger. The system was first trained\\nusing a manually tagged corpus and MAXENT (Maximum Entropy) approach of\\nStanford POS tagger was then used for tagging input sentences. The\\nmorphological analyzer gives approximately 97% correct results. POS tagger\\ngives an accuracy of approximately 87% for the sentences that have the words\\nknown to the trained model file, and 80% accuracy for the sentences that have\\nthe words unknown to the trained model file.\\n',\n",
              " '  In this article we focus firstly on the principle of pedagogical indexing and\\ncharacteristics of Arabic language and secondly on the possibility of adapting\\nthe standard for describing learning resources used (the LOM and its\\nApplication Profiles) with learning conditions such as the educational levels\\nof students and their levels of understanding,... the educational context with\\ntaking into account the representative elements of text, text length, ... in\\nparticular, we put in relief the specificity of the Arabic language which is a\\ncomplex language, characterized by its flexion, its voyellation and\\nagglutination.\\n',\n",
              " '  The sense analysis is still critical problem in machine translation system,\\nespecially such as English-Korean translation which the syntactical different\\nbetween source and target languages is very great. We suggest a method for\\nselecting the noun sense using contextual feature in English-Korean\\nTranslation.\\n',\n",
              " '  With such increasing popularity and availability of digital text data,\\nauthorships of digital texts can not be taken for granted due to the ease of\\ncopying and parsing. This paper presents a new text style analysis called\\nnatural frequency zoned word distribution analysis (NFZ-WDA), and then a basic\\nauthorship attribution scheme and an open authorship attribution scheme for\\ndigital texts based on the analysis. NFZ-WDA is based on the observation that\\nall authors leave distinct intrinsic word usage traces on texts written by them\\nand these intrinsic styles can be identified and employed to analyze the\\nauthorship. The intrinsic word usage styles can be estimated through the\\nanalysis of word distribution within a text, which is more than normal word\\nfrequency analysis and can be expressed as: which groups of words are used in\\nthe text; how frequently does each group of words occur; how are the\\noccurrences of each group of words distributed in the text. Next, the basic\\nauthorship attribution scheme and the open authorship attribution scheme\\nprovide solutions for both closed and open authorship attribution problems.\\nThrough analysis and extensive experimental studies, this paper demonstrates\\nthe efficiency of the proposed method for authorship attribution.\\n',\n",
              " \"  A recent advance in computer technology has permitted scientists to implement\\nand test algorithms that were known from quite some time (or not) but which\\nwere computationally expensive. Two such projects are IBM's Jeopardy as a part\\nof its DeepQA project [1] and Wolfram's Wolframalpha[2]. Both these methods\\nimplement natural language processing (another goal of AI scientists) and try\\nto answer questions as asked by the user. Though the goal of the two projects\\nis similar, both of them have a different procedure at it's core. In the\\nfollowing sections, the mechanism and history of IBM's Jeopardy and Wolfram\\nalpha has been explained followed by the implications of these projects in\\nrealizing Ray Kurzweil's [3] dream of passing the Turing test by 2029. A recipe\\nof taking the above projects to a new level is also explained.\\n\",\n",
              " '  In this paper, we present a new approach dedicated to correcting the spelling\\nerrors of the Arabic language. This approach corrects typographical errors like\\ninserting, deleting, and permutation. Our method is inspired from the\\nLevenshtein algorithm, and allows a finer and better scheduling than\\nLevenshtein. The results obtained are very satisfactory and encouraging, which\\nshows the interest of our new approach.\\n',\n",
              " '  Dynamics of average length of words in Russian and English is analysed in the\\narticle. Words belonging to the diachronic text corpus Google Books Ngram and\\ndated back to the last two centuries are studied. It was found out that average\\nword length slightly increased in the 19th century, and then it was growing\\nrapidly most of the 20th century and started decreasing over the period from\\nthe end of the 20th - to the beginning of the 21th century. Words which\\ncontributed mostly to increase or decrease of word average length were\\nidentified. At that, content words and functional words are analysed\\nseparately. Long content words contribute mostly to word average length of\\nword. As it was shown, these words reflect the main tendencies of social\\ndevelopment and thus, are used frequently. Change of frequency of personal\\npronouns also contributes significantly to change of average word length. The\\nother parameters connected with average length of word were also analysed.\\n',\n",
              " '  Stylometry is the study of the unique linguistic styles and writing behaviors\\nof individuals. It belongs to the core task of text categorization like\\nauthorship identification, plagiarism detection etc. Though reasonable number\\nof studies have been conducted in English language, no major work has been done\\nso far in Bengali. In this work, We will present a demonstration of authorship\\nidentification of the documents written in Bengali. We adopt a set of\\nfine-grained stylistic features for the analysis of the text and use them to\\ndevelop two different models: statistical similarity model consisting of three\\nmeasures and their combination, and machine learning model with Decision Tree,\\nNeural Network and SVM. Experimental results show that SVM outperforms other\\nstate-of-the-art methods after 10-fold cross validations. We also validate the\\nrelative importance of each stylistic feature to show that some of them remain\\nconsistently significant in every model used in this experiment.\\n',\n",
              " '  Data association methods are used by autonomous robots to find matches\\nbetween the current landmarks and the new set of observed features. We seek a\\nframework for opinion mining to benefit from advancements in autonomous robot\\nnavigation in both research and development\\n',\n",
              " '  Written Communication on Computers requires knowledge of writing text for the\\ndesired language using Computer. Mostly people do not use any other language\\nbesides English. This creates a barrier. To resolve this issue we have\\ndeveloped a scheme to input text in Hindi using phonetic mapping scheme. Using\\nthis scheme we generate intermediate code strings and match them with\\npronunciations of input text. Our system show significant success over other\\ninput systems available.\\n',\n",
              " '  Natural Language Parsing has been the most prominent research area since the\\ngenesis of Natural Language Processing. Probabilistic Parsers are being\\ndeveloped to make the process of parser development much easier, accurate and\\nfast. In Indian context, identification of which Computational Grammar\\nFormalism is to be used is still a question which needs to be answered. In this\\npaper we focus on this problem and try to analyze different formalisms for\\nIndian languages.\\n',\n",
              " \"  This paper defines a method for lexicon in the biomedical domain from\\ncomparable corpora. The method is based on compositional translation and\\nexploits morpheme-level translation equivalences. It can generate translations\\nfor a large variety of morphologically constructed words and can also generate\\n'fertile' translations. We show that fertile translations increase the overall\\nquality of the extracted lexicon for English to French translation.\\n\",\n",
              " '  In linguistic morphology and information retrieval, stemming is the process\\nfor reducing inflected (or sometimes derived) words to their stem, base or root\\nform; generally a written word form. In this work is presented suffix stripping\\nstemmer for Serbian language, one of the highly inflectional languages.\\n',\n",
              " '  The utility and power of Natural Language Processing (NLP) seems destined to\\nchange our technological society in profound and fundamental ways. However\\nthere are, to date, few accessible descriptions of the science of NLP that have\\nbeen written for a popular audience, or even for an audience of intelligent,\\nbut uninitiated scientists. This paper aims to provide just such an overview.\\nIn short, the objective of this article is to describe the purpose, procedures\\nand practical applications of NLP in a clear, balanced, and readable way. We\\nwill examine the most recent literature describing the methods and processes of\\nNLP, analyze some of the challenges that researchers are faced with, and\\nbriefly survey some of the current and future applications of this science to\\nIT research in general.\\n',\n",
              " '  In this paper, we show the possibility of using a linear Conditional Random\\nFields (CRF) for terminology extraction from a specialized text corpus.\\n',\n",
              " '  We present an approach for detecting multiword phrases in mathematical text\\ncorpora. The method used is based on characteristic features of mathematical\\nterminology. It makes use of a software tool named Lingo which allows to\\nidentify words by means of previously defined dictionaries for specific word\\nclasses as adjectives, personal names or nouns. The detection of multiword\\ngroups is done algorithmically. Possible advantages of the method for indexing\\nand information retrieval and conclusions for applying dictionary-based methods\\nof automatic indexing instead of stemming procedures are discussed.\\n',\n",
              " '  Quick Summary is an innovate implementation of an automatic document\\nsummarizer that inputs a document in the English language and evaluates each\\nsentence. The scanner or evaluator determines criteria based on its grammatical\\nstructure and place in the paragraph. The program then asks the user to specify\\nthe number of sentences the person wishes to highlight. For example should the\\nuser ask to have three of the most important sentences, it would highlight the\\nfirst and most important sentence in green. Commonly this is the sentence\\ncontaining the conclusion. Then Quick Summary finds the second most important\\nsentence usually called a satellite and highlights it in yellow. This is\\nusually the topic sentence. Then the program finds the third most important\\nsentence and highlights it in red. The implementations of this technology are\\nuseful in a society of information overload when a person typically receives 42\\nemails a day (Microsoft). The paper also is a candid look at difficulty that\\nmachine learning has in textural translating. However, it speaks on how to\\novercome the obstacles that historically prevented progress. This paper\\nproposes mathematical meta-data criteria that justify the place of importance\\nof a sentence. Just as tools for the study of relational symmetry in\\nbio-informatics, this tool seeks to classify words with greater clarity.\\n\"Survey Finds Workers Average Only Three Productive Days per Week.\" Microsoft\\nNews Center. Microsoft. Web. 31 Mar. 2012.\\n',\n",
              " \"  Stylometry, the science of inferring characteristics of the author from the\\ncharacteristics of documents written by that author, is a problem with a long\\nhistory and belongs to the core task of Text categorization that involves\\nauthorship identification, plagiarism detection, forensic investigation,\\ncomputer security, copyright and estate disputes etc. In this work, we present\\na strategy for stylometry detection of documents written in Bengali. We adopt a\\nset of fine-grained attribute features with a set of lexical markers for the\\nanalysis of the text and use three semi-supervised measures for making\\ndecisions. Finally, a majority voting approach has been taken for final\\nclassification. The system is fully automatic and language-independent.\\nEvaluation results of our attempt for Bengali author's stylometry detection\\nshow reasonably promising accuracy in comparison to the baseline model.\\n\",\n",
              " \"  Financial statements contain quantitative information and manager's\\nsubjective evaluation of firm's financial status. Using information released in\\nU.S. 10-K filings. Both qualitative and quantitative appraisals are crucial for\\nquality financial decisions. To extract such opinioned statements from the\\nreports, we built tagging models based on the conditional random field (CRF)\\ntechniques, considering a variety of combinations of linguistic factors\\nincluding morphology, orthography, predicate-argument structure, syntax, and\\nsimple semantics. Our results show that the CRF models are reasonably effective\\nto find opinion holders in experiments when we adopted the popular MPQA corpus\\nfor training and testing. The contribution of our paper is to identify opinion\\npatterns in multiword expressions (MWEs) forms rather than in single word\\nforms.\\n  We find that the managers of corporations attempt to use more optimistic\\nwords to obfuscate negative financial performance and to accentuate the\\npositive financial performance. Our results also show that decreasing earnings\\nwere often accompanied by ambiguous and mild statements in the reporting year\\nand that increasing earnings were stated in assertive and positive way.\\n\",\n",
              " \"  The majority of online reviews consist of plain-text feedback together with a\\nsingle numeric score. However, there are multiple dimensions to products and\\nopinions, and understanding the `aspects' that contribute to users' ratings may\\nhelp us to better understand their individual preferences. For example, a\\nuser's impression of an audiobook presumably depends on aspects such as the\\nstory and the narrator, and knowing their opinions on these aspects may help us\\nto recommend better products. In this paper, we build models for rating systems\\nin which such dimensions are explicit, in the sense that users leave separate\\nratings for each aspect of a product. By introducing new corpora consisting of\\nfive million reviews, rated with between three and six aspects, we evaluate our\\nmodels on three prediction tasks: First, we use our model to uncover which\\nparts of a review discuss which of the rated aspects. Second, we use our model\\nto summarize reviews, which for us means finding the sentences that best\\nexplain a user's rating. Finally, since aspect ratings are optional in many of\\nthe datasets we consider, we use our model to recover those ratings that are\\nmissing from a user's evaluation. Our model matches state-of-the-art approaches\\non existing small-scale datasets, while scaling to the real-world datasets we\\nintroduce. Moreover, our model is able to `disentangle' content and sentiment\\nwords: we automatically learn content words that are indicative of a particular\\naspect as well as the aspect-specific sentiment words that are indicative of a\\nparticular rating.\\n\",\n",
              " \"  We present a study of the relationship between gender, linguistic style, and\\nsocial networks, using a novel corpus of 14,000 Twitter users. Prior\\nquantitative work on gender often treats this social variable as a female/male\\nbinary; we argue for a more nuanced approach. By clustering Twitter users, we\\nfind a natural decomposition of the dataset into various styles and topical\\ninterests. Many clusters have strong gender orientations, but their use of\\nlinguistic resources sometimes directly conflicts with the population-level\\nlanguage statistics. We view these clusters as a more accurate reflection of\\nthe multifaceted nature of gendered language styles. Previous corpus-based work\\nhas also had little to say about individuals whose linguistic styles defy\\npopulation-level gender patterns. To identify such individuals, we train a\\nstatistical classifier, and measure the classifier confidence for each\\nindividual in the dataset. Examining individuals whose language does not match\\nthe classifier's model for their gender, we find that they have social networks\\nthat include significantly fewer same-gender social connections and that, in\\ngeneral, social network homophily is correlated with the use of same-gender\\nlanguage markers. Pairing computational methods and social theory thus offers a\\nnew perspective on how gender emerges as individuals position themselves\\nrelative to audiences, topics, and mainstream gender norms.\\n\",\n",
              " '  This paper presents a novel approach to the problem of semantic parsing via\\nlearning the correspondences between complex sentences and rich sets of events.\\nOur main intuition is that correct correspondences tend to occur more\\nfrequently. Our model benefits from a discriminative notion of similarity to\\nlearn the correspondence between sentence and an event and a ranking machinery\\nthat scores the popularity of each correspondence. Our method can discover a\\ngroup of events (called macro-events) that best describes a sentence. We\\nevaluate our method on our novel dataset of professional soccer commentaries.\\nThe empirical results show that our method significantly outperforms the\\nstate-of-theart.\\n',\n",
              " '  Computer-mediated communication is driving fundamental changes in the nature\\nof written language. We investigate these changes by statistical analysis of a\\ndataset comprising 107 million Twitter messages (authored by 2.7 million unique\\nuser accounts). Using a latent vector autoregressive model to aggregate across\\nthousands of words, we identify high-level patterns in diffusion of linguistic\\nchange over the United States. Our model is robust to unpredictable changes in\\nTwitter\\'s sampling rate, and provides a probabilistic characterization of the\\nrelationship of macro-scale linguistic influence to a set of demographic and\\ngeographic predictors. The results of this analysis offer support for prior\\narguments that focus on geographical proximity and population size. However,\\ndemographic similarity -- especially with regard to race -- plays an even more\\ncentral role, as cities with similar racial demographics are far more likely to\\nshare linguistic influence. Rather than moving towards a single unified\\n\"netspeak\" dialect, language evolution in computer-mediated communication\\nreproduces existing fault lines in spoken American English.\\n',\n",
              " '  Basic body-part names (BBPNs) were defined as body-part names in Swadesh\\nbasic 200 words. Non-Mayan cognates of Mayan (MY) BBPNs were extensively\\nsearched for, by comparing with non-MY vocabulary, including ca.1300 basic\\nwords of 82 AN languages listed by Tryon (1985), etc. Thus found cognates (CGs)\\nin non-MY are listed in Table 1, as classified by language groups to which most\\nsimilar cognates (MSCs) of MY BBPNs belong. CGs of MY are classified to 23\\nmutually unrelated CG-items, of which 17.5 CG-items have their MSCs in\\nAustronesian (AN), giving its closest similarity score (CSS), CSS(AN) = 17.5,\\nwhich consists of 10.33 MSCs in Formosan, 1.83 MSCs in Western\\nMalayo-Polynesian (W.MP), 0.33 in Central MP, 0.0 in SHWNG, and 5.0 in Oceanic\\n[i.e., CSS(FORM)= 10.33, CSS(W.MP) = 1.88, ..., CSS(OC)= 5.0]. These CSSs for\\nlanguage (sub)groups are also listed in the underline portion of every section\\nof (Section1 - Section 6) in Table 1. Chi-squar test (degree of freedom = 1)\\nusing [Eq 1] and [Eqs.2] revealed that MSCs of MY BBPNs are distributed in\\nFormosan in significantly higher frequency (P < 0.001) than in other subgroups\\nof AN, as well as than in non-AN languages. MY is thus concluded to have been\\nderived from Formosan of AN. Eskimo shows some BBPN similarities to FORM and\\nMY.\\n',\n",
              " '  Gujarati is a resource poor language with almost no language processing tools\\nbeing available. In this paper we have shown an implementation of a rule based\\nstemmer of Gujarati. We have shown the creation of rules for stemming and the\\nrichness in morphology that Gujarati possesses. We have also evaluated our\\nresults by verifying it with a human expert.\\n',\n",
              " '  Developing parallel corpora is an important and a difficult activity for\\nMachine Translation. This requires manual annotation by Human Translators.\\nTranslating same text again is a useless activity. There are tools available to\\nimplement this for European Languages, but no such tool is available for Indian\\nLanguages. In this paper we present a tool for Indian Languages which not only\\nprovides automatic translations of the previously available translation but\\nalso provides multiple translations, in cases where a sentence has multiple\\ntranslations, in ranked list of suggestive translations for a sentence.\\nMoreover this tool also lets translators have global and local saving options\\nof their work, so that they may share it with others, which further lightens\\nthe task.\\n',\n",
              " \"  In this paper, we demonstrate and discuss results of our mining the abstracts\\nof the publications in Harvard Business Review between 1922 and 2012.\\nTechniques for computing n-grams, collocations, basic sentiment analysis, and\\nnamed-entity recognition were employed to uncover trends hidden in the\\nabstracts. We present findings about international relationships, sentiment in\\nHBR's abstracts, important international companies, influential technological\\ninventions, renown researchers in management theories, US presidents via\\nchronological analyses.\\n\",\n",
              " '  This paper proposes a method for extracting translations of morphologically\\nconstructed terms from comparable corpora. The method is based on compositional\\ntranslation and exploits translation equivalences at the morpheme-level, which\\nallows for the generation of \"fertile\" translations (translation pairs in which\\nthe target term has more words than the source term). Ranking methods relying\\non corpus-based and translation-based features are used to select the best\\ncandidate translation. We obtain an average precision of 91% on the Top1\\ncandidate translation. The method was tested on two language pairs\\n(English-French and English-German) and with a small specialized comparable\\ncorpora (400k words per language).\\n',\n",
              " '  We report applications of language technology to analyzing historical\\ndocuments in the Database for the Study of Modern Chinese Thoughts and\\nLiterature (DSMCTL). We studied two historical issues with the reported\\ntechniques: the conceptualization of \"huaren\" (Chinese people) and the attempt\\nto institute constitutional monarchy in the late Qing dynasty. We also discuss\\nresearch challenges for supporting sophisticated issues using our experience\\nwith DSMCTL, the Database of Government Officials of the Republic of China, and\\nthe Dream of the Red Chamber. Advanced techniques and tools for lexical,\\nsyntactic, semantic, and pragmatic processing of language information, along\\nwith more thorough data collection, are needed to strengthen the collaboration\\nbetween historians and computer scientists.\\n',\n",
              " \"  The use of naive Bayesian classifier (NB) and the classifier by the k nearest\\nneighbors (kNN) in classification semantic analysis of authors' texts of\\nEnglish fiction has been analysed. The authors' works are considered in the\\nvector space the basis of which is formed by the frequency characteristics of\\nsemantic fields of nouns and verbs. Highly precise classification of authors'\\ntexts in the vector space of semantic fields indicates about the presence of\\nparticular spheres of author's idiolect in this space which characterizes the\\nindividual author's style.\\n\",\n",
              " '  This paper describes the Hangulphabet, a new writing system that should prove\\nuseful in a number of contexts. Using the Hangulphabet, a user can instantly\\nsee voicing, manner and place of articulation of any phoneme found in human\\nlanguage. The Hangulphabet places consonant graphemes on a grid with the x-axis\\nrepresenting the place of articulation and the y-axis representing manner of\\narticulation. Each individual grapheme contains radicals from both axes where\\nthe points intersect. The top radical represents manner of articulation where\\nthe bottom represents place of articulation. A horizontal line running through\\nthe middle of the bottom radical represents voicing. For vowels, place of\\narticulation is located on a grid that represents the position of the tongue in\\nthe mouth. This grid is similar to that of the IPA vowel chart (International\\nPhonetic Association, 1999). The difference with the Hangulphabet being the\\ntrapezoid representing the vocal apparatus is on a slight tilt. Place of\\narticulation for a vowel is represented by a breakout figure from the grid.\\nThis system can be used as an alternative to the International Phonetic\\nAlphabet (IPA) or as a complement to it. Beginning students of linguistics may\\nfind it particularly useful. A Hangulphabet font has been created to facilitate\\nswitching between the Hangulphabet and the IPA.\\n',\n",
              " '  The model of semantic concept lattice for data mining of microblogs has been\\nproposed in this work. It is shown that the use of this model is effective for\\nthe semantic relations analysis and for the detection of associative rules of\\nkey words.\\n',\n",
              " '  In this paper, we investigate how to optimize the vocabulary for a voice\\nsearch language model. The metric we optimize over is the out-of-vocabulary\\n(OoV) rate since it is a strong indicator of user experience. In a departure\\nfrom the usual way of measuring OoV rates, web search logs allow us to compute\\nthe per-session OoV rate and thus estimate the percentage of users that\\nexperience a given OoV rate. Under very conservative text normalization, we\\nfind that a voice search vocabulary consisting of 2 to 2.5 million words\\nextracted from 1 week of search query data will result in an aggregate OoV rate\\nof 1%; at that size, the same OoV rate will also be experienced by 90% of\\nusers. The number of words included in the vocabulary is a stable indicator of\\nthe OoV rate. Altering the freshness of the vocabulary or the duration of the\\ntime window over which the training data is gathered does not significantly\\nchange the OoV rate. Surprisingly, a significantly larger vocabulary\\n(approximately 10 million words) is required to guarantee OoV rates below 1%\\nfor 95% of the users.\\n',\n",
              " '  Large language models have been proven quite beneficial for a variety of\\nautomatic speech recognition tasks in Google. We summarize results on Voice\\nSearch and a few YouTube speech transcription tasks to highlight the impact\\nthat one can expect from increasing both the amount of training data, and the\\nsize of the language model estimated from such data. Depending on the task,\\navailability and amount of training data used, language model size and amount\\nof work and care put into integrating them in the lattice rescoring step we\\nobserve reductions in word error rate between 6% and 10% relative, for systems\\non a wide range of operating points between 17% and 52% word error rate.\\n',\n",
              " '  In principle, the design of transition-based dependency parsers makes it\\npossible to experiment with any general-purpose classifier without other\\nchanges to the parsing algorithm. In practice, however, it often takes\\nsubstantial software engineering to bridge between the different\\nrepresentations used by two software packages. Here we present extensions to\\nMaltParser that allow the drop-in use of any classifier conforming to the\\ninterface of the Weka machine learning package, a wrapper for the TiMBL\\nmemory-based learner to this interface, and experiments on multilingual\\ndependency parsing with a variety of classifiers. While earlier work had\\nsuggested that memory-based learners might be a good choice for low-resource\\nparsing scenarios, we cannot support that hypothesis in this work. We observed\\nthat support-vector machines give better parsing performance than the\\nmemory-based learner, regardless of the size of the training set.\\n',\n",
              " '  Controlled natural languages (mostly English-based) recently have emerged as\\nseemingly informal supplementary means for OWL ontology authoring, if compared\\nto the formal notations that are used by professional knowledge engineers. In\\nthis paper we present by examples controlled Latvian language that has been\\ndesigned to be compliant with the state of the art Attempto Controlled English.\\nWe also discuss relation with controlled Lithuanian language that is being\\ndesigned in parallel.\\n',\n",
              " \"  Analyzing writing styles of non-native speakers is a challenging task. In\\nthis paper, we analyze the comments written in the discussion pages of the\\nEnglish Wikipedia. Using learning algorithms, we are able to detect native\\nspeakers' writing style with an accuracy of 74%. Given the diversity of the\\nEnglish Wikipedia users and the large number of languages they speak, we\\nmeasure the similarities among their native languages by comparing the\\ninfluence they have on their English writing style. Our results show that\\nlanguages known to have the same origin and development path have similar\\nfootprint on their speakers' English writing style. To enable further studies,\\nthe dataset we extracted from Wikipedia will be made available publicly.\\n\",\n",
              " \"  This paper tackles temporal resolution of documents, such as determining when\\na document is about or when it was written, based only on its text. We apply\\ntechniques from information retrieval that predict dates via language models\\nover a discretized timeline. Unlike most previous works, we rely {\\\\it solely}\\non temporal cues implicit in the text. We consider both document-likelihood and\\ndivergence based techniques and several smoothing methods for both of them. Our\\nbest model predicts the mid-point of individuals' lives with a median of 22 and\\nmean error of 36 years for Wikipedia biographies from 3800 B.C. to the present\\nday. We also show that this approach works well when training on such\\nbiographies and predicting dates both for non-biographical Wikipedia pages\\nabout specific years (500 B.C. to 2010 A.D.) and for publication dates of short\\nstories (1798 to 2008). Together, our work shows that, even in absence of\\ntemporal extraction resources, it is possible to achieve remarkable temporal\\nlocality across a diverse set of texts.\\n\",\n",
              " '  Aiming at increasing system simplicity and flexibility, an audio evoked based\\nsystem was developed by integrating simplified headphone and user-friendly\\nsoftware design. This paper describes a Hindi Speech Actuated Computer\\nInterface for Web search (HSACIWS), which accepts spoken queries in Hindi\\nlanguage and provides the search result on the screen. This system recognizes\\nspoken queries by large vocabulary continuous speech recognition (LVCSR),\\nretrieves relevant document by text retrieval, and provides the search result\\non the Web by the integration of the Web and the voice systems. The LVCSR in\\nthis system showed enough performance levels for speech with acoustic and\\nlanguage models derived from a query corpus with target contents.\\n',\n",
              " '  Controlled natural languages (CNL) with a direct mapping to formal logic have\\nbeen proposed to improve the usability of knowledge representation systems,\\nquery interfaces, and formal specifications. Predictive editors are a popular\\napproach to solve the problem that CNLs are easy to read but hard to write.\\nSuch predictive editors need to be able to \"look ahead\" in order to show all\\npossible continuations of a given unfinished sentence. Such lookahead features,\\nhowever, are difficult to implement in a satisfying way with existing grammar\\nframeworks, especially if the CNL supports complex nonlocal structures such as\\nanaphoric references. Here, methods and algorithms are presented for a new\\ngrammar notation called Codeco, which is specifically designed for controlled\\nnatural languages and predictive editors. A parsing approach for Codeco based\\non an extended chart parsing algorithm is presented. A large subset of Attempto\\nControlled English (ACE) has been represented in Codeco. Evaluation of this\\ngrammar and the parser implementation shows that the approach is practical,\\nadequate and efficient.\\n',\n",
              " \"  Web users produce more and more documents expressing opinions. Because these\\nhave become important resources for customers and manufacturers, many have\\nfocused on them. Opinions are often expressed through adjectives with positive\\nor negative semantic values. In extracting information from users' opinion in\\nonline reviews, exact recognition of the semantic polarity of adjectives is one\\nof the most important requirements. Since adjectives have different semantic\\norientations according to contexts, it is not satisfying to extract opinion\\ninformation without considering the semantic and lexical relations between the\\nadjectives and the feature nouns appropriate to a given domain. In this paper,\\nwe present a classification of adjectives by polarity, and we analyze\\nadjectives that are undetermined in the absence of contexts. Our research\\nshould be useful for accurately predicting semantic orientations of opinion\\nsentences, and should be taken into account before relying on an automatic\\nmethods.\\n\",\n",
              " '  The performance of a Statistical Machine Translation System (SMT) system is\\nproportionally directed to the quality and length of the parallel corpus it\\nuses. However for some pair of languages there is a considerable lack of them.\\nThe long term goal is to construct a Japanese-Spanish parallel corpus to be\\nused for SMT, whereas, there are a lack of useful Japanese-Spanish parallel\\nCorpus. To address this problem, In this study we proposed a method for\\nextracting Japanese-Spanish Parallel Sentences from Wikipedia using POS tagging\\nand Rule-Based approach. The main focus of this approach is the syntactic\\nfeatures of both languages. Human evaluation was performed over a sample and\\nshows promising results, in comparison with the baseline.\\n',\n",
              " '  In this paper, I describe several approaches to automatic or semi-automatic\\ndevelopment of symbolic rules for grammar checkers from the information\\ncontained in corpora. The rules obtained this way are an important addition to\\nmanually-created rules that seem to dominate in rule-based checkers. However,\\nthe manual process of creation of rules is costly, time-consuming and\\nerror-prone. It seems therefore advisable to use machine-learning algorithms to\\ncreate the rules automatically or semi-automatically. The results obtained seem\\nto corroborate my initial hypothesis that symbolic machine learning algorithms\\ncan be useful for acquiring new rules for grammar checking. It turns out,\\nhowever, that for practical uses, error corpora cannot be the sole source of\\ninformation used in grammar checking. I suggest therefore that only by using\\ndifferent approaches, grammar-checkers, or more generally, computer-aided\\nproofreading tools, will be able to cover most frequent and severe mistakes and\\navoid false alarms that seem to distract users.\\n',\n",
              " '  We introduce efficient algorithms for finding the $k$ shortest paths of a\\nweighted pushdown automaton (WPDA), a compact representation of a weighted set\\nof strings with potential applications in parsing and machine translation. Both\\nof our algorithms are derived from the same weighted deductive logic\\ndescription of the execution of a WPDA using different search strategies.\\nExperimental results show our Algorithm 2 adds very little overhead vs. the\\nsingle shortest path algorithm, even with a large $k$.\\n',\n",
              " \"  In this paper we present a new and simple language-independent method for\\nword-alignment based on the use of external sources of bilingual information\\nsuch as machine translation systems. We show that the few parameters of the\\naligner can be trained on a very small corpus, which leads to results\\ncomparable to those obtained by the state-of-the-art tool GIZA++ in terms of\\nprecision. Regarding other metrics, such as alignment error rate or F-measure,\\nthe parametric aligner, when trained on a very small gold-standard (450 pairs\\nof sentences), provides results comparable to those produced by GIZA++ when\\ntrained on an in-domain corpus of around 10,000 pairs of sentences.\\nFurthermore, the results obtained indicate that the training is\\ndomain-independent, which enables the use of the trained aligner 'on the fly'\\non any new pair of sentences.\\n\",\n",
              " \"  The clustering of text documents in the vector space of semantic fields and\\nin the semantic space with orthogonal basis has been analysed. It is shown that\\nusing the vector space model with the basis of semantic fields is effective in\\nthe cluster analysis algorithms of author's texts in English fiction. The\\nanalysis of the author's texts distribution in cluster structure showed the\\npresence of the areas of semantic space that represent the author's ideolects\\nof individual authors. SVD factorization of the semantic fields matrix makes it\\npossible to reduce significantly the dimension of the semantic space in the\\ncluster analysis of author's texts.\\n\",\n",
              " '  Both supervised learning methods and LDA based topic model have been\\nsuccessfully applied in the field of query focused multi-document\\nsummarization. In this paper, we propose a novel supervised approach that can\\nincorporate rich sentence features into Bayesian topic models in a principled\\nway, thus taking advantages of both topic model and feature based supervised\\nlearning methods. Experiments on TAC2008 and TAC2009 demonstrate the\\neffectiveness of our approach.\\n',\n",
              " '  Graph-based semi-supervised learning has proven to be an effective approach\\nfor query-focused multi-document summarization. The problem of previous\\nsemi-supervised learning is that sentences are ranked without considering the\\nhigher level information beyond sentence level. Researches on general\\nsummarization illustrated that the addition of topic level can effectively\\nimprove the summary quality. Inspired by previous researches, we propose a\\ntwo-layer (i.e. sentence layer and topic layer) graph-based semi-supervised\\nlearning approach. At the same time, we propose a novel topic model which makes\\nfull use of the dependence between sentences and words. Experimental results on\\nDUC and TAC data sets demonstrate the effectiveness of our proposed approach.\\n',\n",
              " \"  Language learning is thought to be a highly complex process. One of the\\nhurdles in learning a language is to learn the rules of syntax of the language.\\nRules of syntax are often ordered in that before one rule can applied one must\\napply another. It has been thought that to learn the order of n rules one must\\ngo through all n! permutations. Thus to learn the order of 27 rules would\\nrequire 27! steps or 1.08889x10^{28} steps. This number is much greater than\\nthe number of seconds since the beginning of the universe! In an insightful\\nanalysis the linguist Block ([Block 86], pp. 62-63, p.238) showed that with the\\nassumption of transitivity this vast number of learning steps reduces to a mere\\n377 steps. We present a mathematical analysis of the complexity of Block's\\nalgorithm. The algorithm has a complexity of order n^2 given n rules. In\\naddition, we improve Block's results exponentially, by introducing an algorithm\\nthat has complexity of order less than n log n.\\n\",\n",
              " '  We show that power-law analyses of financial commentaries from newspaper\\nweb-sites can be used to identify stock market bubbles, supplementing\\ntraditional volatility analyses. Using a four-year corpus of 17,713 online,\\nfinance-related articles (10M+ words) from the Financial Times, the New York\\nTimes, and the BBC, we show that week-to-week changes in power-law\\ndistributions reflect market movements of the Dow Jones Industrial Average\\n(DJI), the FTSE-100, and the NIKKEI-225. Notably, the statistical regularities\\nin language track the 2007 stock market bubble, showing emerging structure in\\nthe language of commentators, as progressively greater agreement arose in their\\npositive perceptions of the market. Furthermore, during the bubble period, a\\nmarked divergence in positive language occurs as revealed by a Kullback-Leibler\\nanalysis.\\n',\n",
              " '  Using a corpus of over 17,000 financial news reports (involving over 10M\\nwords), we perform an analysis of the argument-distributions of the UP- and\\nDOWN-verbs used to describe movements of indices, stocks, and shares. Using\\nmeasures of the overlap in the argument distributions of these verbs and\\nk-means clustering of their distributions, we advance evidence for the proposal\\nthat the metaphors referred to by these verbs are organised into hierarchical\\nstructures of superordinate and subordinate groups.\\n',\n",
              " '  Using a corpus of 17,000+ financial news reports (involving over 10M words),\\nwe perform an analysis of the argument-distributions of the UP and DOWN verbs\\nused to describe movements of indices, stocks and shares. In Study 1\\nparticipants identified antonyms of these verbs in a free-response task and a\\nmatching task from which the most commonly identified antonyms were compiled.\\nIn Study 2, we determined whether the argument-distributions for the verbs in\\nthese antonym-pairs were sufficiently similar to predict the most\\nfrequently-identified antonym. Cosine similarity correlates moderately with the\\nproportions of antonym-pairs identified by people (r = 0.31). More\\nimpressively, 87% of the time the most frequently-identified antonym is either\\nthe first- or second-most similar pair in the set of alternatives. The\\nimplications of these results for distributional approaches to determining\\nmetaphoric knowledge are discussed.\\n',\n",
              " '  We present a method of finding and analyzing shifts in grammatical relations\\nfound in diachronic corpora. Inspired by the econometric technique of measuring\\nreturn and volatility instead of relative frequencies, we propose them as a way\\nto better characterize changes in grammatical patterns like nominalization,\\nmodification and comparison. To exemplify the use of these techniques, we\\nexamine a corpus of NIPS papers and report trends which manifest at the token,\\npart-of-speech and grammatical levels. Building up from frequency observations\\nto a second-order analysis, we show that shifts in frequencies overlook deeper\\ntrends in language, even when part-of-speech information is included. Examining\\ntoken, POS and grammatical levels of variation enables a summary view of\\ndiachronic text as a whole. We conclude with a discussion about how these\\nmethods can inform intuitions about specialist domains as well as changes in\\nlanguage use as a whole.\\n',\n",
              " '  This paper explores two separate questions: Can we perform natural language\\nprocessing tasks without a lexicon?; and, Should we? Existing natural language\\nprocessing techniques are either based on words as units or use units such as\\ngrams only for basic classification tasks. How close can a machine come to\\nreasoning about the meanings of words and phrases in a corpus without using any\\nlexicon, based only on grams?\\n  Our own motivation for posing this question is based on our efforts to find\\npopular trends in words and phrases from online Chinese social media. This form\\nof written Chinese uses so many neologisms, creative character placements, and\\ncombinations of writing systems that it has been dubbed the \"Martian Language.\"\\nReaders must often use visual queues, audible queues from reading out loud, and\\ntheir knowledge and understanding of current events to understand a post. For\\nanalysis of popular trends, the specific problem is that it is difficult to\\nbuild a lexicon when the invention of new ways to refer to a word or concept is\\neasy and common. For natural language processing in general, we argue in this\\npaper that new uses of language in social media will challenge machines\\'\\nabilities to operate with words as the basic unit of understanding, not only in\\nChinese but potentially in other languages.\\n',\n",
              " '  Previous works demonstrated that Automatic Text Summarization (ATS) by\\nsentences extraction may be improved using sentence compression. In this work\\nwe present a sentence compressions approach guided by level-sentence discourse\\nsegmentation and probabilistic language models (LM). The results presented here\\nshow that the proposed solution is able to generate coherent summaries with\\ngrammatical compressed sentences. The approach is simple enough to be\\ntransposed into other languages.\\n',\n",
              " '  Representation of semantic information contained in the words is needed for\\nany Arabic Text Mining applications. More precisely, the purpose is to better\\ntake into account the semantic dependencies between words expressed by the\\nco-occurrence frequencies of these words. There have been many proposals to\\ncompute similarities between words based on their distributions in contexts. In\\nthis paper, we compare and contrast the effect of two preprocessing techniques\\napplied to Arabic corpus: Rootbased (Stemming), and Stem-based (Light Stemming)\\napproaches for measuring the similarity between Arabic words with the well\\nknown abstractive model -Latent Semantic Analysis (LSA)- with a wide variety of\\ndistance functions and similarity measures, such as the Euclidean Distance,\\nCosine Similarity, Jaccard Coefficient, and the Pearson Correlation\\nCoefficient. The obtained results show that, on the one hand, the variety of\\nthe corpus produces more accurate results; on the other hand, the Stem-based\\napproach outperformed the Root-based one because this latter affects the words\\nmeanings.\\n',\n",
              " '  Many approaches to sentiment analysis rely on lexica where words are tagged\\nwith their prior polarity - i.e. if a word out of context evokes something\\npositive or something negative. In particular, broad-coverage resources like\\nSentiWordNet provide polarities for (almost) every word. Since words can have\\nmultiple senses, we address the problem of how to compute the prior polarity of\\na word starting from the polarity of each sense and returning its polarity\\nstrength as an index between -1 and 1. We compare 14 such formulae that appear\\nin the literature, and assess which one best approximates the human judgement\\nof prior polarities, with both regression and classification models.\\n',\n",
              " '  In this paper, we define event expression over sentences of natural language\\nand semantic relations between events. Based on this definition, we formally\\nconsider text understanding process having events as basic unit.\\n',\n",
              " '  We present a new efficient method for approximate search in electronic\\nlexica. Given an input string (the pattern) and a similarity threshold, the\\nalgorithm retrieves all entries of the lexicon that are sufficiently similar to\\nthe pattern. Search is organized in subsearches that always start with an exact\\npartial match where a substring of the input pattern is aligned with a\\nsubstring of a lexicon word. Afterwards this partial match is extended stepwise\\nto larger substrings. For aligning further parts of the pattern with\\ncorresponding parts of lexicon entries, more errors are tolerated at each\\nsubsequent step. For supporting this alignment order, which may start at any\\npart of the pattern, the lexicon is represented as a structure that enables\\nimmediate access to any substring of a lexicon word and permits the extension\\nof such substrings in both directions. Experimental evaluations of the\\napproximate search procedure are given that show significant efficiency\\nimprovements compared to existing techniques. Since the technique can be used\\nfor large error bounds it offers interesting possibilities for approximate\\nsearch in special collections of \"long\" strings, such as phrases, sentences, or\\nbook ti\\n',\n",
              " '  This paper refers to the syntactic analysis of phrases in Romanian, as an\\nimportant process of natural language processing. We will suggest a real-time\\nsolution, based on the idea of using some words or groups of words that\\nindicate grammatical category; and some specific endings of some parts of\\nsentence. Our idea is based on some characteristics of the Romanian language,\\nwhere some prepositions, adverbs or some specific endings can provide a lot of\\ninformation about the structure of a complex sentence. Such characteristics can\\nbe found in other languages, too, such as French. Using a special grammar, we\\ndeveloped a system (DIASEXP) that can perform a dialogue in natural language\\nwith assertive and interogative sentences about a \"story\" (a set of sentences\\ndescribing some events from the real life).\\n',\n",
              " '  The present paper explores various arguments in favour of making the Text\\nEncoding Initia-tive (TEI) guidelines an appropriate serialisation for ISO\\nstandard 24613:2008 (LMF, Lexi-cal Mark-up Framework) . It also identifies the\\nissues that would have to be resolved in order to reach an appropriate\\nimplementation of these ideas, in particular in terms of infor-mational\\ncoverage. We show how the customisation facilities offered by the TEI\\nguidelines can provide an adequate background, not only to cover missing\\ncomponents within the current Dictionary chapter of the TEI guidelines, but\\nalso to allow specific lexical projects to deal with local constraints. We\\nexpect this proposal to be a basis for a future ISO project in the context of\\nthe on going revision of LMF.\\n',\n",
              " \"  When learning grammar of the new language, a teacher should routinely check\\nstudent's exercises for grammatical correctness. The paper describes a method\\nof automatically detecting and reporting grammar mistakes, regarding an order\\nof tokens in the response. It could report extra tokens, missing tokens and\\nmisplaced tokens. The method is useful when teaching language, where order of\\ntokens is important, which includes most formal languages and some natural ones\\n(like English). The method was implemented in a question type plug-in\\nCorrectWriting for the widely used learning manage system Moodle.\\n\",\n",
              " '  Deep Learning models enjoy considerable success in Natural Language\\nProcessing. While deep architectures produce useful representations that lead\\nto improvements in various tasks, they are often difficult to interpret. This\\nmakes the analysis of learned structures particularly difficult. In this paper,\\nwe rely on empirical tests to see whether a particular structure makes sense.\\nWe present an analysis of the Semi-Supervised Recursive Autoencoder, a\\nwell-known model that produces structural representations of text. We show that\\nfor certain tasks, the structure of the autoencoder can be significantly\\nreduced without loss of classification accuracy and we evaluate the produced\\nstructures using human judgment.\\n',\n",
              " '  Online content analysis employs algorithmic methods to identify entities in\\nunstructured text. Both machine learning and knowledge-base approaches lie at\\nthe foundation of contemporary named entities extraction systems. However, the\\nprogress in deploying these approaches on web-scale has been been hampered by\\nthe computational cost of NLP over massive text corpora. We present SpeedRead\\n(SR), a named entity recognition pipeline that runs at least 10 times faster\\nthan Stanford NLP pipeline. This pipeline consists of a high performance Penn\\nTreebank- compliant tokenizer, close to state-of-art part-of-speech (POS)\\ntagger and knowledge-based named entity recognizer.\\n',\n",
              " '  Sentiment analysis predicts the presence of positive or negative emotions in\\na text document. In this paper, we consider higher dimensional extensions of\\nthe sentiment concept, which represent a richer set of human emotions. Our\\napproach goes beyond previous work in that our model contains a continuous\\nmanifold rather than a finite set of human emotions. We investigate the\\nresulting model, compare it to psychological observations, and explore its\\npredictive capabilities.\\n',\n",
              " \"  The goal of this research was to find a way to extend the capabilities of\\ncomputers through the processing of language in a more human way, and present\\napplications which demonstrate the power of this method. This research presents\\na novel approach, Rhetorical Analysis, to solving problems in Natural Language\\nProcessing (NLP). The main benefit of Rhetorical Analysis, as opposed to\\nprevious approaches, is that it does not require the accumulation of large sets\\nof training data, but can be used to solve a multitude of problems within the\\nfield of NLP. The NLP problems investigated with Rhetorical Analysis were the\\nAuthor Identification problem - predicting the author of a piece of text based\\non its rhetorical strategies, Election Prediction - predicting the winner of a\\npresidential candidate's re-election campaign based on rhetorical strategies\\nwithin that president's inaugural address, Natural Language Generation - having\\na computer produce text containing rhetorical strategies, and Document\\nSummarization. The results of this research indicate that an Author\\nIdentification system based on Rhetorical Analysis could predict the correct\\nauthor 100% of the time, that a re-election predictor based on Rhetorical\\nAnalysis could predict the correct winner of a re-election campaign 55% of the\\ntime, that a Natural Language Generation system based on Rhetorical Analysis\\ncould output text with up to 87.3% similarity to Shakespeare in style, and that\\na Document Summarization system based on Rhetorical Analysis could extract\\nhighly relevant sentences. Overall, this study demonstrated that Rhetorical\\nAnalysis could be a useful approach to solving problems in NLP.\\n\",\n",
              " \"  A neural probabilistic language model (NPLM) provides an idea to achieve the\\nbetter perplexity than n-gram language model and their smoothed language\\nmodels. This paper investigates application area in bilingual NLP, specifically\\nStatistical Machine Translation (SMT). We focus on the perspectives that NPLM\\nhas potential to open the possibility to complement potentially `huge'\\nmonolingual resources into the `resource-constraint' bilingual resources. We\\nintroduce an ngram-HMM language model as NPLM using the non-parametric Bayesian\\nconstruction. In order to facilitate the application to various tasks, we\\npropose the joint space model of ngram-HMM language model. We show an\\nexperiment of system combination in the area of SMT. One discovery was that our\\ntreatment of noise improved the results 0.20 BLEU points if NPLM is trained in\\nrelatively small corpus, in our case 500,000 sentence pairs, which is often the\\ncase due to the long training time of NPLM.\\n\",\n",
              " '  Knowledge bases provide applications with the benefit of easily accessible,\\nsystematic relational knowledge but often suffer in practice from their\\nincompleteness and lack of knowledge of new entities and relations. Much work\\nhas focused on building or extending them by finding patterns in large\\nunannotated text corpora. In contrast, here we mainly aim to complete a\\nknowledge base by predicting additional true relationships between entities,\\nbased on generalizations that can be discerned in the given knowledgebase. We\\nintroduce a neural tensor network (NTN) model which predicts new relationship\\nentries that can be added to the database. This model can be improved by\\ninitializing entity representations with word vectors learned in an\\nunsupervised fashion from text, and when doing this, existing relations can\\neven be queried for entities that were not present in the database. Our model\\ngeneralizes and outperforms existing models for this problem, and can classify\\nunseen relationships in WordNet with an accuracy of 75.8%.\\n',\n",
              " '  A key characteristic of work on deep learning and neural networks in general\\nis that it relies on representations of the input that support generalization,\\nrobust inference, domain adaptation and other desirable functionalities. Much\\nrecent progress in the field has focused on efficient and effective methods for\\ncomputing representations. In this paper, we propose an alternative method that\\nis more efficient than prior work and produces representations that have a\\nproperty we call focality -- a property we hypothesize to be important for\\nneural network representations. The method consists of a simple application of\\ntwo consecutive SVDs and is inspired by Anandkumar (2012).\\n',\n",
              " '  We propose two novel model architectures for computing continuous vector\\nrepresentations of words from very large data sets. The quality of these\\nrepresentations is measured in a word similarity task, and the results are\\ncompared to the previously best performing techniques based on different types\\nof neural networks. We observe large improvements in accuracy at much lower\\ncomputational cost, i.e. it takes less than a day to learn high quality word\\nvectors from a 1.6 billion words data set. Furthermore, we show that these\\nvectors provide state-of-the-art performance on our test set for measuring\\nsyntactic and semantic word similarities.\\n',\n",
              " '  Children learn their native language by exposure to their linguistic and\\ncommunicative environment, but apparently without requiring that their mistakes\\nare corrected. Such learning from positive evidence has been viewed as raising\\nlogical problems for language acquisition. In particular, without correction,\\nhow is the child to recover from conjecturing an over-general grammar, which\\nwill be consistent with any sentence that the child hears? There have been many\\nproposals concerning how this logical problem can be dissolved. Here, we review\\nrecent formal results showing that the learner has sufficient data to learn\\nsuccessfully from positive evidence, if it favours the simplest encoding of the\\nlinguistic input. Results include the ability to learn a linguistic prediction,\\ngrammaticality judgements, language production, and form-meaning mappings. The\\nsimplicity approach can also be scaled-down to analyse the ability to learn a\\nspecific linguistic constructions, and is amenable to empirical test as a\\nframework for describing human language acquisition.\\n',\n",
              " '  The increasing volume of short texts generated on social media sites, such as\\nTwitter or Facebook, creates a great demand for effective and efficient topic\\nmodeling approaches. While latent Dirichlet allocation (LDA) can be applied, it\\nis not optimal due to its weakness in handling short texts with fast-changing\\ntopics and scalability concerns. In this paper, we propose a transfer learning\\napproach that utilizes abundant labeled documents from other domains (such as\\nYahoo! News or Wikipedia) to improve topic modeling, with better model fitting\\nand result interpretation. Specifically, we develop Transfer Hierarchical LDA\\n(thLDA) model, which incorporates the label information from other domains via\\ninformative priors. In addition, we develop a parallel implementation of our\\nmodel for large-scale applications. We demonstrate the effectiveness of our\\nthLDA model on both a microblogging dataset and standard text collections\\nincluding AP and RCV1 datasets.\\n',\n",
              " '  We present a model for compositional distributional semantics related to the\\nframework of Coecke et al. (2010), and emulating formal semantics by\\nrepresenting functions as tensors and arguments as vectors. We introduce a new\\nlearning method for tensors, generalising the approach of Baroni and Zamparelli\\n(2010). We evaluate it on two benchmark data sets, and find it to outperform\\nexisting leading methods. We argue in our analysis that the nature of this\\nlearning method also renders it suitable for solving more subtle problems\\ncompositional distributional models might face.\\n',\n",
              " '  This paper presents a distributed platform for Natural Language Processing\\ncalled PyPLN. PyPLN leverages a vast array of NLP and text processing open\\nsource tools, managing the distribution of the workload on a variety of\\nconfigurations: from a single server to a cluster of linux servers. PyPLN is\\ndeveloped using Python 2.7.3 but makes it very easy to incorporate other\\nsoftwares for specific tasks as long as a linux version is available. PyPLN\\nfacilitates analyses both at document and corpus level, simplifying management\\nand publication of corpora and analytical results through an easy to use web\\ninterface. In the current (beta) release, it supports English and Portuguese\\nlanguages with support to other languages planned for future releases. To\\nsupport the Portuguese language PyPLN uses the PALAVRAS parser\\\\citep{Bick2000}.\\nCurrently PyPLN offers the following features: Text extraction with encoding\\nnormalization (to UTF-8), part-of-speech tagging, token frequency, semantic\\nannotation, n-gram extraction, word and sentence repertoire, and full-text\\nsearch across corpora. The platform is licensed as GPL-v3.\\n',\n",
              " '  The paper revives an older approach to acoustic modeling that borrows from\\nn-gram language modeling in an attempt to scale up both the amount of training\\ndata and model size (as measured by the number of parameters in the model), to\\napproximately 100 times larger than current sizes used in automatic speech\\nrecognition. In such a data-rich setting, we can expand the phonetic context\\nsignificantly beyond triphones, as well as increase the number of Gaussian\\nmixture components for the context-dependent states that allow it. We have\\nexperimented with contexts that span seven or more context-independent phones,\\nand up to 620 mixture components per state. Dealing with unseen phonetic\\ncontexts is accomplished using the familiar back-off technique used in language\\nmodeling due to implementation simplicity. The back-off acoustic model is\\nestimated, stored and served using MapReduce distributed computing\\ninfrastructure.\\n  Speech recognition experiments are carried out in an N-best list rescoring\\nframework for Google Voice Search. Training big models on large amounts of data\\nproves to be an effective way to increase the accuracy of a state-of-the-art\\nautomatic speech recognition system. We use 87,000 hours of training data\\n(speech along with transcription) obtained by filtering utterances in Voice\\nSearch logs on automatic speech recognition confidence. Models ranging in size\\nbetween 20--40 million Gaussians are estimated using maximum likelihood\\ntraining. They achieve relative reductions in word-error-rate of 11% and 6%\\nwhen combined with first-pass models trained using maximum likelihood, and\\nboosted maximum mutual information, respectively. Increasing the context size\\nbeyond five phones (quinphones) does not help.\\n',\n",
              " '  When developing a conversational agent, there is often an urgent need to have\\na prototype available in order to test the application with real users. A\\nWizard of Oz is a possibility, but sometimes the agent should be simply\\ndeployed in the environment where it will be used. Here, the agent should be\\nable to capture as many interactions as possible and to understand how people\\nreact to failure. In this paper, we focus on the rapid development of a natural\\nlanguage understanding module by non experts. Our approach follows the learning\\nparadigm and sees the process of understanding natural language as a\\nclassification problem. We test our module with a conversational agent that\\nanswers questions in the art domain. Moreover, we show how our approach can be\\nused by a natural language interface to a cinema database.\\n',\n",
              " '  The variation of word meaning according to the context leads us to enrich the\\ntype system of our syntactical and semantic analyser of French based on\\ncategorial grammars and Montague semantics (or lambda-DRT). The main advantage\\nof a deep semantic analyse is too represent meaning by logical formulae that\\ncan be easily used e.g. for inferences. Determiners and quantifiers play a\\nfundamental role in the construction of those formulae. But in our rich type\\nsystem the usual semantic terms do not work. We propose a solution ins- pired\\nby the tau and epsilon operators of Hilbert, kinds of generic elements and\\nchoice functions. This approach unifies the treatment of the different determi-\\nners and quantifiers as well as the dynamic binding of pronouns. Above all,\\nthis fully computational view fits in well within the wide coverage parser\\nGrail, both from a theoretical and a practical viewpoint.\\n',\n",
              " '  The Lexical Access Problem consists of determining the intended sequence of\\nwords corresponding to an input sequence of phonemes (basic speech sounds) that\\ncome from a low-level phoneme recognizer. In this paper we present an\\ninformation-theoretic approach based on the Minimum Message Length Criterion\\nfor solving the Lexical Access Problem. We model sentences using phoneme\\nrealizations seen in training, and word and part-of-speech information obtained\\nfrom text corpora. We show results on multiple-speaker, continuous, read speech\\nand discuss a heuristic using equivalence classes of similar sounding words\\nwhich speeds up the recognition process without significant deterioration in\\nrecognition accuracy.\\n',\n",
              " '  This paper describes our submission to the First Workshop on Reordering for\\nStatistical Machine Translation. We have decided to build a reordering system\\nbased on tree-to-string model, using only publicly available tools to\\naccomplish this task. With the provided training data we have built a\\ntranslation model using Moses toolkit, and then we applied a chart decoder,\\nimplemented in Moses, to reorder the sentences. Even though our submission only\\ncovered English-Farsi language pair, we believe that the approach itself should\\nwork regardless of the choice of the languages, so we have also carried out the\\nexperiments for English-Italian and English-Urdu. For these language pairs we\\nhave noticed a significant improvement over the baseline in BLEU, Kendall-Tau\\nand Hamming metrics. A detailed description is given, so that everyone can\\nreproduce our results. Also, some possible directions for further improvements\\nare discussed.\\n',\n",
              " '  Cross-Language Information Retrieval (CLIR) and machine translation (MT)\\nresources, such as dictionaries and parallel corpora, are scarce and hard to\\ncome by for special domains. Besides, these resources are just limited to a few\\nlanguages, such as English, French, and Spanish and so on. So, obtaining\\ncomparable corpora automatically for such domains could be an answer to this\\nproblem effectively. Comparable corpora, that the subcorpora are not\\ntranslations of each other, can be easily obtained from web. Therefore,\\nbuilding and using comparable corpora is often a more feasible option in\\nmultilingual information processing. Comparability metrics is one of key issues\\nin the field of building and using comparable corpus. Currently, there is no\\nwidely accepted definition or metrics method of corpus comparability. In fact,\\nDifferent definitions or metrics methods of comparability might be given to\\nsuit various tasks about natural language processing. A new comparability,\\nnamely, termhood-based metrics, oriented to the task of bilingual terminology\\nextraction, is proposed in this paper. In this method, words are ranked by\\ntermhood not frequency, and then the cosine similarities, calculated based on\\nthe ranking lists of word termhood, is used as comparability. Experiments\\nresults show that termhood-based metrics performs better than traditional\\nfrequency-based metrics.\\n',\n",
              " '  Purpose: Terminology is the set of technical words or expressions used in\\nspecific contexts, which denotes the core concept in a formal discipline and is\\nusually applied in the fields of machine translation, information retrieval,\\ninformation extraction and text categorization, etc. Bilingual terminology\\nextraction plays an important role in the application of bilingual dictionary\\ncompilation, bilingual Ontology construction, machine translation and\\ncross-language information retrieval etc. This paper addresses the issues of\\nmonolingual terminology extraction and bilingual term alignment based on\\nmulti-level termhood.\\n  Design/methodology/approach: A method based on multi-level termhood is\\nproposed. The new method computes the termhood of the terminology candidate as\\nwell as the sentence that includes the terminology by the comparison of the\\ncorpus. Since terminologies and general words usually have differently\\ndistribution in the corpus, termhood can also be used to constrain and enhance\\nthe performance of term alignment when aligning bilingual terms on the parallel\\ncorpus. In this paper, bilingual term alignment based on termhood constraints\\nis presented.\\n  Findings: Experiment results show multi-level termhood can get better\\nperformance than existing method for terminology extraction. If termhood is\\nused as constrain factor, the performance of bilingual term alignment can be\\nimproved.\\n',\n",
              " '  A compactified horizontal visibility graph for the language network is\\nproposed. It was found that the networks constructed in such way are scale\\nfree, and have a property that among the nodes with largest degrees there are\\nwords that determine not only a text structure communication, but also its\\ninformational structure.\\n',\n",
              " '  Regulations in the Building Industry are becoming increasingly complex and\\ninvolve more than one technical area. They cover products, components and\\nproject implementation. They also play an important role to ensure the quality\\nof a building, and to minimize its environmental impact. In this paper, we are\\nparticularly interested in the modeling of the regulatory constraints derived\\nfrom the Technical Guides issued by CSTB and used to validate Technical\\nAssessments. We first describe our approach for modeling regulatory constraints\\nin the SBVR language, and formalizing them in the SPARQL language. Second, we\\ndescribe how we model the processes of compliance checking described in the\\nCSTB Technical Guides. Third, we show how we implement these processes to\\nassist industrials in drafting Technical Documents in order to acquire a\\nTechnical Assessment; a compliance report is automatically generated to explain\\nthe compliance or noncompliance of this Technical Documents.\\n',\n",
              " '  In natural-language discourse, related events tend to appear near each other\\nto describe a larger scenario. Such structures can be formalized by the notion\\nof a frame (a.k.a. template), which comprises a set of related events and\\nprototypical participants and event transitions. Identifying frames is a\\nprerequisite for information extraction and natural language generation, and is\\nusually done manually. Methods for inducing frames have been proposed recently,\\nbut they typically use ad hoc procedures and are difficult to diagnose or\\nextend. In this paper, we propose the first probabilistic approach to frame\\ninduction, which incorporates frames, events, participants as latent topics and\\nlearns those frame and event transitions that best explain the text. The number\\nof frames is inferred by a novel application of a split-merge method from\\nsyntactic parsing. In end-to-end evaluations from text to induced frames and\\nextracted facts, our method produced state-of-the-art results while\\nsubstantially reducing engineering effort.\\n',\n",
              " '  In the first part of this article, we explore the background of\\ncomputer-assisted learning from its beginnings in the early XIXth century and\\nthe first teaching machines, founded on theories of learning, at the start of\\nthe XXth century. With the arrival of the computer, it became possible to offer\\nlanguage learners different types of language activities such as comprehension\\ntasks, simulations, etc. However, these have limits that cannot be overcome\\nwithout some contribution from the field of natural language processing (NLP).\\nIn what follows, we examine the challenges faced and the issues raised by\\nintegrating NLP into CALL. We hope to demonstrate that the key to success in\\nintegrating NLP into CALL is to be found in multidisciplinary work between\\ncomputer experts, linguists, language teachers, didacticians and NLP\\nspecialists.\\n',\n",
              " '  In this paper, we propose an approach for Relationship Extraction (RE) based\\non labeled graph kernels. The kernel we propose is a particularization of a\\nrandom walk kernel that exploits two properties previously studied in the RE\\nliterature: (i) the words between the candidate entities or connecting them in\\na syntactic representation are particularly likely to carry information\\nregarding the relationship; and (ii) combining information from distinct\\nsources in a kernel may help the RE system make better decisions. We performed\\nexperiments on a dataset of protein-protein interactions and the results show\\nthat our approach obtains effectiveness values that are comparable with the\\nstate-of-the art kernel methods. Moreover, our approach is able to outperform\\nthe state-of-the-art kernels when combined with other kernel methods.\\n',\n",
              " '  This project is a part of nature language processing and its aims to develop\\na system of recognition inference text-appointed TIMINF. This type of system\\ncan detect, given two portions of text, if a text is semantically deducted from\\nthe other. We focused on making the inference time in this type of system. For\\nthat we have built and analyzed a body built from questions collected through\\nthe web. This study has enabled us to classify different types of times\\ninferences and for designing the architecture of TIMINF which seeks to\\nintegrate a module inference time in a detection system inference text. We also\\nassess the performance of sorties TIMINF system on a test corpus with the same\\nstrategy adopted in the challenge RTE.\\n',\n",
              " '  Developing Question Answering systems has been one of the important research\\nissues because it requires insights from a variety of\\ndisciplines,including,Artificial Intelligence,Information Retrieval,\\nInformation Extraction,Natural Language Processing, and Psychology.In this\\npaper we realize a formal model for a lightweight semantic based open domain\\nyes/no Arabic question answering system based on paragraph retrieval with\\nvariable length. We propose a constrained semantic representation. Using an\\nexplicit unification framework based on semantic similarities and query\\nexpansion synonyms and antonyms.This frequently improves the precision of the\\nsystem. Employing the passage retrieval system achieves a better precision by\\nretrieving more paragraphs that contain relevant answers to the question; It\\nsignificantly reduces the amount of text to be processed by the system.\\n',\n",
              " \"  So far, a very large amount of work in Natural Language Processing (NLP) rely\\non trees as the core mathematical structure to represent linguistic\\ninformations (e.g. in Chomsky's work). However, some linguistic phenomena do\\nnot cope properly with trees. In a former paper, we showed the benefit of\\nencoding linguistic structures by graphs and of using graph rewriting rules to\\ncompute on those structures. Justified by some linguistic considerations, graph\\nrewriting is characterized by two features: first, there is no node creation\\nalong computations and second, there are non-local edge modifications. Under\\nthese hypotheses, we show that uniform termination is undecidable and that\\nnon-uniform termination is decidable. We describe two termination techniques\\nbased on weights and we give complexity bound on the derivation length for\\nthese rewriting system.\\n\",\n",
              " '  Probabilistic approaches to part-of-speech tagging rely primarily on\\nwhole-word statistics about word/tag combinations as well as contextual\\ninformation. But experience shows about 4 per cent of tokens encountered in\\ntest sets are unknown even when the training set is as large as a million\\nwords. Unseen words are tagged using secondary strategies that exploit word\\nfeatures such as endings, capitalizations and punctuation marks. In this work,\\nword-ending statistics are primary and whole-word statistics are secondary.\\nFirst, a tagger was trained and tested on word endings only. Subsequent\\nexperiments added back whole-word statistics for the words occurring most\\nfrequently in the training set. As grew larger, performance was expected to\\nimprove, in the limit performing the same as word-based taggers. Surprisingly,\\nthe ending-based tagger initially performed nearly as well as the word-based\\ntagger; in the best case, its performance significantly exceeded that of the\\nword-based tagger. Lastly, and unexpectedly, an effect of negative returns was\\nobserved - as grew larger, performance generally improved and then declined. By\\nvarying factors such as ending length and tag-list strategy, we achieved a\\nsuccess rate of 97.5 percent.\\n',\n",
              " '  We describe our language-independent unsupervised word sense induction\\nsystem. This system only uses topic features to cluster different word senses\\nin their global context topic space. Using unlabeled data, this system trains a\\nlatent Dirichlet allocation (LDA) topic model then uses it to infer the topics\\ndistribution of the test instances. By clustering these topics distributions in\\ntheir topic space we cluster them into different senses. Our hypothesis is that\\ncloseness in topic space reflects similarity between different word senses.\\nThis system participated in SemEval-2 word sense induction and disambiguation\\ntask and achieved the second highest V-measure score among all other systems.\\n',\n",
              " '  There are different ways to define similarity for grouping similar texts into\\nclusters, as the concept of similarity may depend on the purpose of the task.\\nFor instance, in topic extraction similar texts mean those within the same\\nsemantic field, whereas in author recognition stylistic features should be\\nconsidered. In this study, we introduce ways to classify texts employing\\nconcepts of complex networks, which may be able to capture syntactic, semantic\\nand even pragmatic features. The interplay between the various metrics of the\\ncomplex networks is analyzed with three applications, namely identification of\\nmachine translation (MT) systems, evaluation of quality of machine translated\\ntexts and authorship recognition. We shall show that topological features of\\nthe networks representing texts can enhance the ability to identify MT systems\\nin particular cases. For evaluating the quality of MT texts, on the other hand,\\nhigh correlation was obtained with methods capable of capturing the semantics.\\nThis was expected because the golden standards used are themselves based on\\nword co-occurrence. Notwithstanding, the Katz similarity, which involves\\nsemantic and structure in the comparison of texts, achieved the highest\\ncorrelation with the NIST measurement, indicating that in some cases the\\ncombination of both approaches can improve the ability to quantify quality in\\nMT. In authorship recognition, again the topological features were relevant in\\nsome contexts, though for the books and authors analyzed good results were\\nobtained with semantic features as well. Because hybrid approaches encompassing\\nsemantic and topological features have not been extensively used, we believe\\nthat the methodology proposed here may be useful to enhance text classification\\nconsiderably, as it combines well-established strategies.\\n',\n",
              " '  The classification of opinion texts in positive and negative is becoming a\\nsubject of great interest in sentiment analysis. The existence of many labeled\\nopinions motivates the use of statistical and machine-learning methods.\\nFirst-order statistics have proven to be very limited in this field. The Opinum\\napproach is based on the order of the words without using any syntactic and\\nsemantic information. It consists of building one probabilistic model for the\\npositive and another one for the negative opinions. Then the test opinions are\\ncompared to both models and a decision and confidence measure are calculated.\\nIn order to reduce the complexity of the training corpus we first lemmatize the\\ntexts and we replace most named-entities with wildcards. Opinum presents an\\naccuracy above 81% for Spanish opinions in the financial products domain. In\\nthis work we discuss which are the most important factors that have impact on\\nthe classification performance.\\n',\n",
              " '  Now a days, the text document is spontaneously increasing over the internet,\\ne-mail and web pages and they are stored in the electronic database format. To\\narrange and browse the document it becomes difficult. To overcome such problem\\nthe document preprocessing, term selection, attribute reduction and maintaining\\nthe relationship between the important terms using background knowledge,\\nWordNet, becomes an important parameters in data mining. In these paper the\\ndifferent stages are formed, firstly the document preprocessing is done by\\nremoving stop words, stemming is performed using porter stemmer algorithm, word\\nnet thesaurus is applied for maintaining relationship between the important\\nterms, global unique words, and frequent word sets get generated, Secondly,\\ndata matrix is formed, and thirdly terms are extracted from the documents by\\nusing term selection approaches tf-idf, tf-df, and tf2 based on their minimum\\nthreshold value. Further each and every document terms gets preprocessed, where\\nthe frequency of each term within the document is counted for representation.\\nThe purpose of this approach is to reduce the attributes and find the effective\\nterm selection method using WordNet for better clustering accuracy. Experiments\\nare evaluated on Reuters Transcription Subsets, wheat, trade, money grain, and\\nship, Reuters 21578, Classic 30, 20 News group (atheism), 20 News group\\n(Hardware), 20 News group (Computer Graphics) etc.\\n',\n",
              " '  We present the results of research with the goal of automatically creating a\\nmultilingual thesaurus based on the freely available resources of Wikipedia and\\nWordNet. Our goal is to increase resources for natural language processing\\ntasks such as machine translation targeting the Japanese-Spanish language pair.\\nGiven the scarcity of resources, we use existing English resources as a pivot\\nfor creating a trilingual Japanese-Spanish-English thesaurus. Our approach\\nconsists of extracting the translation tuples from Wikipedia, disambiguating\\nthem by mapping them to WordNet word senses. We present results comparing two\\nmethods of disambiguation, the first using VSM on Wikipedia article texts and\\nWordNet definitions, and the second using categorical information extracted\\nfrom Wikipedia, We find that mixing the two methods produces favorable results.\\nUsing the proposed method, we have constructed a multilingual\\nSpanish-Japanese-English thesaurus consisting of 25,375 entries. The same\\nmethod can be applied to any pair of languages that are linked to English in\\nWikipedia.\\n',\n",
              " '  This article reports on the results of the research done towards the fully\\nautomatically merging of lexical resources. Our main goal is to show the\\ngenerality of the proposed approach, which have been previously applied to\\nmerge Spanish Subcategorization Frames lexica. In this work we extend and apply\\nthe same technique to perform the merging of morphosyntactic lexica encoded in\\nLMF. The experiments showed that the technique is general enough to obtain good\\nresults in these two different tasks which is an important step towards\\nperforming the merging of lexical resources fully automatically.\\n',\n",
              " '  The work we present here addresses cue-based noun classification in English\\nand Spanish. Its main objective is to automatically acquire lexical semantic\\ninformation by classifying nouns into previously known noun lexical classes.\\nThis is achieved by using particular aspects of linguistic contexts as cues\\nthat identify a specific lexical class. Here we concentrate on the task of\\nidentifying such cues and the theoretical background that allows for an\\nassessment of the complexity of the task. The results show that, despite of the\\na-priori complexity of the task, cue-based classification is a useful tool in\\nthe automatic acquisition of lexical semantic classes.\\n',\n",
              " '  Subjective language detection is one of the most important challenges in\\nSentiment Analysis. Because of the weight and frequency in opinionated texts,\\nadjectives are considered a key piece in the opinion extraction process. These\\nsubjective units are more and more frequently collected in polarity lexicons in\\nwhich they appear annotated with their prior polarity. However, at the moment,\\nany polarity lexicon takes into account prior polarity variations across\\ndomains. This paper proves that a majority of adjectives change their prior\\npolarity value depending on the domain. We propose a distinction between domain\\ndependent and domain independent adjectives. Moreover, our analysis led us to\\npropose a further classification related to subjectivity degree: constant,\\nmixed and highly subjective adjectives. Following this classification, polarity\\nvalues will be a better support for Sentiment Analysis.\\n',\n",
              " '  The objective of the PANACEA ICT-2007.2.2 EU project is to build a platform\\nthat automates the stages involved in the acquisition, production, updating and\\nmaintenance of the large language resources required by, among others, MT\\nsystems. The development of a Corpus Acquisition Component (CAC) for extracting\\nmonolingual and bilingual data from the web is one of the most innovative\\nbuilding blocks of PANACEA. The CAC, which is the first stage in the PANACEA\\npipeline for building Language Resources, adopts an efficient and distributed\\nmethodology to crawl for web documents with rich textual content in specific\\nlanguages and predefined domains. The CAC includes modules that can acquire\\nparallel data from sites with in-domain content available in more than one\\nlanguage. In order to extrinsically evaluate the CAC methodology, we have\\nconducted several experiments that used crawled parallel corpora for the\\nidentification and extraction of parallel sentences using sentence alignment.\\nThe corpora were then successfully used for domain adaptation of Machine\\nTranslation Systems.\\n',\n",
              " '  In this work we present the results of our experimental work on the\\ndevelop-ment of lexical class-based lexica by automatic means. The objective is\\nto as-sess the use of linguistic lexical-class based information as a feature\\nselection methodology for the use of classifiers in quick lexical development.\\nThe results show that the approach can help in re-ducing the human effort\\nrequired in the development of language resources sig-nificantly.\\n',\n",
              " '  Acquiring lexical information is a complex problem, typically approached by\\nrelying on a number of contexts to contribute information for classification.\\nOne of the first issues to address in this domain is the determination of such\\ncontexts. The work presented here proposes the use of automatically obtained\\nFORMAL role descriptors as features used to draw nouns from the same lexical\\nsemantic class together in an unsupervised clustering task. We have dealt with\\nthree lexical semantic classes (HUMAN, LOCATION and EVENT) in English. The\\nresults obtained show that it is possible to discriminate between elements from\\ndifferent lexical semantic classes using only FORMAL role information, hence\\nvalidating our initial hypothesis. Also, iterating our method accurately\\naccounts for fine-grained distinctions within lexical classes, namely\\ndistinctions involving ambiguous expressions. Moreover, a filtering and\\nbootstrapping strategy employed in extracting FORMAL role descriptors proved to\\nminimize effects of sparse data and noise in our task.\\n',\n",
              " '  This article presents a probabilistic generative model for text based on\\nsemantic topics and syntactic classes called Part-of-Speech LDA (POSLDA).\\nPOSLDA simultaneously uncovers short-range syntactic patterns (syntax) and\\nlong-range semantic patterns (topics) that exist in document collections. This\\nresults in word distributions that are specific to both topics (sports,\\neducation, ...) and parts-of-speech (nouns, verbs, ...). For example,\\nmultinomial distributions over words are uncovered that can be understood as\\n\"nouns about weather\" or \"verbs about law\". We describe the model and an\\napproximate inference algorithm and then demonstrate the quality of the learned\\ntopics both qualitatively and quantitatively. Then, we discuss an NLP\\napplication where the output of POSLDA can lead to strong improvements in\\nquality: unsupervised part-of-speech tagging. We describe algorithms for this\\ntask that make use of POSLDA-learned distributions that result in improved\\nperformance beyond the state of the art.\\n',\n",
              " \"  The role of types in categorical models of meaning is investigated. A general\\nscheme for how typed models of meaning may be used to compare sentences,\\nregardless of their grammatical structure is described, and a toy example is\\nused as an illustration. Taking as a starting point the question of whether the\\nevaluation of such a type system 'loses information', we consider the\\nparametrized typing associated with connectives from this viewpoint.\\n  The answer to this question implies that, within full categorical models of\\nmeaning, the objects associated with types must exhibit a simple but subtle\\ncategorical property known as self-similarity. We investigate the category\\ntheory behind this, with explicit reference to typed systems, and their\\nmonoidal closed structure. We then demonstrate close connections between such\\nself-similar structures and dagger Frobenius algebras. In particular, we\\ndemonstrate that the categorical structures implied by the polymorphically\\ntyped connectives give rise to a (lax unitless) form of the special forms of\\nFrobenius algebras known as classical structures, used heavily in abstract\\ncategorical approaches to quantum mechanics.\\n\",\n",
              " '  Achieving homophily, or association based on similarity, between a human user\\nand a robot holds a promise of improved perception and task performance.\\nHowever, no previous studies that address homophily via ethnic similarity with\\nrobots exist. In this paper, we discuss the difficulties of evoking ethnic cues\\nin a robot, as opposed to a virtual agent, and an approach to overcome those\\ndifficulties based on using ethnically salient behaviors. We outline our\\nmethodology for selecting and evaluating such behaviors, and culminate with a\\nstudy that evaluates our hypotheses of the possibility of ethnic attribution of\\na robot character through verbal and nonverbal behaviors and of achieving the\\nhomophily effect.\\n',\n",
              " '  Achieving and maintaining the performance of ubiquitous (Automatic Speech\\nRecognition) ASR system is a real challenge. The main objective of this work is\\nto develop a method that will improve and show the consistency in performance\\nof ubiquitous ASR system for real world noisy environment. An adaptive\\nmethodology has been developed to achieve an objective with the help of\\nimplementing followings, -Cleaning speech signal as much as possible while\\npreserving originality / intangibility using various modified filters and\\nenhancement techniques. -Extracting features from speech signals using various\\nsizes of parameter. -Train the system for ubiquitous environment using\\nmulti-environmental adaptation training methods. -Optimize the word recognition\\nrate with appropriate variable size of parameters using fuzzy technique. The\\nconsistency in performance is tested using standard noise databases as well as\\nin real world environment. A good improvement is noticed. This work will be\\nhelpful to give discriminative training of ubiquitous ASR system for better\\nHuman Computer Interaction (HCI) using Speech User Interface (SUI).\\n',\n",
              " '  We describe a semantic wiki system with an underlying controlled natural\\nlanguage grammar implemented in Grammatical Framework (GF). The grammar\\nrestricts the wiki content to a well-defined subset of Attempto Controlled\\nEnglish (ACE), and facilitates a precise bidirectional automatic translation\\nbetween ACE and language fragments of a number of other natural languages,\\nmaking the wiki content accessible multilingually. Additionally, our approach\\nallows for automatic translation into the Web Ontology Language (OWL), which\\nenables automatic reasoning over the wiki content. The developed wiki\\nenvironment thus allows users to build, query and view OWL knowledge bases via\\na user-friendly multilingual natural language interface. As a further feature,\\nthe underlying multilingual grammar is integrated into the wiki and can be\\ncollaboratively edited to extend the vocabulary of the wiki or even customize\\nits sentence structures. This work demonstrates the combination of the existing\\ntechnologies of Attempto Controlled English and Grammatical Framework, and is\\nimplemented as an extension of the existing semantic wiki engine AceWiki.\\n',\n",
              " \"  Human language is a combination of elemental languages/domains/styles that\\nchange across and sometimes within discourses. Language models, which play a\\ncrucial role in speech recognizers and machine translation systems, are\\nparticularly sensitive to such changes, unless some form of adaptation takes\\nplace. One approach to speech language model adaptation is self-training, in\\nwhich a language model's parameters are tuned based on automatically\\ntranscribed audio. However, transcription errors can misguide self-training,\\nparticularly in challenging settings such as conversational speech. In this\\nwork, we propose a model that considers the confusions (errors) of the ASR\\nchannel. By modeling the likely confusions in the ASR output instead of using\\njust the 1-best, we improve self-training efficacy by obtaining a more reliable\\nreference transcription estimate. We demonstrate improved topic-based language\\nmodeling adaptation results over both 1-best and lattice self-training using\\nour ASR channel confusion estimates on telephone conversations.\\n\",\n",
              " '  From the existing research it has been observed that many techniques and\\nmethodologies are available for performing every step of Automatic Speech\\nRecognition (ASR) system, but the performance (Minimization of Word Error\\nRecognition-WER and Maximization of Word Accuracy Rate- WAR) of the methodology\\nis not dependent on the only technique applied in that method. The research\\nwork indicates that, performance mainly depends on the category of the noise,\\nthe level of the noise and the variable size of the window, frame, frame\\noverlap etc is considered in the existing methods. The main aim of the work\\npresented in this paper is to use variable size of parameters like window size,\\nframe size and frame overlap percentage to observe the performance of\\nalgorithms for various categories of noise with different levels and also train\\nthe system for all size of parameters and category of real world noisy\\nenvironment to improve the performance of the speech recognition system. This\\npaper presents the results of Signal-to-Noise Ratio (SNR) and Accuracy test by\\napplying variable size of parameters. It is observed that, it is really very\\nhard to evaluate test results and decide parameter size for ASR performance\\nimprovement for its resultant optimization. Hence, this study further suggests\\nthe feasible and optimum parameter size using Fuzzy Inference System (FIS) for\\nenhancing resultant accuracy in adverse real world noisy environmental\\nconditions. This work will be helpful to give discriminative training of\\nubiquitous ASR system for better Human Computer Interaction (HCI).\\n',\n",
              " '  The main motivation for Automatic Speech Recognition (ASR) is efficient\\ninterfaces to computers, and for the interfaces to be natural and truly useful,\\nit should provide coverage for a large group of users. The purpose of these\\ntasks is to further improve man-machine communication. ASR systems exhibit\\nunacceptable degradations in performance when the acoustical environments used\\nfor training and testing the system are not the same. The goal of this research\\nis to increase the robustness of the speech recognition systems with respect to\\nchanges in the environment. A system can be labeled as environment-independent\\nif the recognition accuracy for a new environment is the same or higher than\\nthat obtained when the system is retrained for that environment. Attaining such\\nperformance is the dream of the researchers. This paper elaborates some of the\\ndifficulties with Automatic Speech Recognition (ASR). These difficulties are\\nclassified into Speakers characteristics and environmental conditions, and\\ntried to suggest some techniques to compensate variations in speech signal.\\nThis paper focuses on the robustness with respect to speakers variations and\\nchanges in the acoustical environment. We discussed several different external\\nfactors that change the environment and physiological differences that affect\\nthe performance of a speech recognition system followed by techniques that are\\nhelpful to design a robust ASR system.\\n',\n",
              " \"  SYNTAGMA is a rule-based parsing system, structured on two levels: a general\\nparsing engine and a language specific grammar. The parsing engine is a\\nlanguage independent program, while grammar and language specific rules and\\nresources are given as text files, consisting in a list of constituent\\nstructuresand a lexical database with word sense related features and\\nconstraints. Since its theoretical background is principally Tesniere's\\nElements de syntaxe, SYNTAGMA's grammar emphasizes the role of argument\\nstructure (valency) in constraint satisfaction, and allows also horizontal\\nbounds, for instance treating coordination. Notions such as Pro, traces, empty\\ncategories are derived from Generative Grammar and some solutions are close to\\nGovernment&Binding Theory, although they are the result of an autonomous\\nresearch. These properties allow SYNTAGMA to manage complex syntactic\\nconfigurations and well known weak points in parsing engineering. An important\\nresource is the semantic network, which is used in disambiguation tasks.\\nParsing process follows a bottom-up, rule driven strategy. Its behavior can be\\ncontrolled and fine-tuned.\\n\",\n",
              " '  In this paper, we show that certain phrases although not present in a given\\nquestion/query, play a very important role in answering the question. Exploring\\nthe role of such phrases in answering questions not only reduces the dependency\\non matching question phrases for extracting answers, but also improves the\\nquality of the extracted answers. Here matching question phrases means phrases\\nwhich co-occur in given question and candidate answers. To achieve the above\\ndiscussed goal, we introduce a bigram-based word graph model populated with\\nsemantic and topical relatedness of terms in the given document. Next, we apply\\nan improved version of ranking with a prior-based approach, which ranks all\\nwords in the candidate document with respect to a set of root words (i.e.\\nnon-stopwords present in the question and in the candidate document). As a\\nresult, terms logically related to the root words are scored higher than terms\\nthat are not related to the root words. Experimental results show that our\\ndevised system performs better than state-of-the-art for the task of answering\\nWhy-questions.\\n',\n",
              " '  Computers still have a long way to go before they can interact with users in\\na truly natural fashion. From a users perspective, the most natural way to\\ninteract with a computer would be through a speech and gesture interface.\\nAlthough speech recognition has made significant advances in the past ten\\nyears, gesture recognition has been lagging behind. Sign Languages (SL) are the\\nmost accomplished forms of gestural communication. Therefore, their automatic\\nanalysis is a real challenge, which is interestingly implied to their lexical\\nand syntactic organization levels. Statements dealing with sign language occupy\\na significant interest in the Automatic Natural Language Processing (ANLP)\\ndomain. In this work, we are dealing with sign language recognition, in\\nparticular of French Sign Language (FSL). FSL has its own specificities, such\\nas the simultaneity of several parameters, the important role of the facial\\nexpression or movement and the use of space for the proper utterance\\norganization. Unlike speech recognition, Frensh sign language (FSL) events\\noccur both sequentially and simultaneously. Thus, the computational processing\\nof FSL is too complex than the spoken languages. We present a novel approach\\nbased on HMM to reduce the recognition complexity.\\n',\n",
              " '  Mixing dependency lengths from sequences of different length is a common\\npractice in language research. However, the empirical distribution of\\ndependency lengths of sentences of the same length differs from that of\\nsentences of varying length and the distribution of dependency lengths depends\\non sentence length for real sentences and also under the null hypothesis that\\ndependencies connect vertices located in random positions of the sequence. This\\nsuggests that certain results, such as the distribution of syntactic dependency\\nlengths mixing dependencies from sentences of varying length, could be a mere\\nconsequence of that mixing. Furthermore, differences in the global averages of\\ndependency length (mixing lengths from sentences of varying length) for two\\ndifferent languages do not simply imply a priori that one language optimizes\\ndependency lengths better than the other because those differences could be due\\nto differences in the distribution of sentence lengths and other factors.\\n',\n",
              " '  Here tree dependency structures are studied from three different\\nperspectives: their degree variance (hubiness), the mean dependency length and\\nthe number of dependency crossings. Bounds that reveal pairwise dependencies\\namong these three metrics are derived. Hubiness (the variance of degrees) plays\\na central role: the mean dependency length is bounded below by hubiness while\\nthe number of crossings is bounded above by hubiness. Our findings suggest that\\nthe online memory cost of a sentence might be determined not just by the\\nordering of words but also by the hubiness of the underlying structure. The 2nd\\nmoment of degree plays a crucial role that is reminiscent of its role in large\\ncomplex networks.\\n',\n",
              " '  Our day-to-day life has always been influenced by what people think. Ideas\\nand opinions of others have always affected our own opinions. The explosion of\\nWeb 2.0 has led to increased activity in Podcasting, Blogging, Tagging,\\nContributing to RSS, Social Bookmarking, and Social Networking. As a result\\nthere has been an eruption of interest in people to mine these vast resources\\nof data for opinions. Sentiment Analysis or Opinion Mining is the computational\\ntreatment of opinions, sentiments and subjectivity of text. In this report, we\\ntake a look at the various challenges and applications of Sentiment Analysis.\\nWe will discuss in details various approaches to perform a computational\\ntreatment of sentiments and opinions. Various supervised or data-driven\\ntechniques to SA like Na\\\\\"ive Byes, Maximum Entropy, SVM, and Voted Perceptrons\\nwill be discussed and their strengths and drawbacks will be touched upon. We\\nwill also see a new dimension of analyzing sentiments by Cognitive Psychology\\nmainly through the work of Janyce Wiebe, where we will see ways to detect\\nsubjectivity, perspective in narrative and understanding the discourse\\nstructure. We will also study some specific topics in Sentiment Analysis and\\nthe contemporary works in those areas.\\n',\n",
              " '  In the geolocation field where high-level programs and low-level devices\\ncoexist, it is often difficult to find a friendly user inter- face to configure\\nall the parameters. The challenge addressed in this paper is to propose\\nintuitive and simple, thus natural lan- guage interfaces to interact with\\nlow-level devices. Such inter- faces contain natural language processing and\\nfuzzy represen- tations of words that facilitate the elicitation of\\nbusiness-level objectives in our context.\\n',\n",
              " '  Question answering involves developing methods to extract useful information\\nfrom large collections of documents. This is done with specialised search\\nengines such as Answer Finder. The aim of Answer Finder is to provide an answer\\nto a question rather than a page listing related documents that may contain the\\ncorrect answer. So, a question such as \"How tall is the Eiffel Tower\" would\\nsimply return \"325m\" or \"1,063ft\". Our task was to build on the current version\\nof Answer Finder by improving information retrieval, and also improving the\\npre-processing involved in question series analysis.\\n',\n",
              " '  Word ambiguity removal is a task of removing ambiguity from a word, i.e.\\ncorrect sense of word is identified from ambiguous sentences. This paper\\ndescribes a model that uses Part of Speech tagger and three categories for word\\nsense disambiguation (WSD). Human Computer Interaction is very needful to\\nimprove interactions between users and computers. For this, the Supervised and\\nUnsupervised methods are combined. The WSD algorithm is used to find the\\nefficient and accurate sense of a word based on domain information. The\\naccuracy of this work is evaluated with the aim of finding best suitable domain\\nof word.\\n',\n",
              " '  TimeML is an XML-based schema for annotating temporal information over\\ndiscourse. The standard has been used to annotate a variety of resources and is\\nfollowed by a number of tools, the creation of which constitute hundreds of\\nthousands of man-hours of research work. However, the current state of\\nresources is such that many are not valid, or do not produce valid output, or\\ncontain ambiguous or custom additions and removals. Difficulties arising from\\nthese variances were highlighted in the TempEval-3 exercise, which included its\\nown extra stipulations over conventional TimeML as a response.\\n  To unify the state of current resources, and to make progress toward easy\\nadoption of its current incarnation ISO-TimeML, this paper introduces\\nTimeML-strict: a valid, unambiguous, and easy-to-process subset of TimeML. We\\nalso introduce three resources -- a schema for TimeML-strict; a validator tool\\nfor TimeML-strict, so that one may ensure documents are in the correct form;\\nand a repair tool that corrects common invalidating errors and adds\\ndisambiguating markup in order to convert documents from the laxer TimeML\\nstandard to TimeML-strict.\\n',\n",
              " '  Researchers since at least Darwin have debated whether and to what extent\\nemotions are universal or culture-dependent. However, previous studies have\\nprimarily focused on facial expressions and on a limited set of emotions. Given\\nthat emotions have a substantial impact on human lives, evidence for cultural\\nemotional relativity might be derived by applying distributional semantics\\ntechniques to a text corpus of self-reported behaviour. Here, we explore this\\nidea by measuring the valence and arousal of the twelve most popular emotion\\nkeywords expressed on the micro-blogging site Twitter. We do this in three\\ngeographical regions: Europe, Asia and North America. We demonstrate that in\\nour sample, the valence and arousal levels of the same emotion keywords differ\\nsignificantly with respect to these geographical regions --- Europeans are, or\\nat least present themselves as more positive and aroused, North Americans are\\nmore negative and Asians appear to be more positive but less aroused when\\ncompared to global valence and arousal levels of the same emotion keywords. Our\\nwork is the first in kind to programatically map large text corpora to a\\ndimensional model of affect.\\n',\n",
              " '  Machine Translation is the translation of one natural language into another\\nusing automated and computerized means. For a multilingual country like India,\\nwith the huge amount of information exchanged between various regions and in\\ndifferent languages in digitized format, it has become necessary to find an\\nautomated process from one language to another. In this paper, we take a look\\nat the various Machine Translation System in India which is specifically built\\nfor the purpose of translation between the Indian languages. We discuss the\\nvarious approaches taken for building the machine translation system and then\\ndiscuss some of the Machine Translation Systems in India along with their\\nfeatures.\\n',\n",
              " '  This paper describes a temporal expression identification and normalization\\nsystem, ManTIME, developed for the TempEval-3 challenge. The identification\\nphase combines the use of conditional random fields along with a\\npost-processing identification pipeline, whereas the normalization phase is\\ncarried out using NorMA, an open-source rule-based temporal normalizer. We\\ninvestigate the performance variation with respect to different feature types.\\nSpecifically, we show that the use of WordNet-based features in the\\nidentification task negatively affects the overall performance, and that there\\nis no statistically significant difference in using gazetteers, shallow parsing\\nand propositional noun phrases labels on top of the morphological features. On\\nthe test data, the best run achieved 0.95 (P), 0.85 (R) and 0.90 (F1) in the\\nidentification phase. Normalization accuracies are 0.84 (type attribute) and\\n0.77 (value attribute). Surprisingly, the use of the silver data (alone or in\\naddition to the gold annotated ones) does not improve the performance.\\n',\n",
              " '  We discuss an algorithm which produces the meaning of a sentence given\\nmeanings of its words, and its resemblance to quantum teleportation. In fact,\\nthis protocol was the main source of inspiration for this algorithm which has\\nmany applications in the area of Natural Language Processing.\\n',\n",
              " '  We consider the unsupervised alignment of the full text of a book with a\\nhuman-written summary. This presents challenges not seen in other text\\nalignment problems, including a disparity in length and, consequent to this, a\\nviolation of the expectation that individual words and phrases should align,\\nsince large passages and chapters can be distilled into a single summary\\nphrase. We present two new methods, based on hidden Markov models, specifically\\ntargeted to this problem, and demonstrate gains on an extractive book\\nsummarization task. While there is still much room for improvement,\\nunsupervised alignment holds intrinsic value in offering insight into what\\nfeatures of a book are deemed worthy of summarization.\\n',\n",
              " '  The signal sound contains many different features, including Voice Onset Time\\n(VOT), which is a very important feature of stop sounds in many languages. The\\nonly application of VOT values is stopping phoneme subsets. This subset of\\nconsonant sounds is stop phonemes exist in the Arabic language, and in fact,\\nall languages. The pronunciation of these sounds is hard and unique especially\\nfor less-educated Arabs and non-native Arabic speakers. VOT can be utilized by\\nthe human auditory system to distinguish between voiced and unvoiced stops such\\nas /p/ and /b/ in English.This search focuses on computing and analyzing VOT of\\nModern Standard Arabic (MSA), within the Arabic language, for all pairs of\\nnon-emphatic (namely, /d/ and /t/) and emphatic pairs (namely, /d?/ and /t?/)\\ndepending on carrier words. This research uses a database built by ourselves,\\nand uses the carrier words syllable structure: CV-CV-CV. One of the main\\noutcomes always found is the emphatic sounds (/d?/, /t?/) are less than 50% of\\nnon-emphatic (counter-part) sounds ( /d/, /t/).Also, VOT can be used to\\nclassify or detect for a dialect ina language.\\n',\n",
              " '  Automatic speech recognition enables a wide range of current and emerging\\napplications such as automatic transcription, multimedia content analysis, and\\nnatural human-computer interfaces. This paper provides a glimpse of the\\nopportunities and challenges that parallelism provides for automatic speech\\nrecognition and related application research from the point of view of speech\\nresearchers. The increasing parallelism in computing platforms opens three\\nmajor possibilities for speech recognition systems: improving recognition\\naccuracy in non-ideal, everyday noisy environments; increasing recognition\\nthroughput in batch processing of speech data; and reducing recognition latency\\nin realtime usage scenarios. This paper describes technical challenges,\\napproaches taken, and possible directions for future research to guide the\\ndesign of efficient parallel software and hardware infrastructures.\\n',\n",
              " '  In this age of information technology, information access in a convenient\\nmanner has gained importance. Since speech is a primary mode of communication\\namong human beings, it is natural for people to expect to be able to carry out\\nspoken dialogue with computer. Speech recognition system permits ordinary\\npeople to speak to the computer to retrieve information. It is desirable to\\nhave a human computer dialogue in local language. Hindi being the most widely\\nspoken Language in India is the natural primary human language candidate for\\nhuman machine interaction. There are five pairs of vowels in Hindi languages;\\none member is longer than the other one. This paper describes an overview of\\nspeech recognition system that includes how speech is produced and the\\nproperties and characteristics of Hindi Phoneme.\\n',\n",
              " \"  The project presented in this article aims to formalize criteria and\\nprocedures in order to extract semantic information from parsed dictionary\\nglosses. The actual purpose of the project is the generation of a semantic\\nnetwork (nearly an ontology) issued from a monolingual Italian dictionary,\\nthrough unsupervised procedures. Since the project involves rule-based Parsing,\\nSemantic Tagging and Word Sense Disambiguation techniques, its outcomes may\\nfind an interest also beyond this immediate intent. The cooperation of both\\nsyntactic and semantic features in meaning construction are investigated, and\\nprocedures which allows a translation of syntactic dependencies in semantic\\nrelations are discussed. The procedures that rise from this project can be\\napplied also to other text types than dictionary glosses, as they convert the\\noutput of a parsing process into a semantic representation. In addition some\\nmechanism are sketched that may lead to a kind of procedural semantics, through\\nwhich multiple paraphrases of an given expression can be generated. Which means\\nthat these techniques may find an application also in 'query expansion'\\nstrategies, interesting Information Retrieval, Search Engines and Question\\nAnswering Systems.\\n\",\n",
              " '  Chinese word segmentation is a fundamental task for Chinese language\\nprocessing. The granularity mismatch problem is the main cause of the errors.\\nThis paper showed that the binary tree representation can store outputs with\\ndifferent granularity. A binary tree based framework is also designed to\\novercome the granularity mismatch problem. There are two steps in this\\nframework, namely tree building and tree pruning. The tree pruning step is\\nspecially designed to focus on the granularity problem. Previous work for\\nChinese word segmentation such as the sequence tagging can be easily employed\\nin this framework. This framework can also provide quantitative error analysis\\nmethods. The experiments showed that after using a more sophisticated tree\\npruning function for a state-of-the-art conditional random field based\\nbaseline, the error reduction can be up to 20%.\\n',\n",
              " '  It has been hypothesized that the rather small number of crossings in real\\nsyntactic dependency trees is a side-effect of pressure for dependency length\\nminimization. Here we answer a related important research question: what would\\nbe the expected number of crossings if the natural order of a sentence was lost\\nand replaced by a random ordering? We show that this number depends only on the\\nnumber of vertices of the dependency tree (the sentence length) and the second\\nmoment about zero of vertex degrees. The expected number of crossings is\\nminimum for a star tree (crossings are impossible) and maximum for a linear\\ntree (the number of crossings is of the order of the square of the sequence\\nlength).\\n',\n",
              " '  Conceptual combination performs a fundamental role in creating the broad\\nrange of compound phrases utilized in everyday language. This article provides\\na novel probabilistic framework for assessing whether the semantics of\\nconceptual combinations are compositional, and so can be considered as a\\nfunction of the semantics of the constituent concepts, or not. While the\\nsystematicity and productivity of language provide a strong argument in favor\\nof assuming compositionality, this very assumption is still regularly\\nquestioned in both cognitive science and philosophy. Additionally, the\\nprinciple of semantic compositionality is underspecified, which means that\\nnotions of both \"strong\" and \"weak\" compositionality appear in the literature.\\nRather than adjudicating between different grades of compositionality, the\\nframework presented here contributes formal methods for determining a clear\\ndividing line between compositional and non-compositional semantics. In\\naddition, we suggest that the distinction between these is contextually\\nsensitive. Utilizing formal frameworks developed for analyzing composite\\nsystems in quantum theory, we present two methods that allow the semantics of\\nconceptual combinations to be classified as \"compositional\" or\\n\"non-compositional\". Compositionality is first formalised by factorising the\\njoint probability distribution modeling the combination, where the terms in the\\nfactorisation correspond to individual concepts. This leads to the necessary\\nand sufficient condition for the joint probability distribution to exist. A\\nfailure to meet this condition implies that the underlying concepts cannot be\\nmodeled in a single probability space when considering their combination, and\\nthe combination is thus deemed \"non-compositional\". The formal analysis methods\\nare demonstrated by applying them to an empirical study of twenty-four\\nnon-lexicalised conceptual combinations.\\n',\n",
              " '  We describe an inventory of semantic relations that are expressed by\\nprepositions. We define these relations by building on the word sense\\ndisambiguation task for prepositions and propose a mapping from preposition\\nsenses to the relation labels by collapsing semantically related senses across\\nprepositions.\\n',\n",
              " '  Conventional statistics-based methods for joint Chinese word segmentation and\\npart-of-speech tagging (S&T) have generalization ability to recognize new words\\nthat do not appear in the training data. An undesirable side effect is that a\\nnumber of meaningless words will be incorrectly created. We propose an\\neffective and efficient framework for S&T that introduces features to\\nsignificantly reduce meaningless words generation. A general lexicon, Wikepedia\\nand a large-scale raw corpus of 200 billion characters are used to generate\\nword-based features for the wordhood. The word-lattice based framework consists\\nof a character-based model and a word-based model in order to employ our\\nword-based features. Experiments on Penn Chinese treebank 5 show that this\\nmethod has a 62.9% reduction of meaningless word generation in comparison with\\nthe baseline. As a result, the F1 measure for segmentation is increased to\\n0.984.\\n',\n",
              " '  We have explored different methods of improving the accuracy of a Naive Bayes\\nclassifier for sentiment analysis. We observed that a combination of methods\\nlike negation handling, word n-grams and feature selection by mutual\\ninformation results in a significant improvement in accuracy. This implies that\\na highly accurate and fast sentiment classifier can be built using a simple\\nNaive Bayes model that has linear training and testing time complexities. We\\nachieved an accuracy of 88.80% on the popular IMDB movie reviews dataset.\\n',\n",
              " '  We live in a translingual society, in order to communicate with people from\\ndifferent parts of the world we need to have an expertise in their respective\\nlanguages. Learning all these languages is not at all possible; therefore we\\nneed a mechanism which can do this task for us. Machine translators have\\nemerged as a tool which can perform this task. In order to develop a machine\\ntranslator we need to develop several different rules. The very first module\\nthat comes in machine translation pipeline is morphological analysis. Stemming\\nand lemmatization comes under morphological analysis. In this paper we have\\ncreated a lemmatizer which generates rules for removing the affixes along with\\nthe addition of rules for creating a proper root word.\\n',\n",
              " '  First-order multiplicative intuitionistic linear logic (MILL1) can be seen as\\nan extension of the Lambek calculus. In addition to the fragment of MILL1 which\\ncorresponds to the Lambek calculus (of Moot & Piazza 2001), I will show\\nfragments of MILL1 which generate the multiple context-free languages and which\\ncorrespond to the Displacement calculus of Morrilll e.a.\\n',\n",
              " '  With the release of SentiWordNet 3.0 the related Web interface has been\\nrestyled and improved in order to allow users to submit feedback on the\\nSentiWordNet entries, in the form of the suggestion of alternative triplets of\\nvalues for an entry. This paper reports on the release of the user feedback\\ncollected so far and on the plans for the future.\\n',\n",
              " '  We introduce a framework for lightweight dependency syntax annotation. Our\\nformalism builds upon the typical representation for unlabeled dependencies,\\npermitting a simple notation and annotation workflow. Moreover, the formalism\\nencourages annotators to underspecify parts of the syntax if doing so would\\nstreamline the annotation process. We demonstrate the efficacy of this\\nannotation on three languages and develop algorithms to evaluate and compare\\nunderspecified annotations.\\n',\n",
              " '  With the increasing empirical success of distributional models of\\ncompositional semantics, it is timely to consider the types of textual logic\\nthat such models are capable of capturing. In this paper, we address\\nshortcomings in the ability of current models to capture logical operations\\nsuch as negation. As a solution we propose a tripartite formulation for a\\ncontinuous vector space representation of semantics and subsequently use this\\nrepresentation to develop a formal compositional notion of negation within such\\nmodels.\\n',\n",
              " \"  The mathematical formalism of quantum theory has been successfully used in\\nhuman cognition to model decision processes and to deliver representations of\\nhuman knowledge. As such, quantum cognition inspired tools have improved\\ntechnologies for Natural Language Processing and Information Retrieval. In this\\npaper, we overview the quantum cognition approach developed in our Brussels\\nteam during the last two decades, specifically our identification of quantum\\nstructures in human concepts and language, and the modeling of data from\\npsychological and corpus-text-based experiments. We discuss our\\nquantum-theoretic framework for concepts and their conjunctions/disjunctions in\\na Fock-Hilbert space structure, adequately modeling a large amount of data\\ncollected on concept combinations. Inspired by this modeling, we put forward\\nelements for a quantum contextual and meaning-based approach to information\\ntechnologies in which 'entities of meaning' are inversely reconstructed from\\ntexts, which are considered as traces of these entities' states.\\n\",\n",
              " '  The compositionality of meaning extends beyond the single sentence. Just as\\nwords combine to form the meaning of sentences, so do sentences combine to form\\nthe meaning of paragraphs, dialogues and general discourse. We introduce both a\\nsentence model and a discourse model corresponding to the two levels of\\ncompositionality. The sentence model adopts convolution as the central\\noperation for composing semantic vectors and is based on a novel hierarchical\\nconvolutional neural network. The discourse model extends the sentence model\\nand is based on a recurrent neural network that is conditioned in a novel way\\nboth on the current sentence and on the current speaker. The discourse model is\\nable to capture both the sequentiality of sentences and the interaction between\\ndifferent speakers. Without feature engineering or pretraining and with simple\\ngreedy decoding, the discourse model coupled to the sentence model obtains\\nstate of the art performance on a dialogue act classification experiment.\\n',\n",
              " '  The IMPACT-es diachronic corpus of historical Spanish compiles over one\\nhundred books --containing approximately 8 million words-- in addition to a\\ncomplementary lexicon which links more than 10 thousand lemmas with\\nattestations of the different variants found in the documents. This textual\\ncorpus and the accompanying lexicon have been released under an open license\\n(Creative Commons by-nc-sa) in order to permit their intensive exploitation in\\nlinguistic research. Approximately 7% of the words in the corpus (a selection\\naimed at enhancing the coverage of the most frequent word forms) have been\\nannotated with their lemma, part of speech, and modern equivalent. This paper\\ndescribes the annotation criteria followed and the standards, based on the Text\\nEncoding Initiative recommendations, used to the represent the texts in digital\\nform. As an illustration of the possible synergies between diachronic textual\\nresources and linguistic research, we describe the application of statistical\\nmachine translation techniques to infer probabilistic context-sensitive rules\\nfor the automatic modernisation of spelling. The automatic modernisation with\\nthis type of statistical methods leads to very low character error rates when\\nthe output is compared with the supervised modern version of the text.\\n',\n",
              " '  Patterns of topological arrangement are widely used for both animal and human\\nbrains in the learning process. Nevertheless, automatic learning techniques\\nfrequently overlook these patterns. In this paper, we apply a learning\\ntechnique based on the structural organization of the data in the attribute\\nspace to the problem of discriminating the senses of 10 polysemous words. Using\\ntwo types of characterization of meanings, namely semantical and topological\\napproaches, we have observed significative accuracy rates in identifying the\\nsuitable meanings in both techniques. Most importantly, we have found that the\\ncharacterization based on the deterministic tourist walk improves the\\ndisambiguation process when one compares with the discrimination achieved with\\ntraditional complex networks measurements such as assortativity and clustering\\ncoefficient. To our knowledge, this is the first time that such deterministic\\nwalk has been applied to such a kind of problem. Therefore, our finding\\nsuggests that the tourist walk characterization may be useful in other related\\napplications.\\n',\n",
              " '  A Dialogue System is a system which interacts with human in natural language.\\nAt present many universities are developing the dialogue system in their\\nregional language. This paper will discuss about dialogue system, its\\ncomponents, challenges and its evaluation. This paper helps the researchers for\\ngetting info regarding dialogues system.\\n',\n",
              " '  Unlike most user-computer interfaces, a natural language interface allows\\nusers to communicate fluently with a computer system with very little\\npreparation. Databases are often hard to use in cooperating with the users\\nbecause of their rigid interface. A good NLIDB allows a user to enter commands\\nand ask questions in native language and then after interpreting respond to the\\nuser in native language. For a large number of applications requiring\\ninteraction between humans and the computer systems, it would be convenient to\\nprovide the end-user friendly interface. Punjabi language interface to database\\nwould proof fruitful to native people of Punjab, as it provides ease to them to\\nuse various e-governance applications like Punjab Sewa, Suwidha, Online Public\\nUtility Forms, Online Grievance Cell, Land Records Management System,legacy\\nmatters, e-District, agriculture, etc. Punjabi is the mother tongue of more\\nthan 110 million people all around the world. According to available\\ninformation, Punjabi ranks 10th from top out of a total of 6,900 languages\\nrecognized internationally by the United Nations. This paper covers a brief\\noverview of the Natural language interface to database, its different\\ncomponents, its advantages, disadvantages, approaches and techniques used. The\\npaper ends with the work done on Punjabi language interface to database and\\nfuture enhancements that can be done.\\n',\n",
              " '  Fast and effective automated indexing is critical for search and personalized\\nservices. Key phrases that consist of one or more words and represent the main\\nconcepts of the document are often used for the purpose of indexing. In this\\npaper, we investigate the use of additional semantic features and\\npre-processing steps to improve automatic key phrase extraction. These features\\ninclude the use of signal words and freebase categories. Some of these features\\nlead to significant improvements in the accuracy of the results. We also\\nexperimented with 2 forms of document pre-processing that we call light\\nfiltering and co-reference normalization. Light filtering removes sentences\\nfrom the document, which are judged peripheral to its main content.\\nCo-reference normalization unifies several written forms of the same named\\nentity into a unique form. We also needed a \"Gold Standard\" - a set of labeled\\ndocuments for training and evaluation. While the subjective nature of key\\nphrase selection precludes a true \"Gold Standard\", we used Amazon\\'s Mechanical\\nTurk service to obtain a useful approximation. Our data indicates that the\\nbiggest improvements in performance were due to shallow semantic features, news\\ncategories, and rhetorical signals (nDCG 78.47% vs. 68.93%). The inclusion of\\ndeeper semantic features such as Freebase sub-categories was not beneficial by\\nitself, but in combination with pre-processing, did cause slight improvements\\nin the nDCG scores.\\n',\n",
              " '  This paper explores the impact of light filtering on automatic key phrase\\nextraction (AKE) applied to Broadcast News (BN). Key phrases are words and\\nexpressions that best characterize the content of a document. Key phrases are\\noften used to index the document or as features in further processing. This\\nmakes improvements in AKE accuracy particularly important. We hypothesized that\\nfiltering out marginally relevant sentences from a document would improve AKE\\naccuracy. Our experiments confirmed this hypothesis. Elimination of as little\\nas 10% of the document sentences lead to a 2% improvement in AKE precision and\\nrecall. AKE is built over MAUI toolkit that follows a supervised learning\\napproach. We trained and tested our AKE method on a gold standard made of 8 BN\\nprograms containing 110 manually annotated news stories. The experiments were\\nconducted within a Multimedia Monitoring Solution (MMS) system for TV and radio\\nnews/programs, running daily, and monitoring 12 TV and 4 radio channels.\\n',\n",
              " '  We extend the concept of Named Entities to Named Events - commonly occurring\\nevents such as battles and earthquakes. We propose a method for finding\\nspecific passages in news articles that contain information about such events\\nand report our preliminary evaluation results. Collecting \"Gold Standard\" data\\npresents many problems, both practical and conceptual. We present a method for\\nobtaining such data using the Amazon Mechanical Turk service.\\n',\n",
              " '  We propose a computational framework for identifying linguistic aspects of\\npoliteness. Our starting point is a new corpus of requests annotated for\\npoliteness, which we use to evaluate aspects of politeness theory and to\\nuncover new interactions between politeness markers and context. These findings\\nguide our construction of a classifier with domain-independent lexical and\\nsyntactic features operationalizing key components of politeness theory, such\\nas indirection, deference, impersonalization and modality. Our classifier\\nachieves close to human performance and is effective across domains. We use our\\nframework to study the relationship between politeness and social power,\\nshowing that polite Wikipedia editors are more likely to achieve high status\\nthrough elections, but, once elevated, they become less polite. We see a\\nsimilar negative correlation between politeness and power on Stack Exchange,\\nwhere users at the top of the reputation scale are less polite than those at\\nthe bottom. Finally, we apply our classifier to a preliminary analysis of\\npoliteness variation by gender and community.\\n',\n",
              " \"  My system utilizes the outcomes feature found in Moodle and other learning\\ncontent management systems (LCMSs) to keep track of where students are in terms\\nof what language competencies they have mastered and the competencies they need\\nto get where they want to go. These competencies are based on the Common\\nEuropean Framework for (English) Language Learning. This data can be available\\nfor everyone involved with a given student's progress (e.g. educators, parents,\\nsupervisors and the students themselves). A given student's record of past\\naccomplishments can also be meshed with those of his classmates. Not only are a\\nstudent's competencies easily seen and tracked, educators can view competencies\\nof a group of students that were achieved prior to enrollment in the class.\\nThis should make curriculum decision making easier and more efficient for\\neducators.\\n\",\n",
              " '  Arabizi is Arabic text that is written using Latin characters. Arabizi is\\nused to present both Modern Standard Arabic (MSA) or Arabic dialects. It is\\ncommonly used in informal settings such as social networking sites and is often\\nwith mixed with English. In this paper we address the problems of: identifying\\nArabizi in text and converting it to Arabic characters. We used word and\\nsequence-level features to identify Arabizi that is mixed with English. We\\nachieved an identification accuracy of 98.5%. As for conversion, we used\\ntransliteration mining with language modeling to generate equivalent Arabic\\ntext. We achieved 88.7% conversion accuracy, with roughly a third of errors\\nbeing spelling and morphological variants of the forms in ground truth.\\n',\n",
              " '  A high-quality content analysis is essential for retrieval functionalities\\nbut the manual extraction of key phrases and classification is expensive.\\nNatural language processing provides a framework to automatize the process.\\nHere, a machine-based approach for the content analysis of mathematical texts\\nis described. A prototype for key phrase extraction and classification of\\nmathematical texts is presented.\\n',\n",
              " '  We design a new co-occurrence based word association measure by incorporating\\nthe concept of significant cooccurrence in the popular word association measure\\nPointwise Mutual Information (PMI). By extensive experiments with a large\\nnumber of publicly available datasets we show that the newly introduced measure\\nperforms better than other co-occurrence based measures and despite being\\nresource-light, compares well with the best known resource-heavy distributional\\nsimilarity and knowledge based word association measures. We investigate the\\nsource of this performance improvement and find that of the two types of\\nsignificant co-occurrence - corpus-level and document-level, the concept of\\ncorpus level significance combined with the use of document counts in place of\\nword counts is responsible for all the performance gains observed. The concept\\nof document level significance is not helpful for PMI adaptation.\\n',\n",
              " '  Distributed word representations (word embeddings) have recently contributed\\nto competitive performance in language modeling and several NLP tasks. In this\\nwork, we train word embeddings for more than 100 languages using their\\ncorresponding Wikipedias. We quantitatively demonstrate the utility of our word\\nembeddings by using them as the sole features for training a part of speech\\ntagger for a subset of these languages. We find their performance to be\\ncompetitive with near state-of-art methods in English, Danish and Swedish.\\nMoreover, we investigate the semantic features captured by these embeddings\\nthrough the proximity of word groupings. We will release these embeddings\\npublicly to help researchers in the development and enhancement of multilingual\\napplications.\\n',\n",
              " '  Inferring evaluation scores based on human judgments is invaluable compared\\nto using current evaluation metrics which are not suitable for real-time\\napplications e.g. post-editing. However, these judgments are much more\\nexpensive to collect especially from expert translators, compared to evaluation\\nbased on indicators contrasting source and translation texts. This work\\nintroduces a novel approach for quality estimation by combining learnt\\nconfidence scores from a probabilistic inference model based on human\\njudgments, with selective linguistic features-based scores, where the proposed\\ninference model infers the credibility of given human ranks to solve the\\nscarcity and inconsistency issues of human judgments. Experimental results,\\nusing challenging language-pairs, demonstrate improvement in correlation with\\nhuman judgments over traditional evaluation metrics.\\n',\n",
              " '  Machine Translation for Indian languages is an emerging research area.\\nTransliteration is one such module that we design while designing a translation\\nsystem. Transliteration means mapping of source language text into the target\\nlanguage. Simple mapping decreases the efficiency of overall translation\\nsystem. We propose the use of stemming and part-of-speech tagging for\\ntransliteration. The effectiveness of translation can be improved if we use\\npart-of-speech tagging and stemming assisted transliteration.We have shown that\\nmuch of the content in Gujarati gets transliterated while being processed for\\ntranslation to Hindi language.\\n',\n",
              " '  The current research is focusing on the area of Opinion Mining also called as\\nsentiment analysis due to sheer volume of opinion rich web resources such as\\ndiscussion forums, review sites and blogs are available in digital form. One\\nimportant problem in sentiment analysis of product reviews is to produce\\nsummary of opinions based on product features. We have surveyed and analyzed in\\nthis paper, various techniques that have been developed for the key tasks of\\nopinion mining. We have provided an overall picture of what is involved in\\ndeveloping a software system for opinion mining on the basis of our survey and\\nanalysis.\\n',\n",
              " '  With the growing number of textual resources available, the ability to\\nunderstand them becomes critical. An essential first step in understanding\\nthese sources is the ability to identify the part of speech in each sentence.\\nArabic is a morphologically rich language, wich presents a challenge for part\\nof speech tagging. In this paper, our goal is to propose, improve and implement\\na part of speech tagger based on a genetic alorithm. The accuracy obtained with\\nthis method is comparable to that of other probabilistic approaches.\\n',\n",
              " '  In this paper we present a Marathi part of speech tagger. It is a\\nmorphologically rich language. It is spoken by the native people of\\nMaharashtra. The general approach used for development of tagger is statistical\\nusing trigram Method. The main concept of trigram is to explore the most likely\\nPOS for a token based on given information of previous two tags by calculating\\nprobabilities to determine which is the best sequence of a tag. In this paper\\nwe show the development of the tagger. Moreover we have also shown the\\nevaluation done.\\n',\n",
              " '  Machine Transliteration has come out to be an emerging and a very important\\nresearch area in the field of machine translation. Transliteration basically\\naims to preserve the phonological structure of words. Proper transliteration of\\nname entities plays a very significant role in improving the quality of machine\\ntranslation. In this paper we are doing machine transliteration for\\nEnglish-Punjabi language pair using rule based approach. We have constructed\\nsome rules for syllabification. Syllabification is the process to extract or\\nseparate the syllable from the words. In this we are calculating the\\nprobabilities for name entities (Proper names and location). For those words\\nwhich do not come under the category of name entities, separate probabilities\\nare being calculated by using relative frequency through a statistical machine\\ntranslation toolkit known as MOSES. Using these probabilities we are\\ntransliterating our input text from English to Punjabi.\\n',\n",
              " '  We perform an automatic analysis of television news programs, based on the\\nclosed captions that accompany them. Specifically, we collect all the news\\nbroadcasted in over 140 television channels in the US during a period of six\\nmonths. We start by segmenting, processing, and annotating the closed captions\\nautomatically. Next, we focus on the analysis of their linguistic style and on\\nmentions of people using NLP methods. We present a series of key insights about\\nnews providers, people in the news, and we discuss the biases that can be\\nuncovered by automatic means. These insights are contrasted by looking at the\\ndata from multiple points of view, including qualitative assessment.\\n',\n",
              " '  The use of robo-readers to analyze news texts is an emerging technology trend\\nin computational finance. In recent research, a substantial effort has been\\ninvested to develop sophisticated financial polarity-lexicons that can be used\\nto investigate how financial sentiments relate to future company performance.\\nHowever, based on experience from other fields, where sentiment analysis is\\ncommonly applied, it is well-known that the overall semantic orientation of a\\nsentence may differ from the prior polarity of individual words. The objective\\nof this article is to investigate how semantic orientations can be better\\ndetected in financial and economic news by accommodating the overall\\nphrase-structure information and domain-specific use of language. Our three\\nmain contributions are: (1) establishment of a human-annotated finance\\nphrase-bank, which can be used as benchmark for training and evaluating\\nalternative models; (2) presentation of a technique to enhance financial\\nlexicons with attributes that help to identify expected direction of events\\nthat affect overall sentiment; (3) development of a linearized phrase-structure\\nmodel for detecting contextual semantic orientations in financial and economic\\nnews texts. The relevance of the newly added lexicon features and the benefit\\nof using the proposed learning-algorithm are demonstrated in a comparative\\nstudy against previously used general sentiment models as well as the popular\\nword frequency models used in recent financial studies. The proposed framework\\nis parsimonious and avoids the explosion in feature-space caused by the use of\\nconventional n-gram features.\\n',\n",
              " '  Natural language processing area is still under research. But now a day it is\\non platform for worldwide researchers. Natural language processing includes\\nanalyzing the language based on its structure and then tagging of each word\\nappropriately with its grammar base. Here we have 50,000 tagged words set and\\nwe try to cluster those Gujarati words based on proposed algorithm, we have\\ndefined our own algorithm for processing. Many clustering techniques are\\navailable Ex. Single linkage, complete, linkage,average linkage, Hear no of\\nclusters to be formed are not known, so it is all depends on the type of data\\nset provided . Clustering is preprocess for stemming . Stemming is the process\\nwhere root is extracted from its word. Ex. cats= cat+S, meaning. Cat: Noun and\\nplural form.\\n',\n",
              " '  An efficient speech to text converter for mobile application is presented in\\nthis work. The prime motive is to formulate a system which would give optimum\\nperformance in terms of complexity, accuracy, delay and memory requirements for\\nmobile environment. The speech to text converter consists of two stages namely\\nfront-end analysis and pattern recognition. The front end analysis involves\\npreprocessing and feature extraction. The traditional voice activity detection\\nalgorithms which track only energy cannot successfully identify potential\\nspeech from input because the unwanted part of the speech also has some energy\\nand appears to be speech. In the proposed system, VAD that calculates energy of\\nhigh frequency part separately as zero crossing rate to differentiate noise\\nfrom speech is used. Mel Frequency Cepstral Coefficient (MFCC) is used as\\nfeature extraction method and Generalized Regression Neural Network is used as\\nrecognizer. MFCC provides low word error rate and better feature extraction.\\nNeural Network improves the accuracy. Thus a small database containing all\\npossible syllable pronunciation of the user is sufficient to give recognition\\naccuracy closer to 100%. Thus the proposed technique entertains realization of\\nreal time speaker independent applications like mobile phones, PDAs etc.\\n',\n",
              " '  For the past 60 years, Research in machine translation is going on. For the\\ndevelopment in this field, a lot of new techniques are being developed each\\nday. As a result, we have witnessed development of many automatic machine\\ntranslators. A manager of machine translation development project needs to know\\nthe performance increase/decrease, after changes have been done in his system.\\nDue to this reason, a need for evaluation of machine translation systems was\\nfelt. In this article, we shall present the evaluation of some machine\\ntranslators. This evaluation will be done by a human evaluator and by some\\nautomatic evaluation metrics, which will be done at sentence, document and\\nsystem level. In the end we shall also discuss the comparison between the\\nevaluations.\\n',\n",
              " '  Recently, Ferrer i Cancho and Moscoso del Prado Martin [arXiv:1209.1751]\\nargued that an observed linear relationship between word length and average\\nsurprisal (Piantadosi, Tily, & Gibson, 2011) is not evidence for communicative\\nefficiency in human language. We discuss several shortcomings of their approach\\nand critique: their model critically rests on inaccurate assumptions, is\\nincapable of explaining key surprisal patterns in language, and is incompatible\\nwith recent behavioral results. More generally, we argue that statistical\\nmodels must not critically rely on assumptions that are incompatible with the\\nreal system under study.\\n',\n",
              " '  We develop a probabilistic latent-variable model to discover semantic\\nframes---types of events and their participants---from corpora. We present a\\nDirichlet-multinomial model in which frames are latent categories that explain\\nthe linking of verb-subject-object triples, given document-level sparsity. We\\nanalyze what the model learns, and compare it to FrameNet, noting it learns\\nsome novel and interesting frames. This document also contains a discussion of\\ninference issues, including concentration parameter learning; and a small-scale\\nerror analysis of syntactic parsing accuracy.\\n',\n",
              " '  This paper proposes a novel approach for relation extraction from free text\\nwhich is trained to jointly use information from the text and from existing\\nknowledge. Our model is based on two scoring functions that operate by learning\\nlow-dimensional embeddings of words and of entities and relationships from a\\nknowledge base. We empirically show on New York Times articles aligned with\\nFreebase relations that our approach is able to efficiently use the extra\\ninformation provided by a large subset of Freebase data (4M entities, 23k\\nrelationships) to improve over existing methods that rely on text features\\nalone.\\n',\n",
              " '  In this paper, we establish Fog Index (FI) as a text filter to locate the\\nsentences in texts that contain connected biomedical concepts of interest. To\\ndo so, we have used 24 random papers each containing four pairs of connected\\nconcepts. For each pair, we categorize sentences based on whether they contain\\nboth, any or none of the concepts. We then use FI to measure difficulty of the\\nsentences of each category and find that sentences containing both of the\\nconcepts have low readability. We rank sentences of a text according to their\\nFI and select 30 percent of the most difficult sentences. We use an association\\nmatrix to track the most frequent pairs of concepts in them. This matrix\\nreports that the first filter produces some pairs that hold almost no\\nconnections. To remove these unwanted pairs, we use the Equally Weighted\\nHarmonic Mean of their Positive Predictive Value (PPV) and Sensitivity as a\\nsecond filter. Experimental results demonstrate the effectiveness of our\\nmethod.\\n',\n",
              " '  With the proliferation of its applications in various industries, sentiment\\nanalysis by using publicly available web data has become an active research\\narea in text classification during these years. It is argued by researchers\\nthat semi-supervised learning is an effective approach to this problem since it\\nis capable to mitigate the manual labeling effort which is usually expensive\\nand time-consuming. However, there was a long-term debate on the effectiveness\\nof unlabeled data in text classification. This was partially caused by the fact\\nthat many assumptions in theoretic analysis often do not hold in practice. We\\nargue that this problem may be further understood by adding an additional\\ndimension in the experiment. This allows us to address this problem in the\\nperspective of bias and variance in a broader view. We show that the well-known\\nperformance degradation issue caused by unlabeled data can be reproduced as a\\nsubset of the whole scenario. We argue that if the bias-variance trade-off is\\nto be better balanced by a more effective feature selection method unlabeled\\ndata is very likely to boost the classification performance. We then propose a\\nfeature selection framework in which labeled and unlabeled training samples are\\nboth considered. We discuss its potential in achieving such a balance. Besides,\\nthe application in financial sentiment analysis is chosen because it not only\\nexemplifies an important application, the data possesses better illustrative\\npower as well. The implications of this study in text classification and\\nfinancial sentiment analysis are both discussed.\\n',\n",
              " '  We present a new context based event indexing and event ranking model for\\nNews Articles. The context event clusters formed from the UNL Graphs uses the\\nmodified scoring scheme for segmenting events which is followed by clustering\\nof events. From the context clusters obtained three models are developed-\\nIdentification of Main and Sub events; Event Indexing and Event Ranking. Based\\non the properties considered from the UNL Graphs for the modified scoring main\\nevents and sub events associated with main-events are identified. The temporal\\ndetails obtained from the context cluster are stored using hashmap data\\nstructure. The temporal details are place-where the event took; person-who\\ninvolved in that event; time-when the event took place. Based on the\\ninformation collected from the context clusters three indices are generated-\\nTime index, Person index, and Place index. This index gives complete details\\nabout every event obtained from context clusters. A new scoring scheme is\\nintroduced for ranking the events. The scoring scheme for event ranking gives\\nweight-age based on the priority level of the events. The priority level\\nincludes the occurrence of the event in the title of the document, event\\nfrequency, and inverse document frequency of the events.\\n',\n",
              " '  The problem of named entity recognition in the medical/clinical domain has\\ngained increasing attention do to its vital role in a wide range of clinical\\ndecision support applications. The identification of complete and correct term\\nspan is vital for further knowledge synthesis (e.g., coding/mapping concepts\\nthesauruses and classification standards). This paper investigates boundary\\nadjustment by sequence labeling representations models and post-processing\\ntechniques in the problem of clinical named entity recognition (recognition of\\nclinical events). Using current state-of-the-art sequence labeling algorithm\\n(conditional random fields), we show experimentally that sequence labeling\\nrepresentation and post-processing can be significantly helpful in strict\\nboundary identification of clinical events.\\n',\n",
              " \"  An object--oriented approach to create a natural language understanding\\nsystem is considered. The understanding program is a formal system built on the\\nbase of predicative calculus. Horn's clauses are used as well--formed formulas.\\nAn inference is based on the principle of resolution. Sentences of natural\\nlanguage are represented in the view of typical predicate set. These predicates\\ndescribe physical objects and processes, abstract objects, categories and\\nsemantic relations between objects. Predicates for concrete assertions are\\nsaved in a database. To describe the semantics of classes for physical objects,\\nabstract concepts and processes, a knowledge base is applied. The proposed\\nrepresentation of natural language sentences is a semantic net. Nodes of such\\nnet are typical predicates. This approach is perspective as, firstly, such\\ntypification of nodes facilitates essentially forming of processing algorithms\\nand object descriptions, secondly, the effectiveness of algorithms is increased\\n(particularly for the great number of nodes), thirdly, to describe the\\nsemantics of words, encyclopedic knowledge is used, and this permits\\nessentially to extend the class of solved problems.\\n\",\n",
              " '  Analysis of information retrieved from microblogging services such as Twitter\\ncan provide valuable insight into public sentiment in a geographic region. This\\ninsight can be enriched by visualising information in its geographic context.\\nTwo underlying approaches for sentiment analysis are dictionary-based and\\nmachine learning. The former is popular for public sentiment analysis, and the\\nlatter has found limited use for aggregating public sentiment from Twitter\\ndata. The research presented in this paper aims to extend the machine learning\\napproach for aggregating public sentiment. To this end, a framework for\\nanalysing and visualising public sentiment from a Twitter corpus is developed.\\nA dictionary-based approach and a machine learning approach are implemented\\nwithin the framework and compared using one UK case study, namely the royal\\nbirth of 2013. The case study validates the feasibility of the framework for\\nanalysis and rapid visualisation. One observation is that there is good\\ncorrelation between the results produced by the popular dictionary-based\\napproach and the machine learning approach when large volumes of tweets are\\nanalysed. However, for rapid analysis to be possible faster methods need to be\\ndeveloped using big data techniques and parallel methods.\\n',\n",
              " '  We present an effective multifaceted system for exploratory analysis of\\nhighly heterogeneous document collections. Our system is based on intelligently\\ntagging individual documents in a purely automated fashion and exploiting these\\ntags in a powerful faceted browsing framework. Tagging strategies employed\\ninclude both unsupervised and supervised approaches based on machine learning\\nand natural language processing. As one of our key tagging strategies, we\\nintroduce the KERA algorithm (Keyword Extraction for Reports and Articles).\\nKERA extracts topic-representative terms from individual documents in a purely\\nunsupervised fashion and is revealed to be significantly more effective than\\nstate-of-the-art methods. Finally, we evaluate our system in its ability to\\nhelp users locate documents pertaining to military critical technologies buried\\ndeep in a large heterogeneous sea of information.\\n',\n",
              " '  How many words are needed to define all the words in a dictionary?\\nGraph-theoretic analysis reveals that about 10% of a dictionary is a unique\\nKernel of words that define one another and all the rest, but this is not the\\nsmallest such subset. The Kernel consists of one huge strongly connected\\ncomponent (SCC), about half its size, the Core, surrounded by many small SCCs,\\nthe Satellites. Core words can define one another but not the rest of the\\ndictionary. The Kernel also contains many overlapping Minimal Grounding Sets\\n(MGSs), each about the same size as the Core, each part-Core, part-Satellite.\\nMGS words can define all the rest of the dictionary. They are learned earlier,\\nmore concrete and more frequent than the rest of the dictionary. Satellite\\nwords, not correlated with age or frequency, are less concrete (more abstract)\\nwords that are also needed for full lexical power.\\n',\n",
              " '  Discourse analysis may seek to characterize not only the overall composition\\nof a given text but also the dynamic patterns within the data. This technical\\nreport introduces a data format intended to facilitate multi-level\\ninvestigations, which we call the by-word long-form or B(eo)W(u)LF. Inspired by\\nthe long-form data format required for mixed-effects modeling, B(eo)W(u)LF\\nstructures linguistic data into an expanded matrix encoding any number of\\nresearchers-specified markers, making it ideal for recurrence-based analyses.\\nWhile we do not necessarily claim to be the first to use methods along these\\nlines, we have created a series of tools utilizing Python and MATLAB to enable\\nsuch discourse analyses and demonstrate them using 319 lines of the Old English\\nepic poem, Beowulf, translated into modern English.\\n',\n",
              " '  This paper concerns with the conversion of a Spoken English Language Query\\ninto SQL for retrieving data from RDBMS. A User submits a query as speech\\nsignal through the user interface and gets the result of the query in the text\\nformat. We have developed the acoustic and language models using which a speech\\nutterance can be converted into English text query and thus natural language\\nprocessing techniques can be applied on this English text query to generate an\\nequivalent SQL query. For conversion of speech into English text HTK and Julius\\ntools have been used and for conversion of English text query into SQL query we\\nhave implemented a System which uses rule based translation to translate\\nEnglish Language Query into SQL Query. The translation uses lexical analyzer,\\nparser and syntax directed translation techniques like in compilers. JFLex and\\nBYACC tools have been used to build lexical analyzer and parser respectively.\\nSystem is domain independent i.e. system can run on different database as it\\ngenerates lex files from the underlying database.\\n',\n",
              " '  This paper is concerned with the development of Back-propagation Neural\\nNetwork for Bangla Speech Recognition. In this paper, ten bangla digits were\\nrecorded from ten speakers and have been recognized. The features of these\\nspeech digits were extracted by the method of Mel Frequency Cepstral\\nCoefficient (MFCC) analysis. The mfcc features of five speakers were used to\\ntrain the network with Back propagation algorithm. The mfcc features of ten\\nbangla digit speeches, from 0 to 9, of another five speakers were used to test\\nthe system. All the methods and algorithms used in this research were\\nimplemented using the features of Turbo C and C++ languages. From our\\ninvestigation it is seen that the developed system can successfully encode and\\nanalyze the mfcc features of the speech signal to recognition. The developed\\nsystem achieved recognition rate about 96.332% for known speakers (i.e.,\\nspeaker dependent) and 92% for unknown speakers (i.e., speaker independent).\\n',\n",
              " '  It is a long term desire of the computer users to minimize the communication\\ngap between the computer and a human. On the other hand, almost all ICT\\napplications store information in to databases and retrieve from them.\\nRetrieving information from the database requires knowledge of technical\\nlanguages such as Structured Query Language. However majority of the computer\\nusers who interact with the databases do not have a technical background and\\nare intimidated by the idea of using languages such as SQL. For above reasons,\\na Natural Language Web Interface for Database (NLWIDB) has been developed. The\\nNLWIDB allows the user to query the database in a language more like English,\\nthrough a convenient interface over the Internet.\\n',\n",
              " '  In this paper we introduce a method to detect words or phrases in a given\\nsequence of alphabets without knowing the lexicon. Our linear time unsupervised\\nalgorithm relies entirely on statistical relationships among alphabets in the\\ninput sequence to detect location of word boundaries. We compare our algorithm\\nto previous approaches from unsupervised sequence segmentation literature and\\nprovide superior segmentation over number of benchmarks.\\n',\n",
              " '  By investigating the distribution of phrase pairs in phrase translation\\ntables, the work in this paper describes an approach to increase the number of\\nn-gram alignments in phrase translation tables output by a sampling-based\\nalignment method. This approach consists in enforcing the alignment of n-grams\\nin distinct translation subtables so as to increase the number of n-grams.\\nStandard normal distribution is used to allot alignment time among translation\\nsubtables, which results in adjustment of the distribution of n- grams. This\\nleads to better evaluation results on statistical machine translation tasks\\nthan the original sampling-based alignment approach. Furthermore, the\\ntranslation quality obtained by merging phrase translation tables computed from\\nthe sampling-based alignment method and from MGIZA++ is examined.\\n',\n",
              " '  A constant influx of new data poses a challenge in keeping the annotation in\\nbiological databases current. Most biological databases contain significant\\nquantities of textual annotation, which often contains the richest source of\\nknowledge. Many databases reuse existing knowledge, during the curation process\\nannotations are often propagated between entries. However, this is often not\\nmade explicit. Therefore, it can be hard, potentially impossible, for a reader\\nto identify where an annotation originated from. Within this work we attempt to\\nidentify annotation provenance and track its subsequent propagation.\\nSpecifically, we exploit annotation reuse within the UniProt Knowledgebase\\n(UniProtKB), at the level of individual sentences. We describe a visualisation\\napproach for the provenance and propagation of sentences in UniProtKB which\\nenables a large-scale statistical analysis. Initially levels of sentence reuse\\nwithin UniProtKB were analysed, showing that reuse is heavily prevalent, which\\nenables the tracking of provenance and propagation. By analysing sentences\\nthroughout UniProtKB, a number of interesting propagation patterns were\\nidentified, covering over 100, 000 sentences. Over 8000 sentences remain in the\\ndatabase after they have been removed from the entries where they originally\\noccurred. Analysing a subset of these sentences suggest that approximately 30%\\nare erroneous, whilst 35% appear to be inconsistent. These results suggest that\\nbeing able to visualise sentence propagation and provenance can aid in the\\ndetermination of the accuracy and quality of textual annotation. Source code\\nand supplementary data are available from the authors website.\\n',\n",
              " '  Stemming is the process of extracting root word from the given inflection\\nword. It also plays significant role in numerous application of Natural\\nLanguage Processing (NLP). The stemming problem has addressed in many contexts\\nand by researchers in many disciplines. This expository paper presents survey\\nof some of the latest developments on stemming algorithms in data mining and\\nalso presents with some of the solutions for various Indian language stemming\\nalgorithms along with the results.\\n',\n",
              " '  This text is a conceptual introduction to mixed effects modeling with\\nlinguistic applications, using the R programming environment. The reader is\\nintroduced to linear modeling and assumptions, as well as to mixed\\neffects/multilevel modeling, including a discussion of random intercepts,\\nrandom slopes and likelihood ratio tests. The example used throughout the text\\nfocuses on the phonetic analysis of voice pitch data.\\n',\n",
              " '  In this paper, we describe how we created two state-of-the-art SVM\\nclassifiers, one to detect the sentiment of messages such as tweets and SMS\\n(message-level task) and one to detect the sentiment of a term within a\\nsubmissions stood first in both tasks on tweets, obtaining an F-score of 69.02\\nin the message-level task and 88.93 in the term-level task. We implemented a\\nvariety of surface-form, semantic, and sentiment features. with sentiment-word\\nhashtags, and one from tweets with emoticons. In the message-level task, the\\nlexicon-based features provided a gain of 5 F-score points over all others.\\nBoth of our systems can be replicated us available resources.\\n',\n",
              " '  Even though considerable attention has been given to the polarity of words\\n(positive and negative) and the creation of large polarity lexicons, research\\nin emotion analysis has had to rely on limited and small emotion lexicons. In\\nthis paper we show how the combined strength and wisdom of the crowds can be\\nused to generate a large, high-quality, word-emotion and word-polarity\\nassociation lexicon quickly and inexpensively. We enumerate the challenges in\\nemotion annotation in a crowdsourcing scenario and propose solutions to address\\nthem. Most notably, in addition to questions about emotions associated with\\nterms, we show how the inclusion of a word choice question can discourage\\nmalicious data entry, help identify instances where the annotator may not be\\nfamiliar with the target term (allowing us to reject such annotations), and\\nhelp obtain annotations at sense level (rather than at word level). We\\nconducted experiments on how to formulate the emotion-annotation questions, and\\nshow that asking if a term is associated with an emotion leads to markedly\\nhigher inter-annotator agreement than that obtained by asking if a term evokes\\nan emotion.\\n',\n",
              " '  Knowing the degree of semantic contrast between words has widespread\\napplication in natural language processing, including machine translation,\\ninformation retrieval, and dialogue systems. Manually-created lexicons focus on\\nopposites, such as {\\\\rm hot} and {\\\\rm cold}. Opposites are of many kinds such\\nas antipodals, complementaries, and gradable. However, existing lexicons often\\ndo not classify opposites into the different kinds. They also do not explicitly\\nlist word pairs that are not opposites but yet have some degree of contrast in\\nmeaning, such as {\\\\rm warm} and {\\\\rm cold} or {\\\\rm tropical} and {\\\\rm\\nfreezing}. We propose an automatic method to identify contrasting word pairs\\nthat is based on the hypothesis that if a pair of words, $A$ and $B$, are\\ncontrasting, then there is a pair of opposites, $C$ and $D$, such that $A$ and\\n$C$ are strongly related and $B$ and $D$ are strongly related. (For example,\\nthere exists the pair of opposites {\\\\rm hot} and {\\\\rm cold} such that {\\\\rm\\ntropical} is related to {\\\\rm hot,} and {\\\\rm freezing} is related to {\\\\rm\\ncold}.) We will call this the contrast hypothesis. We begin with a large\\ncrowdsourcing experiment to determine the amount of human agreement on the\\nconcept of oppositeness and its different kinds. In the process, we flesh out\\nkey features of different kinds of opposites. We then present an automatic and\\nempirical measure of lexical contrast that relies on the contrast hypothesis,\\ncorpus statistics, and the structure of a {\\\\it Roget}-like thesaurus. We show\\nthat the proposed measure of lexical contrast obtains high precision and large\\ncoverage, outperforming existing methods.\\n',\n",
              " '  In this work, we compare two simple methods of tagging scientific\\npublications with labels reflecting their content. As a first source of labels\\nWikipedia is employed, second label set is constructed from the noun phrases\\noccurring in the analyzed corpus. We examine the statistical properties and the\\neffectiveness of both approaches on the dataset consisting of abstracts from\\n0.7 million of scientific documents deposited in the ArXiv preprint collection.\\nWe believe that obtained tags can be later on applied as useful document\\nfeatures in various machine learning tasks (document similarity, clustering,\\ntopic modelling, etc.).\\n',\n",
              " '  The integration of lexical semantics and pragmatics in the analysis of the\\nmeaning of natural lan- guage has prompted changes to the global framework\\nderived from Montague. In those works, the original lexicon, in which words\\nwere assigned an atomic type of a single-sorted logic, has been re- placed by a\\nset of many-facetted lexical items that can compose their meaning with salient\\ncontextual properties using a rich typing system as a guide. Having related our\\nproposal for such an expanded framework \\\\LambdaTYn, we present some recent\\nadvances in the logical formalisms associated, including constraints on lexical\\ntransformations and polymorphic quantifiers, and ongoing discussions and\\nresearch on the granularity of the type system and the limits of transitivity.\\n',\n",
              " \"  We present an open-domain Question-Answering system that learns to answer\\nquestions based on successful past interactions. We follow a pattern-based\\napproach to Answer-Extraction, where (lexico-syntactic) patterns that relate a\\nquestion to its answer are automatically learned and used to answer future\\nquestions. Results show that our approach contributes to the system's best\\nperformance when it is conjugated with typical Answer-Extraction strategies.\\nMoreover, it allows the system to learn with the answered questions and to\\nrectify wrong or unsolved past questions.\\n\",\n",
              " '  This paper considers the problem for estimating the quality of machine\\ntranslation outputs which are independent of human intervention and are\\ngenerally addressed using machine learning techniques.There are various\\nmeasures through which a machine learns translations quality. Automatic\\nEvaluation metrics produce good co-relation at corpus level but cannot produce\\nthe same results at the same segment or sentence level. In this paper 16\\nfeatures are extracted from the input sentences and their translations and a\\nquality score is obtained based on Bayesian inference produced from training\\ndata.\\n',\n",
              " \"  We show that the Zipf's law for Chinese characters perfectly holds for\\nsufficiently short texts (few thousand different characters). The scenario of\\nits validity is similar to the Zipf's law for words in short English texts. For\\nlong Chinese texts (or for mixtures of short Chinese texts), rank-frequency\\nrelations for Chinese characters display a two-layer, hierarchic structure that\\ncombines a Zipfian power-law regime for frequent characters (first layer) with\\nan exponential-like regime for less frequent characters (second layer). For\\nthese two layers we provide different (though related) theoretical descriptions\\nthat include the range of low-frequency characters (hapax legomena). The\\ncomparative analysis of rank-frequency relations for Chinese characters versus\\nEnglish words illustrates the extent to which the characters play for Chinese\\nwriters the same role as the words for those writing within alphabetical\\nsystems.\\n\",\n",
              " '  This document gives a brief description of Korean data prepared for the SPMRL\\n2013 shared task. A total of 27,363 sentences with 350,090 tokens are used for\\nthe shared task. All constituent trees are collected from the KAIST Treebank\\nand transformed to the Penn Treebank style. All dependency trees are converted\\nfrom the transformed constituent trees using heuristics and labeling rules de-\\nsigned specifically for the KAIST Treebank. In addition to the gold-standard\\nmorphological analysis provided by the KAIST Treebank, two sets of automatic\\nmorphological analysis are provided for the shared task, one is generated by\\nthe HanNanum morphological analyzer, and the other is generated by the Sejong\\nmorphological analyzer.\\n',\n",
              " '  It is well known that the length of a syntactic dependency determines its\\nonline memory cost. Thus, the problem of the placement of a head and its\\ndependents (complements or modifiers) that minimizes online memory is\\nequivalent to the problem of the minimum linear arrangement of a star tree.\\nHowever, how that length is translated into cognitive cost is not known. This\\nstudy shows that the online memory cost is minimized when the head is placed at\\nthe center, regardless of the function that transforms length into cost,\\nprovided only that this function is strictly monotonically increasing. Online\\nmemory defines a quasi-convex adaptive landscape with a single central minimum\\nif the number of elements is odd and two central minima if that number is even.\\nWe discuss various aspects of the dynamics of word order of subject (S), verb\\n(V) and object (O) from a complex systems perspective and suggest that word\\norders tend to evolve by swapping adjacent constituents from an initial or\\nearly SOV configuration that is attracted towards a central word order by\\nonline memory minimization. We also suggest that the stability of SVO is due to\\nat least two factors, the quasi-convex shape of the adaptive landscape in the\\nonline memory dimension and online memory adaptations that avoid regression to\\nSOV. Although OVS is also optimal for placing the verb at the center, its low\\nfrequency is explained by its long distance to the seminal SOV in the\\npermutation space.\\n',\n",
              " '  UNL system is designed and implemented by a nonprofit organization, UNDL\\nFoundation at Geneva in 1999. UNL applications are application softwares that\\nallow end users to accomplish natural language tasks, such as translating,\\nsummarizing, retrieving or extracting information, etc. Two major web based\\napplication softwares are Interactive ANalyzer (IAN), which is a natural\\nlanguage analysis system. It represents natural language sentences as semantic\\nnetworks in the UNL format. Other application software is dEep-to-sUrface\\nGENErator (EUGENE), which is an open-source interactive NLizer. It generates\\nnatural language sentences out of semantic networks represented in the UNL\\nformat. In this paper, NLization framework with EUGENE is focused, while using\\nUNL system for accomplishing the task of machine translation. In whole\\nNLization process, EUGENE takes a UNL input and delivers an output in natural\\nlanguage without any human intervention. It is language-independent and has to\\nbe parametrized to the natural language input through a dictionary and a\\ngrammar, provided as separate interpretable files. In this paper, it is\\nexplained that how UNL input is syntactically and semantically analyzed with\\nthe UNL-NL T-Grammar for NLization of UNL sentences involving verbs, pronouns\\nand determiners for Punjabi natural language.\\n',\n",
              " '  Textual sentiment analysis and emotion detection consists in retrieving the\\nsentiment or emotion carried by a text or document. This task can be useful in\\nmany domains: opinion mining, prediction, feedbacks, etc. However, building a\\ngeneral purpose tool for doing sentiment analysis and emotion detection raises\\na number of issues, theoretical issues like the dependence to the domain or to\\nthe language but also pratical issues like the emotion representation for\\ninteroperability. In this paper we present our sentiment/emotion analysis\\ntools, the way we propose to circumvent the di culties and the applications\\nthey are used for.\\n',\n",
              " '  To mine large digital libraries in humanistically meaningful ways, scholars\\nneed to divide them by genre. This is a task that classification algorithms are\\nwell suited to assist, but they need adjustment to address the specific\\nchallenges of this domain. Digital libraries pose two problems of scale not\\nusually found in the article datasets used to test these algorithms. 1) Because\\nlibraries span several centuries, the genres being identified may change\\ngradually across the time axis. 2) Because volumes are much longer than\\narticles, they tend to be internally heterogeneous, and the classification task\\nneeds to begin with segmentation. We describe a multi-layered solution that\\ntrains hidden Markov models to segment volumes, and uses ensembles of\\noverlapping classifiers to address historical change. We test this approach on\\na collection of 469,200 volumes drawn from HathiTrust Digital Library. To\\ndemonstrate the humanistic value of these methods, we extract 32,209 volumes of\\nfiction from the digital library, and trace the changing proportions of first-\\nand third-person narration in the corpus. We note that narrative points of view\\nseem to have strong associations with particular themes and genres.\\n',\n",
              " '  Given appropriate representations of the semantic relations between carpenter\\nand wood and between mason and stone (for example, vectors in a vector space\\nmodel), a suitable algorithm should be able to recognize that these relations\\nare highly similar (carpenter is to wood as mason is to stone; the relations\\nare analogous). Likewise, with representations of dog, house, and kennel, an\\nalgorithm should be able to recognize that the semantic composition of dog and\\nhouse, dog house, is highly similar to kennel (dog house and kennel are\\nsynonymous). It seems that these two tasks, recognizing relations and\\ncompositions, are closely connected. However, up to now, the best models for\\nrelations are significantly different from the best models for compositions. In\\nthis paper, we introduce a dual-space model that unifies these two tasks. This\\nmodel matches the performance of the best previous models for relations and\\ncompositions. The dual-space model consists of a space for measuring domain\\nsimilarity and a space for measuring function similarity. Carpenter and wood\\nshare the same domain, the domain of carpentry. Mason and stone share the same\\ndomain, the domain of masonry. Carpenter and mason share the same function, the\\nfunction of artisans. Wood and stone share the same function, the function of\\nmaterials. In the composition dog house, kennel has some domain overlap with\\nboth dog and house (the domains of pets and buildings). The function of kennel\\nis similar to the function of house (the function of shelters). By combining\\ndomain and function similarities in various ways, we can model relations,\\ncompositions, and other aspects of semantics.\\n',\n",
              " '  Little is known about why SOV order is initially preferred and then discarded\\nor recovered. Here we present a framework for understanding these and many\\nrelated word order phenomena: the diversity of dominant orders, the existence\\nof free words orders, the need of alternative word orders and word order\\nreversions and cycles in evolution. Under that framework, word order is\\nregarded as a multiconstraint satisfaction problem in which at least two\\nconstraints are in conflict: online memory minimization and maximum\\npredictability.\\n',\n",
              " '  Dictionaries and phrase tables are the basis of modern statistical machine\\ntranslation systems. This paper develops a method that can automate the process\\nof generating and extending dictionaries and phrase tables. Our method can\\ntranslate missing word and phrase entries by learning language structures based\\non large monolingual data and mapping between languages from small bilingual\\ndata. It uses distributed representation of words and learns a linear mapping\\nbetween vector spaces of languages. Despite its simplicity, our method is\\nsurprisingly effective: we can achieve almost 90% precision@5 for translation\\nof words between English and Spanish. This method makes little assumption about\\nthe languages, so it can be used to extend and refine dictionaries and\\ntranslation tables for any language pairs.\\n',\n",
              " '  Learning word representations has recently seen much success in computational\\nlinguistics. However, assuming sequences of word tokens as input to linguistic\\nanalysis is often unjustified. For many languages word segmentation is a\\nnon-trivial task and naturally occurring text is sometimes a mixture of natural\\nlanguage strings and other character data. We propose to learn text\\nrepresentations directly from raw character sequences by training a Simple\\nrecurrent Network to predict the next character in text. The network uses its\\nhidden layer to evolve abstract representations of the character sequences it\\nsees. To demonstrate the usefulness of the learned text embeddings, we use them\\nas features in a supervised character level text segmentation and labeling\\ntask: recognizing spans of text containing programming language code. By using\\nthe embeddings as features we are able to substantially improve over a baseline\\nwhich uses only surface character n-grams.\\n',\n",
              " '  EuroVoc (2012) is a highly multilingual thesaurus consisting of over 6,700\\nhierarchically organised subject domains used by European Institutions and many\\nauthorities in Member States of the European Union (EU) for the classification\\nand retrieval of official documents. JEX is JRC-developed multi-label\\nclassification software that learns from manually labelled data to\\nautomatically assign EuroVoc descriptors to new documents in a profile-based\\ncategory-ranking task. The JEX release consists of trained classifiers for 22\\nofficial EU languages, of parallel training data in the same languages, of an\\ninterface that allows viewing and amending the assignment results, and of a\\nmodule that allows users to re-train the tool on their own document\\ncollections. JEX allows advanced users to change the document representation so\\nas to possibly improve the categorisation result through linguistic\\npre-processing. JEX can be used as a tool for interactive EuroVoc descriptor\\nassignment to increase speed and consistency of the human categorisation\\nprocess, or it can be used fully automatically. The output of JEX is a\\nlanguage-independent EuroVoc feature vector lending itself also as input to\\nvarious other Language Technology tasks, including cross-lingual clustering and\\nclassification, cross-lingual plagiarism detection, sentence selection and\\nranking, and more.\\n',\n",
              " \"  The European Commission's (EC) Directorate General for Translation, together\\nwith the EC's Joint Research Centre, is making available a large translation\\nmemory (TM; i.e. sentences and their professionally produced translations)\\ncovering twenty-two official European Union (EU) languages and their 231\\nlanguage pairs. Such a resource is typically used by translation professionals\\nin combination with TM software to improve speed and consistency of their\\ntranslations. However, this resource has also many uses for translation studies\\nand for language technology applications, including Statistical Machine\\nTranslation (SMT), terminology extraction, Named Entity Recognition (NER),\\nmultilingual classification and clustering, and many more. In this reference\\npaper for DGT-TM, we introduce this new resource, provide statistics regarding\\nits size, and explain how it was produced and how to use it.\\n\",\n",
              " '  Most large organizations have dedicated departments that monitor the media to\\nkeep up-to-date with relevant developments and to keep an eye on how they are\\nrepresented in the news. Part of this media monitoring work can be automated.\\nIn the European Union with its 23 official languages, it is particularly\\nimportant to cover media reports in many languages in order to capture the\\ncomplementary news content published in the different countries. It is also\\nimportant to be able to access the news content across languages and to merge\\nthe extracted information. We present here the four publicly accessible systems\\nof the Europe Media Monitor (EMM) family of applications, which cover between\\n19 and 50 languages (see http://press.jrc.it/overview.html). We give an\\noverview of their functionality and discuss some of the implications of the\\nfact that they cover quite so many languages. We discuss design issues\\nnecessary to be able to achieve this high multilinguality, as well as the\\nbenefits of this multilinguality.\\n',\n",
              " \"  The motor theory of speech perception holds that we perceive the speech of\\nanother in terms of a motor representation of that speech. However, when we\\nhave learned to recognize a foreign accent, it seems plausible that recognition\\nof a word rarely involves reconstruction of the speech gestures of the speaker\\nrather than the listener. To better assess the motor theory and this\\nobservation, we proceed in three stages. Part 1 places the motor theory of\\nspeech perception in a larger framework based on our earlier models of the\\nadaptive formation of mirror neurons for grasping, and for viewing extensions\\nof that mirror system as part of a larger system for neuro-linguistic\\nprocessing, augmented by the present consideration of recognizing speech in a\\nnovel accent. Part 2 then offers a novel computational model of how a listener\\ncomes to understand the speech of someone speaking the listener's native\\nlanguage with a foreign accent. The core tenet of the model is that the\\nlistener uses hypotheses about the word the speaker is currently uttering to\\nupdate probabilities linking the sound produced by the speaker to phonemes in\\nthe native language repertoire of the listener. This, on average, improves the\\nrecognition of later words. This model is neutral regarding the nature of the\\nrepresentations it uses (motor vs. auditory). It serve as a reference point for\\nthe discussion in Part 3, which proposes a dual-stream neuro-linguistic\\narchitecture to revisits claims for and against the motor theory of speech\\nperception and the relevance of mirror neurons, and extracts some implications\\nfor the reframing of the motor theory.\\n\",\n",
              " '  Colour is a key component in the successful dissemination of information.\\nSince many real-world concepts are associated with colour, for example danger\\nwith red, linguistic information is often complemented with the use of\\nappropriate colours in information visualization and product marketing. Yet,\\nthere is no comprehensive resource that captures concept-colour associations.\\nWe present a method to create a large word-colour association lexicon by\\ncrowdsourcing. A word-choice question was used to obtain sense-level\\nannotations and to ensure data quality. We focus especially on abstract\\nconcepts and emotions to show that even they tend to have strong colour\\nassociations. Thus, using the right colours can not only improve semantic\\ncoherence, but also inspire the desired emotional response.\\n',\n",
              " '  The Linguistic Data Consortium (LDC) has developed hundreds of data corpora\\nfor natural language processing (NLP) research. Among these are a number of\\nannotated treebank corpora for Arabic. Typically, these corpora consist of a\\nsingle collection of annotated documents. NLP research, however, usually\\nrequires multiple data sets for the purposes of training models, developing\\ntechniques, and final evaluation. Therefore it becomes necessary to divide the\\ncorpora used into the required data sets (divisions). This document details a\\nset of rules that have been defined to enable consistent divisions for old and\\nnew Arabic treebanks (ATB) and related corpora.\\n',\n",
              " \"  In this paper, a new hybrid algorithm which combines both of token-based and\\ncharacter-based approaches is presented. The basic Levenshtein approach has\\nbeen extended to token-based distance metric. The distance metric is enhanced\\nto set the proper granularity level behavior of the algorithm. It smoothly maps\\na threshold of misspellings differences at the character level, and the\\nimportance of token level errors in terms of token's position and frequency.\\nUsing a large Arabic dataset, the experimental results show that the proposed\\nalgorithm overcomes successfully many types of errors such as: typographical\\nerrors, omission or insertion of middle name components, omission of\\nnon-significant popular name components, and different writing styles character\\nvariations. When compared the results with other classical algorithms, using\\nthe same dataset, the proposed algorithm was found to increase the minimum\\nsuccess level of best tested algorithms, while achieving higher upper limits .\\n\",\n",
              " \"  Assigning a positive or negative score to a word out of context (i.e. a\\nword's prior polarity) is a challenging task for sentiment analysis. In the\\nliterature, various approaches based on SentiWordNet have been proposed. In\\nthis paper, we compare the most often used techniques together with newly\\nproposed ones and incorporate all of them in a learning framework to see\\nwhether blending them can further improve the estimation of prior polarity\\nscores. Using two different versions of SentiWordNet and testing regression and\\nclassification models across tasks and datasets, our learning approach\\nconsistently outperforms the single metrics, providing a new state-of-the-art\\napproach in computing words' prior polarity for sentiment analysis. We conclude\\nour investigation showing interesting biases in calculated prior polarity\\nscores when word Part of Speech and annotator gender are considered.\\n\",\n",
              " \"  Today we have access to unprecedented amounts of literary texts. However,\\nsearch still relies heavily on key words. In this paper, we show how sentiment\\nanalysis can be used in tandem with effective visualizations to quantify and\\ntrack emotions in both individual books and across very large collections. We\\nintroduce the concept of emotion word density, and using the Brothers Grimm\\nfairy tales as example, we show how collections of text can be organized for\\nbetter search. Using the Google Books Corpus we show how to determine an\\nentity's emotion associations from co-occurring words. Finally, we compare\\nemotion words in fairy tales and novels, to show that fairy tales have a much\\nwider range of emotion word densities than novels.\\n\",\n",
              " '  Since many real-world concepts are associated with colour, for example danger\\nwith red, linguistic information is often complimented with the use of\\nappropriate colours in information visualization and product marketing. Yet,\\nthere is no comprehensive resource that captures concept-colour associations.\\nWe present a method to create a large word-colour association lexicon by\\ncrowdsourcing. We focus especially on abstract concepts and emotions to show\\nthat even though they cannot be physically visualized, they too tend to have\\nstrong colour associations. Finally, we show how word-colour associations\\nmanifest themselves in language, and quantify usefulness of co-occurrence and\\npolarity cues in automatically detecting colour associations.\\n',\n",
              " '  This paper describes a new, freely available, highly multilingual named\\nentity resource for person and organisation names that has been compiled over\\nseven years of large-scale multilingual news analysis combined with Wikipedia\\nmining, resulting in 205,000 per-son and organisation names plus about the same\\nnumber of spelling variants written in over 20 different scripts and in many\\nmore languages. This resource, produced as part of the Europe Media Monitor\\nactivity (EMM, http://emm.newsbrief.eu/overview.html), can be used for a number\\nof purposes. These include improving name search in databases or on the\\ninternet, seeding machine learning systems to learn named entity recognition\\nrules, improve machine translation results, and more. We describe here how this\\nresource was created; we give statistics on its current size; we address the\\nissue of morphological inflection; and we give details regarding its\\nfunctionality. Updates to this resource will be made available daily.\\n',\n",
              " '  In this paper, we first present a new variant of Gaussian restricted\\nBoltzmann machine (GRBM) called multivariate Gaussian restricted Boltzmann\\nmachine (MGRBM), with its definition and learning algorithm. Then we propose\\nusing a learned GRBM or MGRBM to extract better features for robust speech\\nrecognition. Our experiments on Aurora2 show that both GRBM-extracted and\\nMGRBM-extracted feature performs much better than Mel-frequency cepstral\\ncoefficient (MFCC) with either HMM-GMM or hybrid HMM-deep neural network (DNN)\\nacoustic model, and MGRBM-extracted feature is slightly better.\\n',\n",
              " '  We are presenting work on recognising acronyms of the form Long-Form\\n(Short-Form) such as \"International Monetary Fund (IMF)\" in millions of news\\narticles in twenty-two languages, as part of our more general effort to\\nrecognise entities and their variants in news text and to use them for the\\nautomatic analysis of the news, including the linking of related news across\\nlanguages. We show how the acronym recognition patterns, initially developed\\nfor medical terms, needed to be adapted to the more general news domain and we\\npresent evaluation results. We describe our effort to automatically merge the\\nnumerous long-form variants referring to the same short-form, while keeping\\nnon-related long-forms separate. Finally, we provide extensive statistics on\\nthe frequency and the distribution of short-form/long-form pairs across\\nlanguages.\\n',\n",
              " '  Recent years have brought a significant growth in the volume of research in\\nsentiment analysis, mostly on highly subjective text types (movie or product\\nreviews). The main difference these texts have with news articles is that their\\ntarget is clearly defined and unique across the text. Following different\\nannotation efforts and the analysis of the issues encountered, we realised that\\nnews opinion mining is different from that of other text types. We identified\\nthree subtasks that need to be addressed: definition of the target; separation\\nof the good and bad news content from the good and bad sentiment expressed on\\nthe target; and analysis of clearly marked opinion that is expressed\\nexplicitly, not needing interpretation or the use of world knowledge.\\nFurthermore, we distinguish three different possible views on newspaper\\narticles - author, reader and text, which have to be addressed differently at\\nthe time of analysing sentiment. Given these definitions, we present work on\\nmining opinions about entities in English language news, in which (a) we test\\nthe relative suitability of various sentiment dictionaries and (b) we attempt\\nto separate positive or negative opinion from good or bad news. In the\\nexperiments described here, we tested whether or not subject domain-defining\\nvocabulary should be ignored. Results showed that this idea is more appropriate\\nin the context of news opinion mining and that the approaches taking this into\\nconsideration produce a better performance.\\n',\n",
              " '  With the widespread use of email, we now have access to unprecedented amounts\\nof text that we ourselves have written. In this paper, we show how sentiment\\nanalysis can be used in tandem with effective visualizations to quantify and\\ntrack emotions in many types of mail. We create a large word--emotion\\nassociation lexicon by crowdsourcing, and use it to compare emotions in love\\nletters, hate mail, and suicide notes. We show that there are marked\\ndifferences across genders in how they use emotion words in work-place email.\\nFor example, women use many words from the joy--sadness axis, whereas men\\nprefer terms from the fear--trust axis. Finally, we show visualizations that\\ncan help people track emotions in their emails.\\n',\n",
              " '  Past work on personality detection has shown that frequency of lexical\\ncategories such as first person pronouns, past tense verbs, and sentiment words\\nhave significant correlations with personality traits. In this paper, for the\\nfirst time, we show that fine affect (emotion) categories such as that of\\nexcitement, guilt, yearning, and admiration are significant indicators of\\npersonality. Additionally, we perform experiments to show that the gains\\nprovided by the fine affect categories are not obtained by using coarse affect\\ncategories alone or with specificity features alone. We employ these features\\nin five SVM classifiers for detecting five personality traits through essays.\\nWe find that the use of fine emotion features leads to statistically\\nsignificant improvement over a competitive baseline, whereas the use of coarse\\naffect and specificity features does not.\\n',\n",
              " '  Ontologies are considered as the backbone of the Semantic Web. With the\\nrising success of the Semantic Web, the number of participating communities\\nfrom different countries is constantly increasing. The growing number of\\nontologies available in different natural languages leads to an\\ninteroperability problem. In this paper, we discuss several approaches for\\nontology matching; examine similarities and differences, identify weaknesses,\\nand compare the existing automated approaches with the manual approaches for\\nintegrating multilingual ontologies. In addition to that, we propose a new\\narchitecture for a multilingual ontology matching service. As a case study we\\nused an example of two multilingual enterprise ontologies - the university\\nontology of Freie Universitaet Berlin and the ontology for Fayoum University in\\nEgypt.\\n',\n",
              " '  This paper focuses on the automatic extraction of domain-specific sentiment\\nword (DSSW), which is a fundamental subtask of sentiment analysis. Most\\nprevious work utilizes manual patterns for this task. However, the performance\\nof those methods highly relies on the labelled patterns or selected seeds. In\\norder to overcome the above problem, this paper presents an automatic framework\\nto detect large-scale domain-specific patterns for DSSW extraction. To this\\nend, sentiment seeds are extracted from massive dataset of user comments.\\nSubsequently, these sentiment seeds are expanded by synonyms using a\\nbootstrapping mechanism. Simultaneously, a synonymy graph is built and the\\ngraph propagation algorithm is applied on the built synonymy graph. Afterwards,\\nsyntactic and sequential relations between target words and high-ranked\\nsentiment words are extracted automatically to construct large-scale patterns,\\nwhich are further used to extracte DSSWs. The experimental results in three\\ndomains reveal the effectiveness of our method.\\n',\n",
              " '  A balanced speech corpus is the basic need for any speech processing task. In\\nthis report we describe our effort on development of Assamese speech corpus. We\\nmainly focused on some issues and challenges faced during development of the\\ncorpus. Being a less computationally aware language, this is the first effort\\nto develop speech corpus for Assamese. As corpus development is an ongoing\\nprocess, in this paper we report only the initial task.\\n',\n",
              " \"  This paper describes the R package crqa to perform cross-recurrence\\nquantification analysis of two time series of either a categorical or\\ncontinuous nature. Streams of behavioral information, from eye movements to\\nlinguistic elements, unfold over time. When two people interact, such as in\\nconversation, they often adapt to each other, leading these behavioral levels\\nto exhibit recurrent states. In dialogue, for example, interlocutors adapt to\\neach other by exchanging interactive cues: smiles, nods, gestures, choice of\\nwords, and so on. In order for us to capture closely the goings-on of dynamic\\ninteraction, and uncover the extent of coupling between two individuals, we\\nneed to quantify how much recurrence is taking place at these levels. Methods\\navailable in crqa would allow researchers in cognitive science to pose such\\nquestions as how much are two people recurrent at some level of analysis, what\\nis the characteristic lag time for one person to maximally match another, or\\nwhether one person is leading another. First, we set the theoretical ground to\\nunderstand the difference between 'correlation' and 'co-visitation' when\\ncomparing two time series, using an aggregative or cross-recurrence approach.\\nThen, we describe more formally the principles of cross-recurrence, and show\\nwith the current package how to carry out analyses applying them. We end the\\npaper by comparing computational efficiency, and results' consistency, of crqa\\nR package, with the benchmark MATLAB toolbox crptoolbox. We show perfect\\ncomparability between the two libraries on both levels.\\n\",\n",
              " '  This paper presents a novel approach to machine translation by combining the\\nstate of art name entity translation scheme. Improper translation of name\\nentities lapse the quality of machine translated output. In this work, name\\nentities are transliterated by using statistical rule based approach. This\\npaper describes the translation and transliteration of name entities from\\nEnglish to Punjabi. We have experimented on four types of name entities which\\nare: Proper names, Location names, Organization names and miscellaneous.\\nVarious rules for the purpose of syllabification have been constructed.\\nTransliteration of name entities is accomplished with the help of Probability\\ncalculation. N-Gram probabilities for the extracted syllables have been\\ncalculated using statistical machine translation toolkit MOSES.\\n',\n",
              " '  Part-of-speech (POS) tagging is a process of assigning the words in a text\\ncorresponding to a particular part of speech. A fundamental version of POS\\ntagging is the identification of words as nouns, verbs, adjectives etc. For\\nprocessing natural languages, Part of Speech tagging is a prominent tool. It is\\none of the simplest as well as most constant and statistical model for many NLP\\napplications. POS Tagging is an initial stage of linguistics, text analysis\\nlike information retrieval, machine translator, text to speech synthesis,\\ninformation extraction etc. In POS Tagging we assign a Part of Speech tag to\\neach word in a sentence and literature. Various approaches have been proposed\\nto implement POS taggers. In this paper we present a Marathi part of speech\\ntagger. It is morphologically rich language. Marathi is spoken by the native\\npeople of Maharashtra. The general approach used for development of tagger is\\nstatistical using Unigram, Bigram, Trigram and HMM Methods. It presents a clear\\nidea about all the algorithms with suitable examples. It also introduces a tag\\nset for Marathi which can be used for tagging Marathi text. In this paper we\\nhave shown the development of the tagger as well as compared to check the\\naccuracy of taggers output. The three Marathi POS taggers viz. Unigram, Bigram,\\nTrigram and HMM gives the accuracy of 77.38%, 90.30%, 91.46% and 93.82%\\nrespectively.\\n',\n",
              " '  Machine translation is research based area where evaluation is very important\\nphenomenon for checking the quality of MT output. The work is based on the\\nevaluation of English to Urdu Machine translation. In this research work we\\nhave evaluated the translation quality of Urdu language which has been\\ntranslated by using different Machine Translation systems like Google, Babylon\\nand Ijunoon. The evaluation process is done by using two approaches - Human\\nevaluation and Automatic evaluation. We have worked for both the approaches\\nwhere in human evaluation emphasis is given to scales and parameters while in\\nautomatic evaluation emphasis is given to some automatic metric such as BLEU,\\nGTM, METEOR and ATEC.\\n',\n",
              " '  Urdu is a combination of several languages like Arabic, Hindi, English,\\nTurkish, Sanskrit etc. It has a complex and rich morphology. This is the reason\\nwhy not much work has been done in Urdu language processing. Stemming is used\\nto convert a word into its respective root form. In stemming, we separate the\\nsuffix and prefix from the word. It is useful in search engines, natural\\nlanguage processing and word processing, spell checkers, word parsing, word\\nfrequency and count studies. This paper presents a rule based stemmer for Urdu.\\nThe stemmer that we have discussed here is used in information retrieval. We\\nhave also evaluated our results by verifying it with a human expert.\\n',\n",
              " '  Stemming is the process of extracting root word from the given inflection\\nword and also plays significant role in numerous application of Natural\\nLanguage Processing (NLP). Tamil Language raises several challenges to NLP,\\nsince it has rich morphological patterns than other languages. The rule based\\napproach light-stemmer is proposed in this paper, to find stem word for given\\ninflection Tamil word. The performance of proposed approach is compared to a\\nrule based suffix removal stemmer based on correctly and incorrectly predicted.\\nThe experimental result clearly show that the proposed approach light stemmer\\nfor Tamil language perform better than suffix removal stemmer and also more\\neffective in Information Retrieval System (IRS).\\n',\n",
              " '  Semantic measures are widely used today to estimate the strength of the\\nsemantic relationship between elements of various types: units of language\\n(e.g., words, sentences, documents), concepts or even instances semantically\\ncharacterized (e.g., diseases, genes, geographical locations). Semantic\\nmeasures play an important role to compare such elements according to semantic\\nproxies: texts and knowledge representations, which support their meaning or\\ndescribe their nature. Semantic measures are therefore essential for designing\\nintelligent agents which will for example take advantage of semantic analysis\\nto mimic human ability to compare abstract or concrete objects. This paper\\nproposes a comprehensive survey of the broad notion of semantic measure for the\\ncomparison of units of language, concepts or instances based on semantic proxy\\nanalyses. Semantic measures generalize the well-known notions of semantic\\nsimilarity, semantic relatedness and semantic distance, which have been\\nextensively studied by various communities over the last decades (e.g.,\\nCognitive Sciences, Linguistics, and Artificial Intelligence to mention a few).\\n',\n",
              " '  Word Sense Disambiguation (WSD), the process of automatically identifying the\\nmeaning of a polysemous word in a sentence, is a fundamental task in Natural\\nLanguage Processing (NLP). Progress in this approach to WSD opens up many\\npromising developments in the field of NLP and its applications. Indeed,\\nimprovement over current performance levels could allow us to take a first step\\ntowards natural language understanding. Due to the lack of lexical resources it\\nis sometimes difficult to perform WSD for under-resourced languages. This paper\\nis an investigation on how to initiate research in WSD for under-resourced\\nlanguages by applying Word Sense Induction (WSI) and suggests some interesting\\ntopics to focus on.\\n',\n",
              " '  This paper discusses the dominancy of local features (LFs), as input to the\\nmultilayer neural network (MLN), extracted from a Bangla input speech over mel\\nfrequency cepstral coefficients (MFCCs). Here, LF-based method comprises three\\nstages: (i) LF extraction from input speech, (ii) phoneme probabilities\\nextraction using MLN from LF and (iii) the hidden Markov model (HMM) based\\nclassifier to obtain more accurate phoneme strings. In the experiments on\\nBangla speech corpus prepared by us, it is observed that the LFbased automatic\\nspeech recognition (ASR) system provides higher phoneme correct rate than the\\nMFCC-based system. Moreover, the proposed system requires fewer mixture\\ncomponents in the HMMs.\\n',\n",
              " '  Active languages such as Bangla (or Bengali) evolve over time due to a\\nvariety of social, cultural, economic, and political issues. In this paper, we\\nanalyze the change in the written form of the modern phase of Bangla\\nquantitatively in terms of character-level, syllable-level, morpheme-level and\\nword-level features. We collect three different types of corpora---classical,\\nnewspapers and blogs---and test whether the differences in their features are\\nstatistically significant. Results suggest that there are significant changes\\nin the length of a word when measured in terms of characters, but there is not\\nmuch difference in usage of different characters, syllables and morphemes in a\\nword or of different words in a sentence. To the best of our knowledge, this is\\nthe first work on Bangla of this kind.\\n',\n",
              " '  We consider a multilingual weakly supervised learning scenario where\\nknowledge from annotated corpora in a resource-rich language is transferred via\\nbitext to guide the learning in other languages. Past approaches project labels\\nacross bitext and use them as features or gold labels for training. We propose\\na new method that projects model expectations rather than labels, which\\nfacilities transfer of model uncertainty across language boundaries. We encode\\nexpectations as constraints and train a discriminative CRF model using\\nGeneralized Expectation Criteria (Mann and McCallum, 2010). Evaluated on\\nstandard Chinese-English and German-English NER datasets, our method\\ndemonstrates F1 scores of 64% and 60% when no labeled data is used. Attaining\\nthe same accuracy with supervised CRFs requires 12k and 1.5k labeled sentences.\\nFurthermore, when combined with labeled examples, our method yields significant\\nimprovements over state-of-the-art supervised methods, achieving best reported\\nnumbers to date on Chinese OntoNotes and German CoNLL-03 datasets.\\n',\n",
              " '  We begin by introducing the Computer Science branch of Natural Language\\nProcessing, then narrowing the attention on its subbranch of Information\\nExtraction and particularly on Named Entity Recognition, discussing briefly its\\nmain methodological approaches. It follows an introduction to state-of-the-art\\nConditional Random Fields under the form of linear chains. Subsequently, the\\nidea of constrained inference as a way to model long-distance relationships in\\na text is presented, based on an Integer Linear Programming representation of\\nthe problem. Adding such relationships to the problem as automatically inferred\\nlogical formulas, translatable into linear conditions, we propose to solve the\\nresulting more complex problem with the aid of Lagrangian relaxation, of which\\nsome technical details are explained. Lastly, we give some experimental\\nresults.\\n',\n",
              " '  ARKref is a tool for noun phrase coreference. It is a deterministic,\\nrule-based system that uses syntactic information from a constituent parser,\\nand semantic information from an entity recognition component. Its architecture\\nis based on the work of Haghighi and Klein (2009). ARKref was originally\\nwritten in 2009. At the time of writing, the last released version was in March\\n2011. This document describes that version, which is open-source and publicly\\navailable at: http://www.ark.cs.cmu.edu/ARKref\\n',\n",
              " \"  We propose an extension of Stabler's version of clitics treatment for a wider\\ncoverage of the French language. For this, we present the lexical entries\\nneeded in the lexicon. Then, we show the recognition of complex syntactic\\nphenomena as (left and right) dislo- cation, clitic climbing over modal and\\nextraction from determiner phrase. The aim of this presentation is the\\nsyntax-semantic interface for clitics analyses in which we will stress on\\nclitic climbing over verb and raising verb.\\n\",\n",
              " '  The recently introduced continuous Skip-gram model is an efficient method for\\nlearning high-quality distributed vector representations that capture a large\\nnumber of precise syntactic and semantic word relationships. In this paper we\\npresent several extensions that improve both the quality of the vectors and the\\ntraining speed. By subsampling of the frequent words we obtain significant\\nspeedup and also learn more regular word representations. We also describe a\\nsimple alternative to the hierarchical softmax called negative sampling. An\\ninherent limitation of word representations is their indifference to word order\\nand their inability to represent idiomatic phrases. For example, the meanings\\nof \"Canada\" and \"Air\" cannot be easily combined to obtain \"Air Canada\".\\nMotivated by this example, we present a simple method for finding phrases in\\ntext, and show that learning good vector representations for millions of\\nphrases is possible.\\n',\n",
              " '  We present the architecture and the evaluation of a new system for\\nrecognizing textual entailment (RTE). In RTE we want to identify automatically\\nthe type of a logical relation between two input texts. In particular, we are\\ninterested in proving the existence of an entailment between them. We conceive\\nour system as a modular environment allowing for a high-coverage syntactic and\\nsemantic text analysis combined with logical inference. For the syntactic and\\nsemantic analysis we combine a deep semantic analysis with a shallow one\\nsupported by statistical models in order to increase the quality and the\\naccuracy of results. For RTE we use logical inference of first-order employing\\nmodel-theoretic techniques and automated reasoning tools. The inference is\\nsupported with problem-relevant background knowledge extracted automatically\\nand on demand from external sources like, e.g., WordNet, YAGO, and OpenCyc, or\\nother, more experimental sources with, e.g., manually defined presupposition\\nresolutions, or with axiomatized general and common sense knowledge. The\\nresults show that fine-grained and consistent knowledge coming from diverse\\nsources is a necessary condition determining the correctness and traceability\\nof results.\\n',\n",
              " '  Vocabulary learning by children can be characterized by many biases. When\\nencountering a new word, children as well as adults, are biased towards\\nassuming that it means something totally different from the words that they\\nalready know. To the best of our knowledge, the 1st mathematical proof of the\\noptimality of this bias is presented here. First, it is shown that this bias is\\na particular case of the maximization of mutual information between words and\\nmeanings. Second, the optimality is proven within a more general information\\ntheoretic framework where mutual information maximization competes with other\\ninformation theoretic principles. The bias is a prediction from modern\\ninformation theory. The relationship between information theoretic principles\\nand the principles of contrast and mutual exclusivity is also shown.\\n',\n",
              " '  This paper describes the corpus of sockpuppet cases we gathered from\\nWikipedia. A sockpuppet is an online user account created with a fake identity\\nfor the purpose of covering abusive behavior and/or subverting the editing\\nregulation process. We used a semi-automated method for crawling and curating a\\ndataset of real sockpuppet investigation cases. To the best of our knowledge,\\nthis is the first corpus available on real-world deceptive writing. We describe\\nthe process for crawling the data and some preliminary results that can be used\\nas baseline for benchmarking research. The dataset will be released under a\\nCreative Commons license from our project website: http://docsig.cis.uab.edu.\\n',\n",
              " '  In recent years, semantic similarity measure has a great interest in Semantic\\nWeb and Natural Language Processing (NLP). Several similarity measures have\\nbeen developed, being given the existence of a structured knowledge\\nrepresentation offered by ontologies and corpus which enable semantic\\ninterpretation of terms. Semantic similarity measures compute the similarity\\nbetween concepts/terms included in knowledge sources in order to perform\\nestimations. This paper discusses the existing semantic similarity methods\\nbased on structure, information content and feature approaches. Additionally,\\nwe present a critical evaluation of several categories of semantic similarity\\napproaches based on two standard benchmarks. The aim of this paper is to give\\nan efficient evaluation of all these measures which help researcher and\\npractitioners to select the measure that best fit for their requirements.\\n',\n",
              " '  Sentiment polarity classification is perhaps the most widely studied topic.\\nIt classifies an opinionated document as expressing a positive or negative\\nopinion. In this paper, using movie review dataset, we perform a comparative\\nstudy with different single kind linguistic features and the combinations of\\nthese features. We find that the classic topic-based classifier(Naive Bayes and\\nSupport Vector Machine) do not perform as well on sentiment polarity\\nclassification. And we find that with some combination of different linguistic\\nfeatures, the classification accuracy can be boosted a lot. We give some\\nreasonable explanations about these boosting outcomes.\\n',\n",
              " '  Principal component analysis (PCA) and related techniques have been\\nsuccessfully employed in natural language processing. Text mining applications\\nin the age of the online social media (OSM) face new challenges due to\\nproperties specific to these use cases (e.g. spelling issues specific to texts\\nposted by users, the presence of spammers and bots, service announcements,\\netc.). In this paper, we employ a Robust PCA technique to separate typical\\noutliers and highly localized topics from the low-dimensional structure present\\nin language use in online social networks. Our focus is on identifying\\ngeospatial features among the messages posted by the users of the Twitter\\nmicroblogging service. Using a dataset which consists of over 200 million\\ngeolocated tweets collected over the course of a year, we investigate whether\\nthe information present in word usage frequencies can be used to identify\\nregional features of language use and topics of interest. Using the PCA pursuit\\nmethod, we are able to identify important low-dimensional features, which\\nconstitute smoothly varying functions of the geographic location.\\n',\n",
              " '  Tweets pertaining to a single event, such as a national election, can number\\nin the hundreds of millions. Automatically analyzing them is beneficial in many\\ndownstream natural language applications such as question answering and\\nsummarization. In this paper, we propose a new task: identifying the purpose\\nbehind electoral tweets--why do people post election-oriented tweets? We show\\nthat identifying purpose is correlated with the related phenomenon of sentiment\\nand emotion detection, but yet significantly different. Detecting purpose has a\\nnumber of applications including detecting the mood of the electorate,\\nestimating the popularity of policies, identifying key issues of contention,\\nand predicting the course of events. We create a large dataset of electoral\\ntweets and annotate a few thousand tweets for purpose. We develop a system that\\nautomatically classifies electoral tweets as per their purpose, obtaining an\\naccuracy of 43.56% on an 11-class task and an accuracy of 73.91% on a 3-class\\ntask (both accuracies well above the most-frequent-class baseline). Finally, we\\nshow that resources developed for emotion detection are also helpful for\\ndetecting purpose.\\n',\n",
              " '  This thesis is about the problem of compositionality in distributional\\nsemantics. Distributional semantics presupposes that the meanings of words are\\na function of their occurrences in textual contexts. It models words as\\ndistributions over these contexts and represents them as vectors in high\\ndimensional spaces. The problem of compositionality for such models concerns\\nitself with how to produce representations for larger units of text by\\ncomposing the representations of smaller units of text.\\n  This thesis focuses on a particular approach to this compositionality\\nproblem, namely using the categorical framework developed by Coecke, Sadrzadeh,\\nand Clark, which combines syntactic analysis formalisms with distributional\\nsemantic representations of meaning to produce syntactically motivated\\ncomposition operations. This thesis shows how this approach can be\\ntheoretically extended and practically implemented to produce concrete\\ncompositional distributional models of natural language semantics. It\\nfurthermore demonstrates that such models can perform on par with, or better\\nthan, other competing approaches in the field of natural language processing.\\n  There are three principal contributions to computational linguistics in this\\nthesis. The first is to extend the DisCoCat framework on the syntactic front\\nand semantic front, incorporating a number of syntactic analysis formalisms and\\nproviding learning procedures allowing for the generation of concrete\\ncompositional distributional models. The second contribution is to evaluate the\\nmodels developed from the procedures presented here, showing that they\\noutperform other compositional distributional models present in the literature.\\nThe third contribution is to show how using category theory to solve linguistic\\nproblems forms a sound basis for research, illustrated by examples of work on\\nthis topic, that also suggest directions for future research.\\n',\n",
              " '  We propose and study a novel supervised approach to learning statistical\\nsemantic relatedness models from subjectively annotated training examples. The\\nproposed semantic model consists of parameterized co-occurrence statistics\\nassociated with textual units of a large background knowledge corpus. We\\npresent an efficient algorithm for learning such semantic models from a\\ntraining sample of relatedness preferences. Our method is corpus independent\\nand can essentially rely on any sufficiently large (unstructured) collection of\\ncoherent texts. Moreover, the approach facilitates the fitting of semantic\\nmodels for specific users or groups of users. We present the results of\\nextensive range of experiments from small to large scale, indicating that the\\nproposed method is effective and competitive with the state-of-the-art.\\n',\n",
              " '  In this paper, we explore a set of novel features for authorship attribution\\nof documents. These features are derived from a word network representation of\\nnatural language text. As has been noted in previous studies, natural language\\ntends to show complex network structure at word level, with low degrees of\\nseparation and scale-free (power law) degree distribution. There has also been\\nwork on authorship attribution that incorporates ideas from complex networks.\\nThe goal of our paper is to explore properties of these complex networks that\\nare suitable as features for machine-learning-based authorship attribution of\\ndocuments. We performed experiments on three different datasets, and obtained\\npromising results.\\n',\n",
              " '  The Cornell Semantic Parsing Framework (SPF) is a learning and inference\\nframework for mapping natural language to formal representation of its meaning.\\n',\n",
              " '  Question answering (QA) system aims at retrieving precise information from a\\nlarge collection of documents against a query. This paper describes the\\narchitecture of a Natural Language Question Answering (NLQA) system for a\\nspecific domain based on the ontological information, a step towards semantic\\nweb question answering. The proposed architecture defines four basic modules\\nsuitable for enhancing current QA capabilities with the ability of processing\\ncomplex questions. The first module was the question processing, which analyses\\nand classifies the question and also reformulates the user query. The second\\nmodule allows the process of retrieving the relevant documents. The next module\\nprocesses the retrieved documents, and the last module performs the extraction\\nand generation of a response. Natural language processing techniques are used\\nfor processing the question and documents and also for answer extraction.\\nOntology and domain knowledge are used for reformulating queries and\\nidentifying the relations. The aim of the system is to generate short and\\nspecific answer to the question that is asked in the natural language in a\\nspecific domain. We have achieved 94 % accuracy of natural language question\\nanswering in our implementation.\\n',\n",
              " '  Machine translation evaluation is a very important activity in machine\\ntranslation development. Automatic evaluation metrics proposed in literature\\nare inadequate as they require one or more human reference translations to\\ncompare them with output produced by machine translation. This does not always\\ngive accurate results as a text can have several different translations. Human\\nevaluation metrics, on the other hand, lacks inter-annotator agreement and\\nrepeatability. In this paper we have proposed a new human evaluation metric\\nwhich addresses these issues. Moreover this metric also provides solid grounds\\nfor making sound assumptions on the quality of the text produced by a machine\\ntranslation.\\n',\n",
              " '  Information Extraction (IE) is the task of automatically extracting\\nstructured information from unstructured/semi-structured machine-readable\\ndocuments. Among various IE tasks, extracting actionable intelligence from\\never-increasing amount of data depends critically upon Cross-Document\\nCoreference Resolution (CDCR) - the task of identifying entity mentions across\\nmultiple documents that refer to the same underlying entity. Recently, document\\ndatasets of the order of peta-/tera-bytes has raised many challenges for\\nperforming effective CDCR such as scaling to large numbers of mentions and\\nlimited representational power. The problem of analysing such datasets is\\ncalled \"big data\". The aim of this paper is to provide readers with an\\nunderstanding of the central concepts, subtasks, and the current\\nstate-of-the-art in CDCR process. We provide assessment of existing\\ntools/techniques for CDCR subtasks and highlight big data challenges in each of\\nthem to help readers identify important and outstanding issues for further\\ninvestigation. Finally, we provide concluding remarks and discuss possible\\ndirections for future work.\\n',\n",
              " '  Text data is often seen as \"take-away\" materials with little noise and easy\\nto process information. Main questions are how to get data and transform them\\ninto a good document format. But data can be sensitive to noise oftenly called\\nambiguities. Ambiguities are aware from a long time, mainly because polysemy is\\nobvious in language and context is required to remove uncertainty. I claim in\\nthis paper that syntactic context is not suffisant to improve interpretation.\\nIn this paper I try to explain that firstly noise can come from natural data\\nthemselves, even involving high technology, secondly texts, seen as verified\\nbut meaningless, can spoil content of a corpus; it may lead to contradictions\\nand background noise.\\n',\n",
              " '  We compared entropy for texts written in natural languages (English, Spanish)\\nand artificial languages (computer software) based on a simple expression for\\nthe entropy as a function of message length and specific word diversity. Code\\ntext written in artificial languages showed higher entropy than text of similar\\nlength expressed in natural languages. Spanish texts exhibit more symbolic\\ndiversity than English ones. Results showed that algorithms based on complexity\\nmeasures differentiate artificial from natural languages, and that text\\nanalysis based on complexity measures allows the unveiling of important aspects\\nof their nature. We propose specific expressions to examine entropy related\\naspects of tests and estimate the values of entropy, emergence,\\nself-organization and complexity based on specific diversity and message\\nlength.\\n',\n",
              " '  Since long, research on machine translation has been ongoing. Still, we do\\nnot get good translations from MT engines so developed. Manual ranking of these\\noutputs tends to be very time consuming and expensive. Identifying which one is\\nbetter or worse than the others is a very taxing task. In this paper, we show\\nan approach which can provide automatic ranks to MT outputs (translations)\\ntaken from different MT Engines and which is based on N-gram approximations. We\\nprovide a solution where no human intervention is required for ranking systems.\\nFurther we also show the evaluations of our results which show equivalent\\nresults as that of human ranking.\\n',\n",
              " '  There are many known Arabic lexicons organized on different ways, each of\\nthem has a different number of Arabic words according to its organization way.\\nThis paper has used mathematical relations to count a number of Arabic words,\\nwhich proofs the number of Arabic words presented by Al Farahidy. The paper\\nalso presents new way to build an electronic Arabic lexicon by using a hash\\nfunction that converts each word (as input) to correspond a unique integer\\nnumber (as output), these integer numbers will be used as an index to a lexicon\\nentry.\\n',\n",
              " '  Objective: Narrative text in Electronic health records (EHR) contain rich\\ninformation for medical and data science studies. This paper introduces the\\ndesign and performance of Narrative Information Linear Extraction (NILE), a\\nnatural language processing (NLP) package for EHR analysis that we share with\\nthe medical informatics community. Methods: NILE uses a modified prefix-tree\\nsearch algorithm for named entity recognition, which can detect prefix and\\nsuffix sharing. The semantic analyses are implemented as rule-based finite\\nstate machines. Analyses include negation, location, modification, family\\nhistory, and ignoring. Result: The processing speed of NILE is hundreds to\\nthousands times faster than existing NLP software for medical text. The\\naccuracy of presence analysis of NILE is on par with the best performing models\\non the 2010 i2b2/VA NLP challenge data. Conclusion: The speed, accuracy, and\\nbeing able to operate via API make NILE a valuable addition to the NLP software\\nfor medical informatics and data science.\\n',\n",
              " '  This paper presents a novel semantic-based phrase translation model. A pair\\nof source and target phrases are projected into continuous-valued vector\\nrepresentations in a low-dimensional latent semantic space, where their\\ntranslation score is computed by the distance between the pair in this new\\nspace. The projection is performed by a multi-layer neural network whose\\nweights are learned on parallel training data. The learning is aimed to\\ndirectly optimize the quality of end-to-end machine translation results.\\nExperimental evaluation has been performed on two Europarl translation tasks,\\nEnglish-French and German-English. The results show that the new semantic-based\\nphrase translation model significantly improves the performance of a\\nstate-of-the-art phrase-based statistical machine translation sys-tem, leading\\nto a gain of 0.7-1.0 BLEU points.\\n',\n",
              " '  The author describes a conceptual study towards mapping grounded natural\\nlanguage discourse representation structures to instances of controlled\\nlanguage statements. This can be achieved via a pipeline of preexisting state\\nof the art technologies, namely natural language syntax to semantic discourse\\nmapping, and a reduction of the latter to controlled language discourse, given\\na set of previously learnt reduction rules. Concludingly a description on\\nevaluation, potential and limitations for ontology-based reasoning is\\npresented.\\n',\n",
              " '  Timeline Generation aims at summarizing news from different epochs and\\ntelling readers how an event evolves. It is a new challenge that combines\\nsalience ranking with novelty detection. For long-term public events, the main\\ntopic usually includes various aspects across different epochs and each aspect\\nhas its own evolving pattern. Existing approaches neglect such hierarchical\\ntopic structure involved in the news corpus in timeline generation. In this\\npaper, we develop a novel time-dependent Hierarchical Dirichlet Model (HDM) for\\ntimeline generation. Our model can aptly detect different levels of topic\\ninformation across corpus and such structure is further used for sentence\\nselection. Based on the topic mined fro HDM, sentences are selected by\\nconsidering different aspects such as relevance, coherence and coverage. We\\ndevelop experimental systems to evaluate 8 long-term events that public\\nconcern. Performance comparison between different systems demonstrates the\\neffectiveness of our model in terms of ROUGE metrics.\\n',\n",
              " '  We propose a new benchmark corpus to be used for measuring progress in\\nstatistical language modeling. With almost one billion words of training data,\\nwe hope this benchmark will be useful to quickly evaluate novel language\\nmodeling techniques, and to compare their contribution when combined with other\\nadvanced techniques. We show performance of several well-known types of\\nlanguage models, with the best results achieved with a recurrent neural network\\nbased language model. The baseline unpruned Kneser-Ney 5-gram model achieves\\nperplexity 67.6; a combination of techniques leads to 35% reduction in\\nperplexity, or 10% reduction in cross-entropy (bits), over that baseline.\\n  The benchmark is available as a code.google.com project; besides the scripts\\nneeded to rebuild the training/held-out data, it also makes available\\nlog-probability values for each word in each of ten held-out data sets, for\\neach of the baseline n-gram models.\\n',\n",
              " '  We propose a cognitively and linguistically motivated set of sorts for\\nlexical semantics in a compositional setting: the classifiers in languages that\\ndo have such pronouns. These sorts are needed to include lexical considerations\\nin a semantical analyser such as Boxer or Grail. Indeed, all proposed lexical\\nextensions of usual Montague semantics to model restriction of selection,\\nfelicitous and infelicitous copredication require a rich and refined type\\nsystem whose base types are the lexical sorts, the basis of the many-sorted\\nlogic in which semantical representations of sentences are stated. However,\\nnone of those approaches define precisely the actual base types or sorts to be\\nused in the lexicon. In this article, we shall discuss some of the options\\ncommonly adopted by researchers in formal lexical semantics, and defend the\\nview that classifiers in the languages which have such pronouns are an\\nappealing solution, both linguistically and cognitively motivated.\\n',\n",
              " '  For any deep computational processing of language we need evidences, and one\\nsuch set of evidences is corpus. This paper describes the development of a\\ntext-based corpus for the Bishnupriya Manipuri language. A Corpus is considered\\nas a building block for any language processing tasks. Due to the lack of\\nawareness like other Indian languages, it is also studied less frequently. As a\\nresult the language still lacks a good corpus and basic language processing\\ntools. As per our knowledge this is the first effort to develop a corpus for\\nBishnupriya Manipuri language.\\n',\n",
              " '  So far and trying to reach human capabilities, research in automatic\\nsummarization has been based on hypothesis that are both enabling and limiting.\\nSome of these limitations are: how to take into account and reflect (in the\\ngenerated summary) the implicit information conveyed in the text, the author\\nintention, the reader intention, the context influence, the general world\\nknowledge. Thus, if we want machines to mimic human abilities, then they will\\nneed access to this same large variety of knowledge. The implicit is affecting\\nthe orientation and the argumentation of the text and consequently its summary.\\nMost of Text Summarizers (TS) are processing as compressing the initial data\\nand they necessarily suffer from information loss. TS are focusing on features\\nof the text only, not on what the author intended or why the reader is reading\\nthe text. In this paper, we address this problem and we present a system\\nfocusing on acquiring knowledge that is implicit. We principally spotlight the\\nimplicit information conveyed by the argumentative connectives such as: but,\\neven, yet and their effect on the summary.\\n',\n",
              " '  Most natural language processing systems based on machine learning are not\\nrobust to domain shift. For example, a state-of-the-art syntactic dependency\\nparser trained on Wall Street Journal sentences has an absolute drop in\\nperformance of more than ten points when tested on textual data from the Web.\\nAn efficient solution to make these methods more robust to domain shift is to\\nfirst learn a word representation using large amounts of unlabeled data from\\nboth domains, and then use this representation as features in a supervised\\nlearning algorithm. In this paper, we propose to use hidden Markov models to\\nlearn word representations for part-of-speech tagging. In particular, we study\\nthe influence of using data from the source, the target or both domains to\\nlearn the representation and the different ways to represent words using an\\nHMM.\\n',\n",
              " '  Deep learning embeddings have been successfully used for many natural\\nlanguage processing problems. Embeddings are mostly computed for word forms\\nalthough a number of recent papers have extended this to other linguistic units\\nlike morphemes and phrases. In this paper, we argue that learning embeddings\\nfor discontinuous linguistic units should also be considered. In an\\nexperimental evaluation on coreference resolution, we show that such embeddings\\nperform better than word form embeddings.\\n',\n",
              " '  Word embeddings resulting from neural language models have been shown to be\\nsuccessful for a large variety of NLP tasks. However, such architecture might\\nbe difficult to train and time-consuming. Instead, we propose to drastically\\nsimplify the word embeddings computation through a Hellinger PCA of the word\\nco-occurence matrix. We compare those new word embeddings with some well-known\\nembeddings on NER and movie review tasks and show that we can reach similar or\\neven better performance. Although deep learning is not really necessary for\\ngenerating good word embeddings, we show that it can provide an easy way to\\nadapt embeddings to specific tasks.\\n',\n",
              " '  There are two main approaches to the distributed representation of words:\\nlow-dimensional deep learning embeddings and high-dimensional distributional\\nmodels, in which each dimension corresponds to a context word. In this paper,\\nwe combine these two approaches by learning embeddings based on\\ndistributional-model vectors - as opposed to one-hot vectors as is standardly\\ndone in deep learning. We show that the combined approach has better\\nperformance on a word relatedness judgment task.\\n',\n",
              " '  This paper investigates the learning of 3rd-order tensors representing the\\nsemantics of transitive verbs. The meaning representations are part of a\\ntype-driven tensor-based semantic framework, from the newly emerging field of\\ncompositional distributional semantics. Standard techniques from the neural\\nnetworks literature are used to learn the tensors, which are tested on a\\nselectional preference-style task with a simple 2-dimensional sentence space.\\nPromising results are obtained against a competitive corpus-based baseline. We\\nargue that extending this work beyond transitive verbs, and to\\nhigher-dimensional sentence spaces, is an interesting and challenging problem\\nfor the machine learning community to consider.\\n',\n",
              " '  Distributed representations of meaning are a natural way to encode covariance\\nrelationships between words and phrases in NLP. By overcoming data sparsity\\nproblems, as well as providing information about semantic relatedness which is\\nnot available in discrete representations, distributed representations have\\nproven useful in many NLP tasks. Recent work has shown how compositional\\nsemantic representations can successfully be applied to a number of monolingual\\napplications such as sentiment analysis. At the same time, there has been some\\ninitial success in work on learning shared word-level representations across\\nlanguages. We combine these two approaches by proposing a method for learning\\ndistributed representations in a multilingual setup. Our model learns to assign\\nsimilar embeddings to aligned sentences and dissimilar ones to sentence which\\nare not aligned while not requiring word alignments. We show that our\\nrepresentations are semantically informative and apply them to a cross-lingual\\ndocument classification task where we outperform the previous state of the art.\\nFurther, by employing parallel corpora of multiple language pairs we find that\\nour model learns representations that capture semantic relationships across\\nlanguages for which no parallel data was used.\\n',\n",
              " '  Recursive neural network models and their accompanying vector representations\\nfor words have seen success in an array of increasingly semantically\\nsophisticated tasks, but almost nothing is known about their ability to\\naccurately capture the aspects of linguistic meaning that are necessary for\\ninterpretation or reasoning. To evaluate this, I train a recursive model on a\\nnew corpus of constructed examples of logical reasoning in short sentences,\\nlike the inference of \"some animal walks\" from \"some dog walks\" or \"some cat\\nwalks,\" given that dogs and cats are animals. This model learns representations\\nthat generalize well to new types of reasoning pattern in all but a few cases,\\na result which is promising for the ability of learned representation models to\\ncapture logical reasoning.\\n',\n",
              " '  Speech representation and modelling in high-dimensional spaces of acoustic\\nwaveforms, or a linear transformation thereof, is investigated with the aim of\\nimproving the robustness of automatic speech recognition to additive noise. The\\nmotivation behind this approach is twofold: (i) the information in acoustic\\nwaveforms that is usually removed in the process of extracting low-dimensional\\nfeatures might aid robust recognition by virtue of structured redundancy\\nanalogous to channel coding, (ii) linear feature domains allow for exact noise\\nadaptation, as opposed to representations that involve non-linear processing\\nwhich makes noise adaptation challenging. Thus, we develop a generative\\nframework for phoneme modelling in high-dimensional linear feature domains, and\\nuse it in phoneme classification and recognition tasks. Results show that\\nclassification and recognition in this framework perform better than analogous\\nPLP and MFCC classifiers below 18 dB SNR. A combination of the high-dimensional\\nand MFCC features at the likelihood level performs uniformly better than either\\nof the individual representations across all noise levels.\\n',\n",
              " '  Ontology Learning (OL) is the computational task of generating a knowledge\\nbase in the form of an ontology given an unstructured corpus whose content is\\nin natural language (NL). Several works can be found in this area most of which\\nare limited to statistical and lexico-syntactic pattern matching based\\ntechniques Light-Weight OL. These techniques do not lead to very accurate\\nlearning mostly because of several linguistic nuances in NL. Formal OL is an\\nalternative (less explored) methodology were deep linguistics analysis is made\\nusing theory and tools found in computational linguistics to generate formal\\naxioms and definitions instead simply inducing a taxonomy. In this paper we\\npropose \"Description Logic (DL)\" based formal OL framework for learning factual\\nIS-A type sentences in English. We claim that semantic construction of IS-A\\nsentences is non trivial. Hence, we also claim that such sentences requires\\nspecial studies in the context of OL before any truly formal OL can be\\nproposed. We introduce a learner tool, called DLOL_IS-A, that generated such\\nontologies in the owl format. We have adopted \"Gold Standard\" based OL\\nevaluation on IS-A rich WCL v.1.1 dataset and our own Community representative\\nIS-A dataset. We observed significant improvement of DLOL_IS-A when compared to\\nthe light-weight OL tool Text2Onto and formal OL tool FRED.\\n',\n",
              " '  The problem of Natural Language Query Formalization (NLQF) is to translate a\\ngiven user query in natural language (NL) into a formal language so that the\\nsemantic interpretation has equivalence with the NL interpretation.\\nFormalization of NL queries enables logic based reasoning during information\\nretrieval, database query, question-answering, etc. Formalization also helps in\\nWeb query normalization and indexing, query intent analysis, etc. In this paper\\nwe are proposing a Description Logics based formal methodology for wh-query\\nintent (also called desire) identification and corresponding formal\\ntranslation. We evaluated the scalability of our proposed formalism using\\nMicrosoft Encarta 98 query dataset and OWL-S TC v.4.0 dataset.\\n',\n",
              " '  We present power low rank ensembles (PLRE), a flexible framework for n-gram\\nlanguage modeling where ensembles of low rank matrices and tensors are used to\\nobtain smoothed probability estimates of words in context. Our method can be\\nunderstood as a generalization of n-gram modeling to non-integer n, and\\nincludes standard techniques such as absolute discounting and Kneser-Ney\\nsmoothing as special cases. PLRE training is efficient and our approach\\noutperforms state-of-the-art modified Kneser Ney baselines in terms of\\nperplexity on large corpora as well as on BLEU score in a downstream machine\\ntranslation task.\\n',\n",
              " '  In this paper we present an approach for estimating the quality of machine\\ntranslation system. There are various methods for estimating the quality of\\noutput sentences, but in this paper we focus on Na\\\\\"ive Bayes classifier to\\nbuild model using features which are extracted from the input sentences. These\\nfeatures are used for finding the likelihood of each of the sentences of the\\ntraining data which are then further used for determining the scores of the\\ntest data. On the basis of these scores we determine the class labels of the\\ntest data.\\n',\n",
              " '  We propose a novel zero-shot learning method for semantic utterance\\nclassification (SUC). It learns a classifier $f: X \\\\to Y$ for problems where\\nnone of the semantic categories $Y$ are present in the training set. The\\nframework uncovers the link between categories and utterances using a semantic\\nspace. We show that this semantic space can be learned by deep neural networks\\ntrained on large amounts of search engine query log data. More precisely, we\\npropose a novel method that can learn discriminative semantic features without\\nsupervision. It uses the zero-shot learning framework to guide the learning of\\nthe semantic features. We demonstrate the effectiveness of the zero-shot\\nsemantic learning algorithm on the SUC dataset collected by (Tur, 2012).\\nFurthermore, we achieve state-of-the-art results by combining the semantic\\nfeatures with a supervised method.\\n',\n",
              " '  In modern electronic medical records (EMR) much of the clinically important\\ndata - signs and symptoms, symptom severity, disease status, etc. - are not\\nprovided in structured data fields, but rather are encoded in clinician\\ngenerated narrative text. Natural language processing (NLP) provides a means of\\n\"unlocking\" this important data source for applications in clinical decision\\nsupport, quality assurance, and public health. This chapter provides an\\noverview of representative NLP systems in biomedicine based on a unified\\narchitectural view. A general architecture in an NLP system consists of two\\nmain components: background knowledge that includes biomedical knowledge\\nresources and a framework that integrates NLP tools to process text. Systems\\ndiffer in both components, which we will review briefly. Additionally,\\nchallenges facing current research efforts in biomedical NLP include the\\npaucity of large, publicly available annotated corpora, although initiatives\\nthat facilitate data sharing, system evaluation, and collaborative work between\\nresearchers in clinical NLP are starting to emerge.\\n',\n",
              " '  Current multi-document summarization systems can successfully extract summary\\nsentences, however with many limitations including: low coverage, inaccurate\\nextraction to important sentences, redundancy and poor coherence among the\\nselected sentences. The present study introduces a new concept of centroid\\napproach and reports new techniques for extracting summary sentences for\\nmulti-document. In both techniques keyphrases are used to weigh sentences and\\ndocuments. The first summarization technique (Sen-Rich) prefers maximum\\nrichness sentences. While the second (Doc-Rich), prefers sentences from\\ncentroid document. To demonstrate the new summarization system application to\\nextract summaries of Arabic documents we performed two experiments. First, we\\napplied Rouge measure to compare the new techniques among systems presented at\\nTAC2011. The results show that Sen-Rich outperformed all systems in ROUGE-S.\\nSecond, the system was applied to summarize multi-topic documents. Using human\\nevaluators, the results show that Doc-Rich is the superior, where summary\\nsentences characterized by extra coverage and more cohesion.\\n',\n",
              " '  We developed a type-theoretical framework for natural lan- guage semantics\\nthat, in addition to the usual Montagovian treatment of compositional\\nsemantics, includes a treatment of some phenomena of lex- ical semantic:\\ncoercions, meaning, transfers, (in)felicitous co-predication. In this setting\\nwe see how the various readings of plurals (collective, dis- tributive,\\ncoverings,...) can be modelled.\\n',\n",
              " '  In this paper we examine the usefulness of two classes of algorithms Distance\\nMethods, Discrete Character Methods (Felsenstein and Felsenstein 2003) widely\\nused in genetics, for predicting the family relationships among a set of\\nrelated languages and therefore, diachronic language change. Applying these\\nalgorithms to the data on the numbers of shared cognates- with-change and\\nchanged as well as unchanged cognates for a group of six languages belonging to\\na Dravidian language sub-family given in Krishnamurti et al. (1983), we\\nobserved that the resultant phylogenetic trees are largely in agreement with\\nthe linguistic family tree constructed using the comparative method of\\nreconstruction with only a few minor differences. Furthermore, we studied these\\nminor differences and found that they were cases of genuine ambiguity even for\\na well-trained historical linguist. We evaluated the trees obtained through our\\nexperiments using a well-defined criterion and report the results here. We\\nfinally conclude that quantitative methods like the ones we examined are quite\\nuseful in predicting family relationships among languages. In addition, we\\nconclude that a modest degree of confidence attached to the intuition that\\nthere could indeed exist a parallelism between the processes of linguistic and\\ngenetic change is not totally misplaced.\\n',\n",
              " \"  In this article, we investigate the properties of phoneme N-grams across half\\nof the world's languages. We investigate if the sizes of three different N-gram\\ndistributions of the world's language families obey a power law. Further, the\\nN-gram distributions of language families parallel the sizes of the families,\\nwhich seem to obey a power law distribution. The correlation between N-gram\\ndistributions and language family sizes improves with increasing values of N.\\nWe applied statistical tests, originally given by physicists, to test the\\nhypothesis of power law fit to twelve different datasets. The study also raises\\nsome new questions about the use of N-gram distributions in linguistic\\nresearch, which we answer by running a statistical test.\\n\",\n",
              " \"  Spoken Language Systems at Saarland University (LSV) participated this year\\nwith 5 runs at the TAC KBP English slot filling track. Effective algorithms for\\nall parts of the pipeline, from document retrieval to relation prediction and\\nresponse post-processing, are bundled in a modular end-to-end relation\\nextraction system called RelationFactory. The main run solely focuses on\\nshallow techniques and achieved significant improvements over LSV's last year's\\nsystem, while using the same training data and patterns. Improvements mainly\\nhave been obtained by a feature representation focusing on surface skip n-grams\\nand improved scoring for extracted distant supervision patterns. Important\\nfactors for effective extraction are the training and tuning scheme for distant\\nsupervision classifiers, and the query expansion by a translation model based\\non Wikipedia links. In the TAC KBP 2013 English Slotfilling evaluation, the\\nsubmitted main run of the LSV RelationFactory system achieved the top-ranked\\nF1-score of 37.3%.\\n\",\n",
              " '  Recent work on learning multilingual word representations usually relies on\\nthe use of word-level alignements (e.g. infered with the help of GIZA++)\\nbetween translated sentences, in order to align the word embeddings in\\ndifferent languages. In this workshop paper, we investigate an autoencoder\\nmodel for learning multilingual word representations that does without such\\nword-level alignements. The autoencoder is trained to reconstruct the\\nbag-of-word representation of given sentence from an encoded representation\\nextracted from its translation. We evaluate our approach on a multilingual\\ndocument classification task, where labeled data is available only for one\\nlanguage (e.g. English) while classification must be performed in a different\\nlanguage (e.g. French). In our experiments, we observe that our method compares\\nfavorably with a previously proposed method that exploits word-level alignments\\nto learn word representations.\\n',\n",
              " \"  Computational measures of semantic similarity between geographic terms\\nprovide valuable support across geographic information retrieval, data mining,\\nand information integration. To date, a wide variety of approaches to\\ngeo-semantic similarity have been devised. A judgment of similarity is not\\nintrinsically right or wrong, but obtains a certain degree of cognitive\\nplausibility, depending on how closely it mimics human behavior. Thus selecting\\nthe most appropriate measure for a specific task is a significant challenge. To\\naddress this issue, we make an analogy between computational similarity\\nmeasures and soliciting domain expert opinions, which incorporate a subjective\\nset of beliefs, perceptions, hypotheses, and epistemic biases. Following this\\nanalogy, we define the semantic similarity ensemble (SSE) as a composition of\\ndifferent similarity measures, acting as a panel of experts having to reach a\\ndecision on the semantic similarity of a set of geographic terms. The approach\\nis evaluated in comparison to human judgments, and results indicate that an SSE\\nperforms better than the average of its parts. Although the best member tends\\nto outperform the ensemble, all ensembles outperform the average performance of\\neach ensemble's member. Hence, in contexts where the best measure is unknown,\\nthe ensemble provides a more cognitively plausible approach.\\n\",\n",
              " '  Dictionaries are essence of any language providing vital linguistic recourse\\nfor the language learners, researchers and scholars. This paper focuses on the\\nmethodology and techniques used in developing software architecture for a\\nUBSESD (Unicode Based Sindhi to English and English to Sindhi Dictionary). The\\nproposed system provides an accurate solution for construction and\\nrepresentation of Unicode based Sindhi characters in a dictionary implementing\\nHash Structure algorithm and a custom java Object as its internal data\\nstructure saved in a file. The System provides facilities for Insertion,\\nDeletion and Editing of new records of Sindhi. Through this framework any type\\nof Sindhi to English and English to Sindhi Dictionary (belonging to different\\ndomains of knowledge, e.g. engineering, medicine, computer, biology etc.) could\\nbe developed easily with accurate representation of Unicode Characters in font\\nindependent manner.\\n',\n",
              " '  In this study, a dictionary-based method is used to extract expressive\\nconcepts from documents. So far, there have been many studies concerning\\nconcept mining in English, but this area of study for Turkish, an agglutinative\\nlanguage, is still immature. We used dictionary instead of WordNet, a lexical\\ndatabase grouping words into synsets that is widely used for concept\\nextraction. The dictionaries are rarely used in the domain of concept mining,\\nbut taking into account that dictionary entries have synonyms, hypernyms,\\nhyponyms and other relationships in their meaning texts, the success rate has\\nbeen high for determining concepts. This concept extraction method is\\nimplemented on documents, that are collected from different corpora.\\n',\n",
              " '  Multilingual text processing is useful because the information content found\\nin different languages is complementary, both regarding facts and opinions.\\nWhile Information Extraction and other text mining software can, in principle,\\nbe developed for many languages, most text analysis tools have only been\\napplied to small sets of languages because the development effort per language\\nis large. Self-training tools obviously alleviate the problem, but even the\\neffort of providing training data and of manually tuning the results is usually\\nconsiderable. In this paper, we gather insights by various multilingual system\\ndevelopers on how to minimise the effort of developing natural language\\nprocessing applications for many languages. We also explain the main guidelines\\nunderlying our own effort to develop complex text mining software for tens of\\nlanguages. While these guidelines - most of all: extreme simplicity - can be\\nvery restrictive and limiting, we believe to have shown the feasibility of the\\napproach through the development of the Europe Media Monitor (EMM) family of\\napplications (http://emm.newsbrief.eu/overview.html). EMM is a set of complex\\nmedia monitoring tools that process and analyse up to 100,000 online news\\narticles per day in between twenty and fifty languages. We will also touch upon\\nthe kind of language resources that would make it easier for all to develop\\nhighly multilingual text mining applications. We will argue that - to achieve\\nthis - the most needed resources would be freely available, simple, parallel\\nand uniform multilingual dictionaries, corpora and software tools.\\n',\n",
              " '  We propose a real-time machine translation system that allows users to select\\na news category and to translate the related live news articles from Arabic,\\nCzech, Danish, Farsi, French, German, Italian, Polish, Portuguese, Spanish and\\nTurkish into English. The Moses-based system was optimised for the news domain\\nand differs from other available systems in four ways: (1) News items are\\nautomatically categorised on the source side, before translation; (2) Named\\nentity translation is optimised by recognising and extracting them on the\\nsource side and by re-inserting their translation in the target language,\\nmaking use of a separate entity repository; (3) News titles are translated with\\na separate translation system which is optimised for the specific style of news\\ntitles; (4) The system was optimised for speed in order to cope with the large\\nvolume of daily news articles.\\n',\n",
              " '  The task of sentiment analysis of reviews is carried out using manually built\\n/ automatically generated lexicon resources of their own with which terms are\\nmatched with lexicon to compute the term count for positive and negative\\npolarity. On the other hand the Sentiwordnet, which is quite different from\\nother lexicon resources that gives scores (weights) of the positive and\\nnegative polarity for each word. The polarity of a word namely positive,\\nnegative and neutral have the score ranging between 0 to 1 indicates the\\nstrength/weight of the word with that sentiment orientation. In this paper, we\\nshow that using the Sentiwordnet, how we could enhance the performance of the\\nclassification at both sentence and document level.\\n',\n",
              " '  This work proposes a novel support vector machine (SVM) based robust\\nautomatic speech recognition (ASR) front-end that operates on an ensemble of\\nthe subband components of high-dimensional acoustic waveforms. The key issues\\nof selecting the appropriate SVM kernels for classification in frequency\\nsubbands and the combination of individual subband classifiers using ensemble\\nmethods are addressed. The proposed front-end is compared with state-of-the-art\\nASR front-ends in terms of robustness to additive noise and linear filtering.\\nExperiments performed on the TIMIT phoneme classification task demonstrate the\\nbenefits of the proposed subband based SVM front-end: it outperforms the\\nstandard cepstral front-end in the presence of noise and linear filtering for\\nsignal-to-noise ratio (SNR) below 12-dB. A combination of the proposed\\nfront-end with a conventional front-end such as MFCC yields further\\nimprovements over the individual front ends across the full range of noise\\nlevels.\\n',\n",
              " \"  A novel approach to the fully automated, unsupervised extraction of\\ndependency grammars and associated syntax-to-semantic-relationship mappings\\nfrom large text corpora is described. The suggested approach builds on the\\nauthors' prior work with the Link Grammar, RelEx and OpenCog systems, as well\\nas on a number of prior papers and approaches from the statistical language\\nlearning literature. If successful, this approach would enable the mining of\\nall the information needed to power a natural language comprehension and\\ngeneration system, directly from a large, unannotated corpus.\\n\",\n",
              " '  This paper presents a new method for inferring the semantic properties of\\ndocuments by leveraging free-text keyphrase annotations. Such annotations are\\nbecoming increasingly abundant due to the recent dramatic growth in\\nsemi-structured, user-generated online content. One especially relevant domain\\nis product reviews, which are often annotated by their authors with pros/cons\\nkeyphrases such as a real bargain or good value. These annotations are\\nrepresentative of the underlying semantic properties; however, unlike expert\\nannotations, they are noisy: lay authors may use different labels to denote the\\nsame property, and some labels may be missing. To learn using such noisy\\nannotations, we find a hidden paraphrase structure which clusters the\\nkeyphrases. The paraphrase structure is linked with a latent topic model of the\\nreview texts, enabling the system to predict the properties of unannotated\\ndocuments and to effectively aggregate the semantic properties of multiple\\nreviews. Our approach is implemented as a hierarchical Bayesian model with\\njoint inference. We find that joint inference increases the robustness of the\\nkeyphrase clustering and encourages the latent topics to correlate with\\nsemantically meaningful properties. Multiple evaluations demonstrate that our\\nmodel substantially outperforms alternative approaches for summarizing single\\nand multiple documents into a set of semantically salient keyphrases.\\n',\n",
              " '  Complex questions that require inferencing and synthesizing information from\\nmultiple documents can be seen as a kind of topic-oriented, informative\\nmulti-document summarization where the goal is to produce a single text as a\\ncompressed version of a set of documents with a minimum loss of relevant\\ninformation. In this paper, we experiment with one empirical method and two\\nunsupervised statistical machine learning techniques: K-means and Expectation\\nMaximization (EM), for computing relative importance of the sentences. We\\ncompare the results of these approaches. Our experiments show that the\\nempirical approach outperforms the other two techniques and EM performs better\\nthan K-means. However, the performance of these approaches depends entirely on\\nthe feature set used and the weighting of these features. In order to measure\\nthe importance and relevance to the user query we extract different kinds of\\nfeatures (i.e. lexical, lexical semantic, cosine similarity, basic element,\\ntree kernel based syntactic and shallow-semantic) for each of the document\\nsentences. We use a local search technique to learn the weights of the\\nfeatures. To the best of our knowledge, no study has used tree kernel functions\\nto encode syntactic/semantic information for more complex tasks such as\\ncomputing the relatedness between the query sentences and the document\\nsentences in order to generate query-focused summaries (or answers to complex\\nquestions). For each of our methods of generating summaries (i.e. empirical,\\nK-means and EM) we show the effects of syntactic and shallow-semantic features\\nover the bag-of-words (BOW) features.\\n',\n",
              " '  This paper presents a multilayered architecture that enhances the\\ncapabilities of current QA systems and allows different types of complex\\nquestions or queries to be processed. The answers to these questions need to be\\ngathered from factual information scattered throughout different documents.\\nSpecifically, we designed a specialized layer to process the different types of\\ntemporal questions. Complex temporal questions are first decomposed into simple\\nquestions, according to the temporal relations expressed in the original\\nquestion. In the same way, the answers to the resulting simple questions are\\nrecomposed, fulfilling the temporal restrictions of the original complex\\nquestion. A novel aspect of this approach resides in the decomposition which\\nuses a minimal quantity of resources, with the final aim of obtaining a\\nportable platform that is easily extensible to other languages. In this paper\\nwe also present a methodology for evaluation of the decomposition of the\\nquestions as well as the ability of the implemented temporal layer to perform\\nat a multilingual level. The temporal layer was first performed for English,\\nthen evaluated and compared with: a) a general purpose QA system (F-measure\\n65.47% for QA plus English temporal layer vs. 38.01% for the general QA\\nsystem), and b) a well-known QA system. Much better results were obtained for\\ntemporal questions with the multilayered system. This system was therefore\\nextended to Spanish and very good results were again obtained in the evaluation\\n(F-measure 40.36% for QA plus Spanish temporal layer vs. 22.94% for the general\\nQA system).\\n',\n",
              " '  In this study it is proven that the Hrebs used in Denotation analysis of\\ntexts and Cohesion Chains (defined as a fusion between Lexical Chains and\\nCoreference Chains) represent similar linguistic tools. This result gives us\\nthe possibility to extend to Cohesion Chains (CCs) some important indicators\\nas, for example the Kernel of CCs, the topicality of a CC, text concentration,\\nCC-diffuseness and mean diffuseness of the text. Let us mention that nowhere in\\nthe Lexical Chains or Coreference Chains literature these kinds of indicators\\nare introduced and used since now. Similarly, some applications of CCs in the\\nstudy of a text (as for example segmentation or summarization of a text) could\\nbe realized starting from hrebs. As an illustration of the similarity between\\nHrebs and CCs a detailed analyze of the poem \"Lacul\" by Mihai Eminescu is\\ngiven.\\n',\n",
              " '  Vast amounts of text on the Web are unstructured and ungrammatical, such as\\nclassified ads, auction listings, forum postings, etc. We call such text\\n\"posts.\" Despite their inconsistent structure and lack of grammar, posts are\\nfull of useful information. This paper presents work on semi-automatically\\nbuilding tables of relational information, called \"reference sets,\" by\\nanalyzing such posts directly. Reference sets can be applied to a number of\\ntasks such as ontology maintenance and information extraction. Our\\nreference-set construction method starts with just a small amount of background\\nknowledge, and constructs tuples representing the entities in the posts to form\\na reference set. We also describe an extension to this approach for the special\\ncase where even this small amount of background knowledge is impossible to\\ndiscover and use. To evaluate the utility of the machine-constructed reference\\nsets, we compare them to manually constructed reference sets in the context of\\nreference-set-based information extraction. Our results show the reference sets\\nconstructed by our method outperform manually constructed reference sets. We\\nalso compare the reference-set-based extraction approach using the\\nmachine-constructed reference set to supervised extraction approaches using\\ngeneric features. These results demonstrate that using machine-constructed\\nreference sets outperforms the supervised methods, even though the supervised\\nmethods require training data.\\n',\n",
              " '  Temporal information has been the focus of recent attention in information\\nextraction, leading to some standardization effort, in particular for the task\\nof relating events in a text. This task raises the problem of comparing two\\nannotations of a given text, because relations between events in a story are\\nintrinsically interdependent and cannot be evaluated separately. A proper\\nevaluation measure is also crucial in the context of a machine learning\\napproach to the problem. Finding a common comparison referent at the text level\\nis not obvious, and we argue here in favor of a shift from event-based measures\\nto measures on a unique textual object, a minimal underlying temporal graph, or\\nmore formally the transitive reduction of the graph of relations between event\\nboundaries. We support it by an investigation of its properties on synthetic\\ndata and on a well-know temporal corpus.\\n',\n",
              " '  We estimate the $n$-gram entropies of natural language texts in word-length\\nrepresentation and find that these are sensitive to text language and genre. We\\nattribute this sensitivity to changes in the probability distribution of the\\nlengths of single words and emphasize the crucial role of the uniformity of\\nprobabilities of having words with length between five and ten. Furthermore,\\ncomparison with the entropies of shuffled data reveals the impact of word\\nlength correlations on the estimated $n$-gram entropies.\\n',\n",
              " '  The Aviation Safety Reporting System collects voluntarily submitted reports\\non aviation safety incidents to facilitate research work aiming to reduce such\\nincidents. To effectively reduce these incidents, it is vital to accurately\\nidentify why these incidents occurred. More precisely, given a set of possible\\ncauses, or shaping factors, this task of cause identification involves\\nidentifying all and only those shaping factors that are responsible for the\\nincidents described in a report. We investigate two approaches to cause\\nidentification. Both approaches exploit information provided by a semantic\\nlexicon, which is automatically constructed via Thelen and Riloffs Basilisk\\nframework augmented with our linguistic and algorithmic modifications. The\\nfirst approach labels a report using a simple heuristic, which looks for the\\nwords and phrases acquired during the semantic lexicon learning process in the\\nreport. The second approach recasts cause identification as a text\\nclassification problem, employing supervised and transductive text\\nclassification algorithms to learn models from incident reports labeled with\\nshaping factors and using the models to label unseen reports. Our experiments\\nshow that both the heuristic-based approach and the learning-based approach\\n(when given sufficient training data) outperform the baseline system\\nsignificantly.\\n',\n",
              " \"  In this paper we explore various parameter settings of the state-of-art\\nStatistical Machine Translation system to improve the quality of the\\ntranslation for a `distant' language pair like English-Hindi. We proposed new\\ntechniques for efficient reordering. A slight improvement over the baseline is\\nreported using these techniques. We also show that a simple pre-processing step\\ncan improve the quality of the translation significantly.\\n\",\n",
              " '  This survey presents in some detail the main advances that have been recently\\ntaking place in Computational Linguistics towards the unification of the two\\nprominent semantic paradigms: the compositional formal semantics view and the\\ndistributional models of meaning based on vector spaces. After an introduction\\nto these two approaches, I review the most important models that aim to provide\\ncompositionality in distributional semantics. Then I proceed and present in\\nmore detail a particular framework by Coecke, Sadrzadeh and Clark (2010) based\\non the abstract mathematical setting of category theory, as a more complete\\nexample capable to demonstrate the diversity of techniques and scientific\\ndisciplines that this kind of research can draw from. This paper concludes with\\na discussion about important open issues that need to be addressed by the\\nresearchers in the future.\\n',\n",
              " '  Domain knowledge is crucial for effective performance in autonomous control\\nsystems. Typically, human effort is required to encode this knowledge into a\\ncontrol algorithm. In this paper, we present an approach to language grounding\\nwhich automatically interprets text in the context of a complex control\\napplication, such as a game, and uses domain knowledge extracted from the text\\nto improve control performance. Both text analysis and control strategies are\\nlearned jointly using only a feedback signal inherent to the application. To\\neffectively leverage textual information, our method automatically extracts the\\ntext segment most relevant to the current game state, and labels it with a\\ntask-centric predicate structure. This labeled text is then used to bias an\\naction selection policy for the game, guiding it towards promising regions of\\nthe action space. We encode our model for text analysis and game playing in a\\nmulti-layer neural network, representing linguistic decisions via latent\\nvariables in the hidden layers, and game action quality via the output layer.\\nOperating within the Monte-Carlo Search framework, we estimate model parameters\\nusing feedback from simulated games. We apply our approach to the complex\\nstrategy game Civilization II using the official game manual as the text guide.\\nOur results show that a linguistically-informed game-playing agent\\nsignificantly outperforms its language-unaware counterpart, yielding a 34%\\nabsolute improvement and winning over 65% of games when playing against the\\nbuilt-in AI of Civilization.\\n',\n",
              " '  Document Clustering is a branch of a larger area of scientific study known as\\ndata mining .which is an unsupervised classification using to find a structure\\nin a collection of unlabeled data. The useful information in the documents can\\nbe accompanied by a large amount of noise words when using Full Text\\nRepresentation, and therefore will affect negatively the result of the\\nclustering process. So it is with great need to eliminate the noise words and\\nkeeping just the useful information in order to enhance the quality of the\\nclustering results. This problem occurs with different degree for any language\\nsuch as English, European, Hindi, Chinese, and Arabic Language. To overcome\\nthis problem, in this paper, we propose a new and efficient Keyphrases\\nextraction method based on the Suffix Tree data structure (KpST), the extracted\\nKeyphrases are then used in the clustering process instead of Full Text\\nRepresentation. The proposed method for Keyphrases extraction is language\\nindependent and therefore it may be applied to any language. In this\\ninvestigation, we are interested to deal with the Arabic language which is one\\nof the most complex languages. To evaluate our method, we conduct an\\nexperimental study on Arabic Documents using the most popular Clustering\\napproach of Hierarchical algorithms: Agglomerative Hierarchical algorithm with\\nseven linkage techniques and a variety of distance functions and similarity\\nmeasures to perform Arabic Document Clustering task. The obtained results show\\nthat our method for extracting Keyphrases increases the quality of the\\nclustering results. We propose also to study the effect of using the stemming\\nfor the testing dataset to cluster it with the same documents clustering\\ntechniques and similarity/distance measures.\\n',\n",
              " '  Large bilingual parallel texts (also known as bitexts) are usually stored in\\na compressed form, and previous work has shown that they can be more\\nefficiently compressed if the fact that the two texts are mutual translations\\nis exploited. For example, a bitext can be seen as a sequence of biwords\\n---pairs of parallel words with a high probability of co-occurrence--- that can\\nbe used as an intermediate representation in the compression process. However,\\nthe simple biword approach described in the literature can only exploit\\none-to-one word alignments and cannot tackle the reordering of words. We\\ntherefore introduce a generalization of biwords which can describe multi-word\\nexpressions and reorderings. We also describe some methods for the binary\\ncompression of generalized biword sequences, and compare their performance when\\ndifferent schemes are applied to the extraction of the biword sequence. In\\naddition, we show that this generalization of biwords allows for the\\nimplementation of an efficient algorithm to look on the compressed bitext for\\nwords or text segments in one of the texts and retrieve their counterpart\\ntranslations in the other text ---an application usually referred to as\\ntranslation spotting--- with only some minor modifications in the compression\\nalgorithm.\\n',\n",
              " '  This paper presents a tree-to-tree transduction method for sentence\\ncompression. Our model is based on synchronous tree substitution grammar, a\\nformalism that allows local distortion of the tree topology and can thus\\nnaturally capture structural mismatches. We describe an algorithm for decoding\\nin this framework and show how the model can be trained discriminatively within\\na large margin framework. Experimental results on sentence compression bring\\nsignificant improvements over a state-of-the-art model.\\n',\n",
              " '  This article considers the task of automatically inducing role-semantic\\nannotations in the FrameNet paradigm for new languages. We propose a general\\nframework that is based on annotation projection, phrased as a graph\\noptimization problem. It is relatively inexpensive and has the potential to\\nreduce the human effort involved in creating role-semantic resources. Within\\nthis framework, we present projection models that exploit lexical and syntactic\\ninformation. We provide an experimental evaluation on an English-German\\nparallel corpus which demonstrates the feasibility of inducing high-precision\\nGerman semantic role annotation both for manually and automatically annotated\\nEnglish data.\\n',\n",
              " '  We demonstrate the effectiveness of multilingual learning for unsupervised\\npart-of-speech tagging. The central assumption of our work is that by combining\\ncues from multiple languages, the structure of each becomes more apparent. We\\nconsider two ways of applying this intuition to the problem of unsupervised\\npart-of-speech tagging: a model that directly merges tag structures for a pair\\nof languages into a single sequence and a second model which instead\\nincorporates multilingual context using latent variables. Both approaches are\\nformulated as hierarchical Bayesian models, using Markov Chain Monte Carlo\\nsampling techniques for inference. Our results demonstrate that by\\nincorporating multilingual evidence we can achieve impressive performance gains\\nacross a range of scenarios. We also found that performance improves steadily\\nas the number of available languages increases.\\n',\n",
              " '  The task of identifying synonymous relations and objects, or synonym\\nresolution, is critical for high-quality information extraction. This paper\\ninvestigates synonym resolution in the context of unsupervised information\\nextraction, where neither hand-tagged training examples nor domain knowledge is\\navailable. The paper presents a scalable, fully-implemented system that runs in\\nO(KN log N) time in the number of extractions, N, and the maximum number of\\nsynonyms per word, K. The system, called Resolver, introduces a probabilistic\\nrelational model for predicting whether two strings are co-referential based on\\nthe similarity of the assertions containing them. On a set of two million\\nassertions extracted from the Web, Resolver resolves objects with 78% precision\\nand 68% recall, and resolves relations with 90% precision and 35% recall.\\nSeveral variations of resolvers probabilistic model are explored, and\\nexperiments demonstrate that under appropriate conditions these variations can\\nimprove F1 by 5%. An extension to the basic Resolver system allows it to handle\\npolysemous names with 97% precision and 95% recall on a data set from the TREC\\ncorpus.\\n',\n",
              " '  Adequate representation of natural language semantics requires access to vast\\namounts of common sense and domain-specific world knowledge. Prior work in the\\nfield was based on purely statistical techniques that did not make use of\\nbackground knowledge, on limited lexicographic knowledge bases such as WordNet,\\nor on huge manual efforts such as the CYC project. Here we propose a novel\\nmethod, called Explicit Semantic Analysis (ESA), for fine-grained semantic\\ninterpretation of unrestricted natural language texts. Our method represents\\nmeaning in a high-dimensional space of concepts derived from Wikipedia, the\\nlargest encyclopedia in existence. We explicitly represent the meaning of any\\ntext in terms of Wikipedia-based concepts. We evaluate the effectiveness of our\\nmethod on text categorization and on computing the degree of semantic\\nrelatedness between fragments of natural language text. Using ESA results in\\nsignificant improvements over the previous state of the art in both tasks.\\nImportantly, due to the use of natural concepts, the ESA model is easy to\\nexplain to human users.\\n',\n",
              " '  In a significant minority of cases, certain pronouns, especially the pronoun\\nit, can be used without referring to any specific entity. This phenomenon of\\npleonastic pronoun usage poses serious problems for systems aiming at even a\\nshallow understanding of natural language texts. In this paper, a novel\\napproach is proposed to identify such uses of it: the extrapositional cases are\\nidentified using a series of queries against the web, and the cleft cases are\\nidentified using a simple set of syntactic rules. The system is evaluated with\\nfour sets of news articles containing 679 extrapositional cases as well as 78\\ncleft constructs. The identification results are comparable to those obtained\\nby human efforts.\\n',\n",
              " '  The computation of relatedness between two fragments of text in an automated\\nmanner requires taking into account a wide range of factors pertaining to the\\nmeaning the two fragments convey, and the pairwise relations between their\\nwords. Without doubt, a measure of relatedness between text segments must take\\ninto account both the lexical and the semantic relatedness between words. Such\\na measure that captures well both aspects of text relatedness may help in many\\ntasks, such as text retrieval, classification and clustering. In this paper we\\npresent a new approach for measuring the semantic relatedness between words\\nbased on their implicit semantic links. The approach exploits only a word\\nthesaurus in order to devise implicit semantic links between words. Based on\\nthis approach, we introduce Omiotis, a new measure of semantic relatedness\\nbetween texts which capitalizes on the word-to-word semantic relatedness\\nmeasure (SR) and extends it to measure the relatedness between texts. We\\ngradually validate our method: we first evaluate the performance of the\\nsemantic relatedness measure between individual words, covering word-to-word\\nsimilarity and relatedness, synonym identification and word analogy; then, we\\nproceed with evaluating the performance of our method in measuring text-to-text\\nsemantic relatedness in two tasks, namely sentence-to-sentence similarity and\\nparaphrase recognition. Experimental evaluation shows that the proposed method\\noutperforms every lexicon-based method of semantic relatedness in the selected\\ntasks and the used data sets, and competes well against corpus-based and hybrid\\napproaches.\\n',\n",
              " '  This paper describes a method for the automatic inference of structural\\ntransfer rules to be used in a shallow-transfer machine translation (MT) system\\nfrom small parallel corpora. The structural transfer rules are based on\\nalignment templates, like those used in statistical MT. Alignment templates are\\nextracted from sentence-aligned parallel corpora and extended with a set of\\nrestrictions which are derived from the bilingual dictionary of the MT system\\nand control their application as transfer rules. The experiments conducted\\nusing three different language pairs in the free/open-source MT platform\\nApertium show that translation quality is improved as compared to word-for-word\\ntranslation (when no transfer rules are used), and that the resulting\\ntranslation quality is close to that obtained using hand-coded transfer rules.\\nThe method we present is entirely unsupervised and benefits from information in\\nthe rest of modules of the MT system in which the inferred rules are applied.\\n',\n",
              " \"  Compact closed categories have found applications in modeling quantum\\ninformation protocols by Abramsky-Coecke. They also provide semantics for\\nLambek's pregroup algebras, applied to formalizing the grammatical structure of\\nnatural language, and are implicit in a distributional model of word meaning\\nbased on vector spaces. Specifically, in previous work Coecke-Clark-Sadrzadeh\\nused the product category of pregroups with vector spaces and provided a\\ndistributional model of meaning for sentences. We recast this theory in terms\\nof strongly monoidal functors and advance it via Frobenius algebras over vector\\nspaces. The former are used to formalize topological quantum field theories by\\nAtiyah and Baez-Dolan, and the latter are used to model classical data in\\nquantum protocols by Coecke-Pavlovic-Vicary. The Frobenius algebras enable us\\nto work in a single space in which meanings of words, phrases, and sentences of\\nany structure live. Hence we can compare meanings of different language\\nconstructs and enhance the applicability of the theory. We report on\\nexperimental results on a number of language tasks and verify the theoretical\\npredictions.\\n\",\n",
              " '  Semantic parsing, i.e., the automatic derivation of meaning representation\\nsuch as an instantiated predicate-argument structure for a sentence, plays a\\ncritical role in deep processing of natural language. Unlike all other top\\nsystems of semantic dependency parsing that have to rely on a pipeline\\nframework to chain up a series of submodels each specialized for a specific\\nsubtask, the one presented in this article integrates everything into one\\nmodel, in hopes of achieving desirable integrity and practicality for real\\napplications while maintaining a competitive performance. This integrative\\napproach tackles semantic parsing as a word pair classification problem using a\\nmaximum entropy classifier. We leverage adaptive pruning of argument candidates\\nand large-scale feature selection engineering to allow the largest feature\\nspace ever in use so far in this field, it achieves a state-of-the-art\\nperformance on the evaluation data set for CoNLL-2008 shared task, on top of\\nall but one top pipeline system, confirming its feasibility and effectiveness.\\n',\n",
              " '  One of the key issues in both natural language understanding and generation\\nis the appropriate processing of Multiword Expressions (MWEs). MWEs pose a huge\\nproblem to the precise language processing due to their idiosyncratic nature\\nand diversity in lexical, syntactical and semantic properties. The semantics of\\na MWE cannot be expressed after combining the semantics of its constituents.\\nTherefore, the formalism of semantic clustering is often viewed as an\\ninstrument for extracting MWEs especially for resource constraint languages\\nlike Bengali. The present semantic clustering approach contributes to locate\\nclusters of the synonymous noun tokens present in the document. These clusters\\nin turn help measure the similarity between the constituent words of a\\npotentially candidate phrase using a vector space model and judge the\\nsuitability of this phrase to be a MWE. In this experiment, we apply the\\nsemantic clustering approach for noun-noun bigram MWEs, though it can be\\nextended to any types of MWEs. In parallel, the well known statistical models,\\nnamely Point-wise Mutual Information (PMI), Log Likelihood Ratio (LLR),\\nSignificance function are also employed to extract MWEs from the Bengali\\ncorpus. The comparative evaluation shows that the semantic clustering approach\\noutperforms all other competing statistical models. As a by-product of this\\nexperiment, we have started developing a standard lexicon in Bengali that\\nserves as a productive Bengali linguistic thesaurus.\\n',\n",
              " '  We consider the problem of fully unsupervised learning of grammatical\\n(part-of-speech) categories from unlabeled text. The standard\\nmaximum-likelihood hidden Markov model for this task performs poorly, because\\nof its weak inductive bias and large model capacity. We address this problem by\\nrefining the model and modifying the learning objective to control its capacity\\nvia para- metric and non-parametric constraints. Our approach enforces\\nword-category association sparsity, adds morphological and orthographic\\nfeatures, and eliminates hard-to-estimate parameters for rare words. We develop\\nan efficient learning algorithm that is not much more computationally intensive\\nthan standard training. We also provide an open-source implementation of the\\nalgorithm. Our experiments on five diverse languages (Bulgarian, Danish,\\nEnglish, Portuguese, Spanish) achieve significant improvements compared with\\nprevious methods for the same task.\\n',\n",
              " '  We study the frequency distributions and correlations of the word lengths of\\nten European languages. Our findings indicate that a) the word-length\\ndistribution of short words quantified by the mean value and the entropy\\ndistinguishes the Uralic (Finnish) corpus from the others, b) the tails at long\\nwords, manifested in the high-order moments of the distributions, differentiate\\nthe Germanic languages (except for English) from the Romanic languages and\\nGreek and c) the correlations between nearby word lengths measured by the\\ncomparison of the real entropies with those of the shuffled texts are found to\\nbe smaller in the case of Germanic and Finnish languages.\\n',\n",
              " \"  We present a statistical parsing framework for sentence-level sentiment\\nclassification in this article. Unlike previous works that employ syntactic\\nparsing results for sentiment analysis, we develop a statistical parser to\\ndirectly analyze the sentiment structure of a sentence. We show that\\ncomplicated phenomena in sentiment analysis (e.g., negation, intensification,\\nand contrast) can be handled the same as simple and straightforward sentiment\\nexpressions in a unified and probabilistic way. We formulate the sentiment\\ngrammar upon Context-Free Grammars (CFGs), and provide a formal description of\\nthe sentiment parsing framework. We develop the parsing model to obtain\\npossible sentiment parse trees for a sentence, from which the polarity model is\\nproposed to derive the sentiment strength and polarity, and the ranking model\\nis dedicated to selecting the best sentiment tree. We train the parser directly\\nfrom examples of sentences annotated only with sentiment polarity labels but\\nwithout any syntactic annotations or polarity annotations of constituents\\nwithin sentences. Therefore we can obtain training data easily. In particular,\\nwe train a sentiment parser, s.parser, from a large amount of review sentences\\nwith users' ratings as rough sentiment polarity labels. Extensive experiments\\non existing benchmark datasets show significant improvements over baseline\\nsentiment classification approaches.\\n\",\n",
              " '  We present a model for aggregation of product review snippets by joint aspect\\nidentification and sentiment analysis. Our model simultaneously identifies an\\nunderlying set of ratable aspects presented in the reviews of a product (e.g.,\\nsushi and miso for a Japanese restaurant) and determines the corresponding\\nsentiment of each aspect. This approach directly enables discovery of\\nhighly-rated or inconsistent aspects of a product. Our generative model admits\\nan efficient variational mean-field inference algorithm. It is also easily\\nextensible, and we describe several modifications and their effects on model\\nstructure and inference. We test our model on two tasks, joint aspect\\nidentification and sentiment analysis on a set of Yelp reviews and aspect\\nidentification alone on a set of medical summaries. We evaluate the performance\\nof the model on aspect identification, sentiment analysis, and per-word\\nlabeling accuracy. We demonstrate that our model outperforms applicable\\nbaselines by a considerable margin, yielding up to 32% relative error reduction\\non aspect identification and up to 20% relative error reduction on sentiment\\nanalysis.\\n',\n",
              " '  This paper presents a machine learning approach for identification of Bengali\\nmultiword expressions (MWE) which are bigram nominal compounds. Our proposed\\napproach has two steps: (1) candidate extraction using chunk information and\\nvarious heuristic rules and (2) training the machine learning algorithm called\\nRandom Forest to classify the candidates into two groups: bigram nominal\\ncompound MWE or not bigram nominal compound MWE. A variety of association\\nmeasures, syntactic and linguistic clues and a set of WordNet-based similarity\\nfeatures have been used for our MWE identification task. The approach presented\\nin this paper can be used to identify bigram nominal compound MWE in Bengali\\nrunning text.\\n',\n",
              " '  Keyword and keyphrase extraction is an important problem in natural language\\nprocessing, with applications ranging from summarization to semantic search to\\ndocument clustering. Graph-based approaches to keyword and keyphrase extraction\\navoid the problem of acquiring a large in-domain training corpus by applying\\nvariants of PageRank algorithm on a network of words. Although graph-based\\napproaches are knowledge-lean and easily adoptable in online systems, it\\nremains largely open whether they can benefit from centrality measures other\\nthan PageRank. In this paper, we experiment with an array of centrality\\nmeasures on word and noun phrase collocation networks, and analyze their\\nperformance on four benchmark datasets. Not only are there centrality measures\\nthat perform as well as or better than PageRank, but they are much simpler\\n(e.g., degree, strength, and neighborhood size). Furthermore, centrality-based\\nmethods give results that are competitive with and, in some cases, better than\\ntwo strong unsupervised baselines.\\n',\n",
              " \"  We propose a lexical account of action nominals, in particular of deverbal\\nnominalisations, whose meaning is related to the event expressed by their base\\nverb. The literature about nominalisations often assumes that the semantics of\\nthe base verb completely defines the structure of action nominals. We argue\\nthat the information in the base verb is not sufficient to completely determine\\nthe semantics of action nominals. We exhibit some data from different\\nlanguages, especially from Romance language, which show that nominalisations\\nfocus on some aspects of the verb semantics. The selected aspects, however,\\nseem to be idiosyncratic and do not automatically result from the internal\\nstructure of the verb nor from its interaction with the morphological suffix.\\nWe therefore propose a partially lexicalist approach view of deverbal nouns. It\\nis made precise and computable by using the Montagovian Generative Lexicon, a\\ntype theoretical framework introduced by Bassac, Mery and Retor\\\\'e in this\\njournal in 2010. This extension of Montague semantics with a richer type system\\neasily incorporates lexical phenomena like the semantics of action nominals in\\nparticular deverbals, including their polysemy and (in)felicitous\\ncopredications.\\n\",\n",
              " '  To tackle the vocabulary problem in conversational systems, previous work has\\napplied unsupervised learning approaches on co-occurring speech and eye gaze\\nduring interaction to automatically acquire new words. Although these\\napproaches have shown promise, several issues related to human language\\nbehavior and human-machine conversation have not been addressed. First,\\npsycholinguistic studies have shown certain temporal regularities between human\\neye movement and language production. While these regularities can potentially\\nguide the acquisition process, they have not been incorporated in the previous\\nunsupervised approaches. Second, conversational systems generally have an\\nexisting knowledge base about the domain and vocabulary. While the existing\\nknowledge can potentially help bootstrap and constrain the acquired new words,\\nit has not been incorporated in the previous models. Third, eye gaze could\\nserve different functions in human-machine conversation. Some gaze streams may\\nnot be closely coupled with speech stream, and thus are potentially detrimental\\nto word acquisition. Automated recognition of closely-coupled speech-gaze\\nstreams based on conversation context is important. To address these issues, we\\ndeveloped new approaches that incorporate user language behavior, domain\\nknowledge, and conversation context in word acquisition. We evaluated these\\napproaches in the context of situated dialogue in a virtual world. Our\\nexperimental results have shown that incorporating the above three types of\\ncontextual information significantly improves word acquisition performance.\\n',\n",
              " '  We propose a novel language-independent approach for improving machine\\ntranslation for resource-poor languages by exploiting their similarity to\\nresource-rich ones. More precisely, we improve the translation from a\\nresource-poor source language X_1 into a resource-rich language Y given a\\nbi-text containing a limited number of parallel sentences for X_1-Y and a\\nlarger bi-text for X_2-Y for some resource-rich language X_2 that is closely\\nrelated to X_1. This is achieved by taking advantage of the opportunities that\\nvocabulary overlap and similarities between the languages X_1 and X_2 in\\nspelling, word order, and syntax offer: (1) we improve the word alignments for\\nthe resource-poor language, (2) we further augment it with additional\\ntranslation options, and (3) we take care of potential spelling differences\\nthrough appropriate transliteration. The evaluation for Indonesian- >English\\nusing Malay and for Spanish -> English using Portuguese and pretending Spanish\\nis resource-poor shows an absolute gain of up to 1.35 and 3.37 BLEU points,\\nrespectively, which is an improvement over the best rivaling approaches, while\\nusing much less additional data. Overall, our method cuts the amount of\\nnecessary \"real training data by a factor of 2--5.\\n',\n",
              " \"  We measured entropy and symbolic diversity for English and Spanish texts\\nincluding literature Nobel laureates and other famous authors. Entropy, symbol\\ndiversity and symbol frequency profiles were compared for these four groups. We\\nalso built a scale sensitive to the quality of writing and evaluated its\\nrelationship with the Flesch's readability index for English and the\\nSzigriszt's perspicuity index for Spanish. Results suggest a correlation\\nbetween entropy and word diversity with quality of writing. Text genre also\\ninfluences the resulting entropy and diversity of the text. Results suggest the\\nplausibility of automated quality assessment of texts.\\n\",\n",
              " '  Inference in natural language often involves recognizing lexical entailment\\n(RLE); that is, identifying whether one word entails another. For example,\\n\"buy\" entails \"own\". Two general strategies for RLE have been proposed: One\\nstrategy is to manually construct an asymmetric similarity measure for context\\nvectors (directional similarity) and another is to treat RLE as a problem of\\nlearning to recognize semantic relations using supervised machine learning\\ntechniques (relation classification). In this paper, we experiment with two\\nrecent state-of-the-art representatives of the two general strategies. The\\nfirst approach is an asymmetric similarity measure (an instance of the\\ndirectional similarity strategy), designed to capture the degree to which the\\ncontexts of a word, a, form a subset of the contexts of another word, b. The\\nsecond approach (an instance of the relation classification strategy)\\nrepresents a word pair, a:b, with a feature vector that is the concatenation of\\nthe context vectors of a and b, and then applies supervised learning to a\\ntraining set of labeled feature vectors. Additionally, we introduce a third\\napproach that is a new instance of the relation classification strategy. The\\nthird approach represents a word pair, a:b, with a feature vector in which the\\nfeatures are the differences in the similarities of a and b to a set of\\nreference words. All three approaches use vector space models (VSMs) of\\nsemantics, based on word-context matrices. We perform an extensive evaluation\\nof the three approaches using three different datasets. The proposed new\\napproach (similarity differences) performs significantly better than the other\\ntwo approaches on some datasets and there is no dataset for which it is\\nsignificantly worse. Our results suggest it is beneficial to make connections\\nbetween the research in lexical entailment and the research in semantic\\nrelation classification.\\n',\n",
              " '  By using a small example, an analogy to photographic compression, and a\\nsimple visualization using heatmaps, we show that latent semantic analysis\\n(LSA) is able to extract what appears to be semantic meaning of words from a\\nset of documents by blurring the distinctions between the words.\\n',\n",
              " '  Although, Chinese and Spanish are two of the most spoken languages in the\\nworld, not much research has been done in machine translation for this language\\npair. This paper focuses on investigating the state-of-the-art of\\nChinese-to-Spanish statistical machine translation (SMT), which nowadays is one\\nof the most popular approaches to machine translation. For this purpose, we\\nreport details of the available parallel corpus which are Basic Traveller\\nExpressions Corpus (BTEC), Holy Bible and United Nations (UN). Additionally, we\\nconduct experimental work with the largest of these three corpora to explore\\nalternative SMT strategies by means of using a pivot language. Three\\nalternatives are considered for pivoting: cascading, pseudo-corpus and\\ntriangulation. As pivot language, we use either English, Arabic or French.\\nResults show that, for a phrase-based SMT system, English is the best pivot\\nlanguage between Chinese and Spanish. We propose a system output combination\\nusing the pivot strategies which is capable of outperforming the direct\\ntranslation strategy. The main objective of this work is motivating and\\ninvolving the research community to work in this important pair of languages\\ngiven their demographic impact.\\n',\n",
              " '  Given a current news event, we tackle the problem of generating plausible\\npredictions of future events it might cause. We present a new methodology for\\nmodeling and predicting such future news events using machine learning and data\\nmining techniques. Our Pundit algorithm generalizes examples of causality pairs\\nto infer a causality predictor. To obtain precisely labeled causality examples,\\nwe mine 150 years of news articles and apply semantic natural language modeling\\ntechniques to headlines containing certain predefined causality patterns. For\\ngeneralization, the model uses a vast number of world knowledge ontologies.\\nEmpirical evaluation on real news articles shows that our Pundit algorithm\\nperforms as well as non-expert humans.\\n',\n",
              " '  Many natural language processing (NLP) applications require the computation\\nof similarities between pairs of syntactic or semantic trees. Many researchers\\nhave used tree edit distance for this task, but this technique suffers from the\\ndrawback that it deals with single node operations only. We have extended the\\nstandard tree edit distance algorithm to deal with subtree transformation\\noperations as well as single nodes. The extended algorithm with subtree\\noperations, TED+ST, is more effective and flexible than the standard algorithm,\\nespecially for applications that pay attention to relations among nodes (e.g.\\nin linguistic trees, deleting a modifier subtree should be cheaper than the sum\\nof deleting its components individually). We describe the use of TED+ST for\\nchecking entailment between two Arabic text snippets. The preliminary results\\nof using TED+ST were encouraging when compared with two string-based approaches\\nand with the standard algorithm.\\n',\n",
              " '  Topic segmentation and labeling is often considered a prerequisite for\\nhigher-level conversation analysis and has been shown to be useful in many\\nNatural Language Processing (NLP) applications. We present two new corpora of\\nemail and blog conversations annotated with topics, and evaluate annotator\\nreliability for the segmentation and labeling tasks in these asynchronous\\nconversations. We propose a complete computational framework for topic\\nsegmentation and labeling in asynchronous conversations. Our approach extends\\nstate-of-the-art methods by considering a fine-grained structure of an\\nasynchronous conversation, along with other conversational features by applying\\nrecent graph-based methods for NLP. For topic segmentation, we propose two\\nnovel unsupervised models that exploit the fine-grained conversational\\nstructure, and a novel graph-theoretic supervised model that combines lexical,\\nconversational and topic features. For topic labeling, we propose two novel\\n(unsupervised) random walk models that respectively capture conversation\\nspecific clues from two different sources: the leading sentences and the\\nfine-grained conversational structure. Empirical evaluation shows that the\\nsegmentation and the labeling performed by our best models beat the\\nstate-of-the-art, and are highly correlated with human annotations.\\n',\n",
              " '  Cross-language learning allows us to use training data from one language to\\nbuild models for a different language. Many approaches to bilingual learning\\nrequire that we have word-level alignment of sentences from parallel corpora.\\nIn this work we explore the use of autoencoder-based methods for cross-language\\nlearning of vectorial word representations that are aligned between two\\nlanguages, while not relying on word-level alignments. We show that by simply\\nlearning to reconstruct the bag-of-words representations of aligned sentences,\\nwithin and between languages, we can in fact learn high-quality representations\\nand do without word alignments. Since training autoencoders on word\\nobservations presents certain computational issues, we propose and compare\\ndifferent variations adapted to this setting. We also propose an explicit\\ncorrelation maximizing regularizer that leads to significant improvement in the\\nperformance. We empirically investigate the success of our approach on the\\nproblem of cross-language test classification, where a classifier trained on a\\ngiven language (e.g., English) must learn to generalize to a different language\\n(e.g., German). These experiments demonstrate that our approaches are\\ncompetitive with the state-of-the-art, achieving up to 10-14 percentage point\\nimprovements over the best reported results on this task.\\n',\n",
              " '  Bilingual machine-readable dictionaries are knowledge resources useful in\\nmany automatic tasks. However, compared to monolingual computational lexicons\\nlike WordNet, bilingual dictionaries typically provide a lower amount of\\nstructured information, such as lexical and semantic relations, and often do\\nnot cover the entire range of possible translations for a word of interest. In\\nthis paper we present Cycles and Quasi-Cycles (CQC), a novel algorithm for the\\nautomated disambiguation of ambiguous translations in the lexical entries of a\\nbilingual machine-readable dictionary. The dictionary is represented as a\\ngraph, and cyclic patterns are sought in the graph to assign an appropriate\\nsense tag to each translation in a lexical entry. Further, we use the\\nalgorithms output to improve the quality of the dictionary itself, by\\nsuggesting accurate solutions to structural problems such as misalignments,\\npartial alignments and missing entries. Finally, we successfully apply CQC to\\nthe task of synonym extraction.\\n',\n",
              " '  We present PR2, a personality recognition system available online, that\\nperforms instance-based classification of Big5 personality types from\\nunstructured text, using language-independent features. It has been tested on\\nEnglish and Italian, achieving performances up to f=.68.\\n',\n",
              " '  Module-Attribute Representation of Verbal Semantics (MARVS) is a theory of\\nthe representation of verbal semantics that is based on Mandarin Chinese data\\n(Huang et al. 2000). In the MARVS theory, there are two different types of\\nmodules: Event Structure Modules and Role Modules. There are also two sets of\\nattributes: Event-Internal Attributes and Role-Internal Attributes, which are\\nlinked to the Event Structure Module and the Role Module, respectively. In this\\nstudy, we focus on four transitive verbs as chi1(eat), wan2(play),\\nhuan4(change) and shao1(burn) and explore their event structures by the MARVS\\ntheory.\\n',\n",
              " '  Speech analysis had been taken to a new level with the discovery of Reverse\\nSpeech (RS). RS is the discovery of hidden messages, referred as reversals, in\\nnormal speech. Works are in progress for exploiting the relevance of RS in\\ndifferent real world applications such as investigation, medical field etc. In\\nthis paper we represent an innovative method for preparing a reliable Software\\nRequirement Specification (SRS) document with the help of reverse speech. As\\nSRS act as the backbone for the successful completion of any project, a\\nreliable method is needed to overcome the inconsistencies. Using RS such a\\nreliable method for SRS documentation was developed.\\n',\n",
              " \"  In geographic information science and semantics, the computation of semantic\\nsimilarity is widely recognised as key to supporting a vast number of tasks in\\ninformation integration and retrieval. By contrast, the role of geo-semantic\\nrelatedness has been largely ignored. In natural language processing, semantic\\nrelatedness is often confused with the more specific semantic similarity. In\\nthis article, we discuss a notion of geo-semantic relatedness based on Lehrer's\\nsemantic fields, and we compare it with geo-semantic similarity. We then\\ndescribe and validate the Geo Relatedness and Similarity Dataset (GeReSiD), a\\nnew open dataset designed to evaluate computational measures of geo-semantic\\nrelatedness and similarity. This dataset is larger than existing datasets of\\nthis kind, and includes 97 geographic terms combined into 50 term pairs rated\\nby 203 human subjects. GeReSiD is available online and can be used as an\\nevaluation baseline to determine empirically to what degree a given\\ncomputational model approximates geo-semantic relatedness and similarity.\\n\",\n",
              " '  This paper presents machine learning solutions to a practical problem of\\nNatural Language Generation (NLG), particularly the word formation in\\nagglutinative languages like Tamil, in a supervised manner. The morphological\\ngenerator is an important component of Natural Language Processing in\\nArtificial Intelligence. It generates word forms given a root and affixes. The\\nmorphophonemic changes like addition, deletion, alternation etc., occur when\\ntwo or more morphemes or words joined together. The Sandhi rules should be\\nexplicitly specified in the rule based morphological analyzers and generators.\\nIn machine learning framework, these rules can be learned automatically by the\\nsystem from the training samples and subsequently be applied for new inputs. In\\nthis paper we proposed the machine learning models which learn the\\nmorphophonemic rules for noun declensions from the given training data. These\\nmodels are trained to learn sandhi rules using various learning algorithms and\\nthe performance of those algorithms are presented. From this we conclude that\\nmachine learning of morphological processing such as word form generation can\\nbe successfully learned in a supervised manner, without explicit description of\\nrules. The performance of Decision trees and Bayesian machine learning\\nalgorithms on noun declensions are discussed.\\n',\n",
              " '  This paper proposes to perform authorship analysis using the Fast Compression\\nDistance (FCD), a similarity measure based on compression with dictionaries\\ndirectly extracted from the written texts. The FCD computes a similarity\\nbetween two documents through an effective binary search on the intersection\\nset between the two related dictionaries. In the reported experiments the\\nproposed method is applied to documents which are heterogeneous in style,\\nwritten in five different languages and coming from different historical\\nperiods. Results are comparable to the state of the art and outperform\\ntraditional compression-based methods.\\n',\n",
              " '  The goal of Text-to-Speech (TTS) synthesis in a particular language is to\\nconvert arbitrary input text to intelligible and natural sounding speech.\\nHowever, for a particular language like Hindi, which is a highly confusing\\nlanguage (due to very close spellings), it is not an easy task to identify\\nerrors/mistakes in input text and an incorrect text degrade the quality of\\noutput speech hence this paper is a contribution to the development of high\\nquality speech synthesis with the involvement of Spellchecker which generates\\nspell suggestions for misspelled words automatically. Involvement of\\nspellchecker would increase the efficiency of speech synthesis by providing\\nspell suggestions for incorrect input text. Furthermore, we have provided the\\ncomparative study for evaluating the resultant effect on to phonetic text by\\nadding spellchecker on to input text.\\n',\n",
              " '  The word2vec software of Tomas Mikolov and colleagues\\n(https://code.google.com/p/word2vec/ ) has gained a lot of traction lately, and\\nprovides state-of-the-art word embeddings. The learning models behind the\\nsoftware are described in two research papers. We found the description of the\\nmodels in these papers to be somewhat cryptic and hard to follow. While the\\nmotivations and presentation may be obvious to the neural-networks\\nlanguage-modeling crowd, we had to struggle quite a bit to figure out the\\nrationale behind the equations.\\n  This note is an attempt to explain equation (4) (negative sampling) in\\n\"Distributed Representations of Words and Phrases and their Compositionality\"\\nby Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado and Jeffrey Dean.\\n',\n",
              " '  A Verbal Autopsy is the record of an interview about the circumstances of an\\nuncertified death. In developing countries, if a death occurs away from health\\nfacilities, a field-worker interviews a relative of the deceased about the\\ncircumstances of the death; this Verbal Autopsy can be reviewed off-site. We\\nreport on a comparative study of the processes involved in Text Classification\\napplied to classifying Cause of Death: feature value representation; machine\\nlearning classification algorithms; and feature reduction strategies in order\\nto identify the suitable approaches applicable to the classification of Verbal\\nAutopsy text. We demonstrate that normalised term frequency and the standard\\nTFiDF achieve comparable performance across a number of classifiers. The\\nresults also show Support Vector Machine is superior to other classification\\nalgorithms employed in this research. Finally, we demonstrate the effectiveness\\nof employing a \"locally-semi-supervised\" feature reduction strategy in order to\\nincrease performance accuracy.\\n',\n",
              " \"  We present a new algorithm to model and investigate the learning process of a\\nlearner mastering a set of grammatical rules from an inconsistent source. The\\ncompelling interest of human language acquisition is that the learning succeeds\\nin virtually every case, despite the fact that the input data are formally\\ninadequate to explain the success of learning. Our model explains how a learner\\ncan successfully learn from or even surpass its imperfect source without\\npossessing any additional biases or constraints about the types of patterns\\nthat exist in the language. We use the data collected by Singleton and Newport\\n(2004) on the performance of a 7-year boy Simon, who mastered the American Sign\\nLanguage (ASL) by learning it from his parents, both of whom were imperfect\\nspeakers of ASL. We show that the algorithm possesses a frequency-boosting\\nproperty, whereby the frequency of the most common form of the source is\\nincreased by the learner. We also explain several key features of Simon's ASL.\\n\",\n",
              " \"  Given the incessant growth of documents describing the opinions of different\\npeople circulating on the web, including Web 2.0 has made it possible to give\\nan opinion on any product in the net. In this paper, we examine the various\\nopinions expressed in the tweets and classify them positive, negative or\\nneutral by using the emoticons for the Bayesian method and adjectives and\\nadverbs for the Turney's method\\n\",\n",
              " '  Automatically inducing the syntactic part-of-speech categories for words in\\ntext is a fundamental task in Computational Linguistics. While the performance\\nof unsupervised tagging models has been slowly improving, current\\nstate-of-the-art systems make the obviously incorrect assumption that all\\ntokens of a given word type must share a single part-of-speech tag. This\\none-tag-per-type heuristic counters the tendency of Hidden Markov Model based\\ntaggers to over generate tags for a given word type. However, it is clearly\\nincompatible with basic syntactic theory. In this paper we extend a\\nstate-of-the-art Pitman-Yor Hidden Markov Model tagger with an explicit model\\nof the lexicon. In doing so we are able to incorporate a soft bias towards\\ninducing few tags per type. We develop a particle filter for drawing samples\\nfrom the posterior of our model and present empirical results that show that\\nour model is competitive with and faster than the state-of-the-art without\\nmaking any unrealistic restrictions.\\n',\n",
              " '  The textual, big-data literature misses Bentley, OBrien, & Brocks (Bentley et\\nals) message on distributions; it largely examines the first-order effects of\\nhow a single, signature distribution can predict population behaviour,\\nneglecting second-order effects involving distributional shifts, either between\\nsignature distributions or within a given signature distribution. Indeed,\\nBentley et al. themselves under-emphasise the potential richness of the latter,\\nwithin-distribution effects.\\n',\n",
              " '  Over the past 50 years many have debated what representation should be used\\nto capture the meaning of natural language utterances. Recently new needs of\\nsuch representations have been raised in research. Here I survey some of the\\ninteresting representations suggested to answer for these new needs.\\n',\n",
              " '  This paper presents an attempt to customise the TEI (Text Encoding\\nInitiative) guidelines in order to offer the possibility to incorporate TBX\\n(TermBase eXchange) based terminological entries within any kind of TEI\\ndocuments. After presenting the general historical, conceptual and technical\\ncontexts, we describe the various design choices we had to take while creating\\nthis customisation, which in turn have led to make various changes in the\\nactual TBX serialisation. Keeping in mind the objective to provide the TEI\\nguidelines with, again, an onomasiological model, we try to identify the best\\ncomprise in maintaining both the isomorphism with the existing TBX Basic\\nstandard and the characteristics of the TEI framework.\\n',\n",
              " \"  Modalities of communication for human beings are gradually increasing in\\nnumber with the advent of new forms of technology. Many human beings can\\nreadily transition between these different forms of communication with little\\nor no effort, which brings about the question: How similar are these different\\ncommunication modalities? To understand technology$\\\\text{'}$s influence on\\nEnglish communication, four different corpora were analyzed and compared:\\nWriting from Books using the 1-grams database from the Google Books project,\\nTwitter, IRC Chat, and transcribed Talking. Multi-word confusion matrices\\nrevealed that Talking has the most similarity when compared to the other modes\\nof communication, while 1-grams were the least similar form of communication\\nanalyzed. Based on the analysis of word usage, word usage frequency\\ndistributions, and word class usage, among other things, Talking is also the\\nmost similar to Twitter and IRC Chat. This suggests that communicating using\\nTwitter and IRC Chat evolved from Talking rather than Writing. When we\\ncommunicate online, even though we are writing, we do not Tweet or Chat how we\\nwrite books; we Tweet and Chat how we Speak. Nonfiction and Fiction writing\\nwere clearly differentiable from our analysis with Twitter and Chat being much\\nmore similar to Fiction than Nonfiction writing. These hypotheses were then\\ntested using author and journalists Cory Doctorow. Mr. Doctorow$\\\\text{'}$s\\nWriting, Twitter usage, and Talking were all found to have very similar\\nvocabulary usage patterns as the amalgamized populations, as long as the\\nwriting was Fiction. However, Mr. Doctorow$\\\\text{'}$s Nonfiction writing is\\ndifferent from 1-grams and other collected Nonfiction writings. This data could\\nperhaps be used to create more entertaining works of Nonfiction.\\n\",\n",
              " '  Developments in the educational landscape have spurred greater interest in\\nthe problem of automatically scoring short answer questions. A recent shared\\ntask on this topic revealed a fundamental divide in the modeling approaches\\nthat have been applied to this problem, with the best-performing systems split\\nbetween those that employ a knowledge engineering approach and those that\\nalmost solely leverage lexical information (as opposed to higher-level\\nsyntactic information) in assigning a score to a given response. This paper\\naims to introduce the NLP community to the largest corpus currently available\\nfor short-answer scoring, provide an overview of methods used in the shared\\ntask using this data, and explore the extent to which more\\nsyntactically-informed features can contribute to the short answer scoring task\\nin a way that avoids the question-specific manual effort of the knowledge\\nengineering approach.\\n',\n",
              " '  In this paper, I propose a novel word sense disambiguation method based on\\nthe global co-occurrence information using NMF. When I calculate the dependency\\nrelation matrix, the existing method tends to produce very sparse co-occurrence\\nmatrix from a small training set. Therefore, the NMF algorithm sometimes does\\nnot converge to desired solutions. To obtain a large number of co-occurrence\\nrelations, I propose to use co-occurrence frequencies of dependency relations\\nbetween word features in the whole training set. This enables us to solve data\\nsparseness problem and induce more effective latent features. To evaluate the\\nefficiency of the method of word sense disambiguation, I make some experiments\\nto compare with the result of the two baseline methods. The results of the\\nexperiments show this method is effective for word sense disambiguation in\\ncomparison with the all baseline methods. Moreover, the proposed method is\\neffective for obtaining a stable effect by analyzing the global co-occurrence\\ninformation.\\n',\n",
              " '  SMS messaging is a popular media of communication. Because of its popularity\\nand privacy, it could be used for many illegal purposes. Additionally, since\\nthey are part of the day to day life, SMSes can be used as evidence for many\\nlegal disputes. Since a cellular phone might be accessible to people close to\\nthe owner, it is important to establish the fact that the sender of the message\\nis indeed the owner of the phone. For this purpose, the straight forward\\nsolutions seem to be the use of popular stylometric methods. However, in\\ncomparison with the data used for stylometry in the literature, SMSes have\\nunusual characteristics making it hard or impossible to apply these methods in\\na conventional way. Our target is to come up with a method of authorship\\ndetection of SMS messages that could still give a usable accuracy. We argue\\nthat, considering the methods of author attribution, the best method that could\\nbe applied to SMS messages is an n-gram method. To prove our point, we checked\\ntwo different methods of distribution comparison with varying number of\\ntraining and testing data. We specifically try to compare how well our\\nalgorithms work under less amount of testing data and large number of candidate\\nauthors (which we believe to be the real world scenario) against controlled\\ntests with less number of authors and selected SMSes with large number of\\nwords. To counter the lack of information in an SMS message, we propose the\\nmethod of stacking together few SMSes.\\n',\n",
              " '  Accurately segmenting a citation string into fields for authors, titles, etc.\\nis a challenging task because the output typically obeys various global\\nconstraints. Previous work has shown that modeling soft constraints, where the\\nmodel is encouraged, but not require to obey the constraints, can substantially\\nimprove segmentation performance. On the other hand, for imposing hard\\nconstraints, dual decomposition is a popular technique for efficient prediction\\ngiven existing algorithms for unconstrained inference. We extend the technique\\nto perform prediction subject to soft constraints. Moreover, with a technique\\nfor performing inference given soft constraints, it is easy to automatically\\ngenerate large families of constraints and learn their costs with a simple\\nconvex optimization problem during training. This allows us to obtain\\nsubstantial gains in accuracy on a new, challenging citation extraction\\ndataset.\\n',\n",
              " '  Disaster response agencies have started to incorporate social media as a\\nsource of fast-breaking information to understand the needs of people affected\\nby the many crises that occur around the world. These agencies look for tweets\\nfrom within the region affected by the crisis to get the latest updates of the\\nstatus of the affected region. However only 1% of all tweets are geotagged with\\nexplicit location information. First responders lose valuable information\\nbecause they cannot assess the origin of many of the tweets they collect. In\\nthis work we seek to identify non-geotagged tweets that originate from within\\nthe crisis region. Towards this, we address three questions: (1) is there a\\ndifference between the language of tweets originating within a crisis region\\nand tweets originating outside the region, (2) what are the linguistic patterns\\nthat can be used to differentiate within-region and outside-region tweets, and\\n(3) for non-geotagged tweets, can we automatically identify those originating\\nwithin the crisis region in real-time?\\n',\n",
              " '  Specificity is important for extracting collocations, keyphrases, multi-word\\nand index terms [Newman et al. 2012]. It is also useful for tagging, ontology\\nconstruction [Ryu and Choi 2006], and automatic summarization of documents\\n[Louis and Nenkova 2011, Chali and Hassan 2012]. Term frequency and\\ninverse-document frequency (TF-IDF) are typically used to do this, but fail to\\ntake advantage of the semantic relationships between terms [Church and Gale\\n1995]. The result is that general idiomatic terms are mistaken for specific\\nterms. We demonstrate use of relational data for estimation of term\\nspecificity. The specificity of a term can be learned from its distribution of\\nrelations with other terms. This technique is useful for identifying relevant\\nwords or terms for other natural language processing tasks.\\n',\n",
              " '  We present a system, TransProse, that automatically generates musical pieces\\nfrom text. TransProse uses known relations between elements of music such as\\ntempo and scale, and the emotions they evoke. Further, it uses a novel\\nmechanism to determine sequences of notes that capture the emotional activity\\nin the text. The work has applications in information visualization, in\\ncreating audio-visual e-books, and in developing music apps.\\n',\n",
              " '  This paper was was first drafted in 2001 as a formalization of the system\\ndescribed in U.S. patent U.S. 7,392,174. It describes a system for implementing\\na parser based on a kind of cross-product over vectors of contextually similar\\nwords. It is being published now in response to nascent interest in vector\\ncombination models of syntax and semantics. The method used aggressive\\nsubstitution of contextually similar words and word groups to enable product\\nvectors to stay in the same space as their operands and make entire sentences\\ncomparable syntactically, and potentially semantically. The vectors generated\\nhad sufficient representational strength to generate parse trees at least\\ncomparable with contemporary symbolic parsers.\\n',\n",
              " \"  In this paper, a novel hierarchical Persian stemming approach based on the\\nPart-Of-Speech of the word in a sentence is presented. The implemented stemmer\\nincludes hash tables and several deterministic finite automata in its different\\nlevels of hierarchy for removing the prefixes and suffixes of the words. We had\\ntwo intentions in using hash tables in our method. The first one is that the\\nDFA don't support some special words, so hash table can partly solve the\\naddressed problem. the second goal is to speed up the implemented stemmer with\\nomitting the time that deterministic finite automata need. Because of the\\nhierarchical organization, this method is fast and flexible enough. Our\\nexperiments on test sets from Hamshahri collection and security news (istna.ir)\\nshow that our method has the average accuracy of 95.37% which is even improved\\nin using the method on a test set with common topics.\\n\",\n",
              " '  Requirements are informal and semi-formal descriptions of the expected\\nbehavior of a complex system from the viewpoints of its stakeholders\\n(customers, users, operators, designers, and engineers). However, for the\\npurpose of design, testing, and verification for critical systems, we can\\ntransform requirements into formal models that can be analyzed automatically.\\nARSENAL is a framework and methodology for systematically transforming natural\\nlanguage (NL) requirements into analyzable formal models and logic\\nspecifications. These models can be analyzed for consistency and\\nimplementability. The ARSENAL methodology is specialized to individual domains,\\nbut the approach is general enough to be adapted to new domains.\\n',\n",
              " '  Language is contextual and sheaf theory provides a high level mathematical\\nframework to model contextuality. We show how sheaf theory can model the\\ncontextual nature of natural language and how gluing can be used to provide a\\nglobal semantics for a discourse by putting together the local logical\\nsemantics of each sentence within the discourse. We introduce a presheaf\\nstructure corresponding to a basic form of Discourse Representation Structures.\\nWithin this setting, we formulate a notion of semantic unification --- gluing\\nmeanings of parts of a discourse into a coherent whole --- as a form of\\nsheaf-theoretic gluing. We illustrate this idea with a number of examples where\\nit can used to represent resolutions of anaphoric references. We also discuss\\nmultivalued gluing, described using a distributions functor, which can be used\\nto represent situations where multiple gluings are possible, and where we may\\nneed to rank them using quantitative measures.\\n  Dedicated to Jim Lambek on the occasion of his 90th birthday.\\n',\n",
              " \"  Much of philosophical logic and all of philosophy of language make empirical\\nclaims about the vernacular natural language. They presume semantics under\\nwhich `and' and `or' are related by the dually paired distributive and\\nabsorption laws. However, at least one of each pair of laws fails in the\\nvernacular. `Implicature'-based auxiliary theories associated with the\\nprogramme of H.P. Grice do not prove remedial. Conceivable alternatives that\\nmight replace the familiar logics as descriptive instruments are briefly noted:\\n(i) substructural logics and (ii) meaning composition in linear algebras over\\nthe reals, occasionally constrained by norms of classical logic. Alternative\\n(ii) locates the problem in violations of one of the idempotent laws. Reasons\\nfor a lack of curiosity about elementary and easily testable implications of\\nthe received theory are considered. The concept of `reflective equilibrium' is\\ncritically examined for its role in reconciling normative desiderata and\\ndescriptive commitments.\\n\",\n",
              " '  We propose a new similarity measure between texts which, contrary to the\\ncurrent state-of-the-art approaches, takes a global view of the texts to be\\ncompared. We have implemented a tool to compute our textual distance and\\nconducted experiments on several corpuses of texts. The experiments show that\\nour methods can reliably identify different global types of texts.\\n',\n",
              " '  Sign Language (SL) linguistic is dependent on the expensive task of\\nannotating. Some automation is already available for low-level information (eg.\\nbody part tracking) and the lexical level has shown significant progresses. The\\nsyntactic level lacks annotated corpora as well as complete and consistent\\nmodels. This article presents a solution for the automatic annotation of SL\\nsyntactic elements. It exposes a formalism able to represent both\\nconstituency-based and dependency-based models. The first enable the\\nrepresentation the structures one may want to annotate, the second aims at\\nfulfilling the holes of the first. A parser is presented and used to conduct\\ntwo experiments on the solution. One experiment is on a real corpus, the other\\nis on a synthetic corpus.\\n',\n",
              " '  Sign Language (SL) automatic processing slowly progresses bottom-up. The\\nfield has seen proposition to handle the video signal, to recognize and\\nsynthesize sublexical and lexical units. It starts to see the development of\\nsupra-lexical processing. But the recognition, at this level, lacks data. The\\nsyntax of SL appears very specific as it uses massively the multiplicity of\\narticulators and its access to the spatial dimensions. Therefore new parsing\\ntechniques are developed. However these need to be evaluated. The shortage on\\nreal data restrains the corpus-based models to small sizes. We propose here a\\nsolution to produce data-sets for the evaluation of parsers on the specific\\nproperties of SL. The article first describes the general model used to\\ngenerates dependency grammars and the phrase generation from these lasts. It\\nthen discusses the limits of approach. The solution shows to be of particular\\ninterest to evaluate the scalability of the techniques on big models.\\n',\n",
              " \"  Statistical error Correction technique is the most accurate and widely used\\napproach today, but for a language like Sindhi which is a low resourced\\nlanguage the trained corpora's are not available, so the statistical techniques\\nare not possible at all. Instead a useful alternative would be to exploit\\nvarious spelling error trends in Sindhi by using a Rule based approach. For\\ndesigning such technique an essential prerequisite would be to study the\\nvarious error patterns in a language. This pa per presents various studies of\\nspelling error trends and their types in Sindhi Language. The research shows\\nthat the error trends common to all languages are also encountered in Sindhi\\nbut their do exist some error patters that are catered specifically to a Sindhi\\nlanguage.\\n\",\n",
              " '  Motivation: Entropy measurements on hierarchical structures have been used in\\nmethods for information retrieval and natural language modeling. Here we\\nexplore its application to semantic similarity. By finding shared ontology\\nterms, semantic similarity can be established between annotated genes. A common\\nprocedure for establishing semantic similarity is to calculate the\\ndescriptiveness (information content) of ontology terms and use these values to\\ndetermine the similarity of annotations. Most often information content is\\ncalculated for an ontology term by analyzing its frequency in an annotation\\ncorpus. The inherent problems in using these values to model functional\\nsimilarity motivates our work. Summary: We present a novel calculation for\\nestablishing the entropy of a DAG-based ontology, which can be used in an\\nalternative method for establishing the information content of its terms. We\\nalso compare our IC metric to two others using semantic and sequence\\nsimilarity.\\n',\n",
              " '  We describe the Clinical TempEval task which is currently in preparation for\\nthe SemEval-2015 evaluation exercise. This task involves identifying and\\ndescribing events, times and the relations between them in clinical text. Six\\ndiscrete subtasks are included, focusing on recognising mentions of times and\\nevents, describing those mentions for both entity types, identifying the\\nrelation between an event and the document creation time, and identifying\\nnarrative container relations.\\n',\n",
              " \"  Matching texts in highly inflected languages such as Arabic by simple\\nstemming strategy is unlikely to perform well. In this paper, we present a\\nstrategy for automatic text matching technique for for inflectional languages,\\nusing Arabic as the test case. The system is an extension of ROUGE test in\\nwhich texts are matched on token's lemma level. The experimental results show\\nan enhancement of detecting similarities between different sentences having\\nsame semantics but written in different lexical forms..\\n\",\n",
              " '  Event classification at sentence level is an important Information Extraction\\ntask with applications in several NLP, IR, and personalization systems.\\nMulti-label binary relevance (BR) are the state-of-art methods. In this work,\\nwe explored new multi-label methods known for capturing relations between event\\ntypes. These new methods, such as the ensemble Chain of Classifiers, improve\\nthe F1 on average across the 6 labels by 2.8% over the Binary Relevance. The\\nlow occurrence of multi-label sentences motivated the reduction of the hard\\nimbalanced multi-label classification problem with low number of occurrences of\\nmultiple labels per instance to an more tractable imbalanced multiclass problem\\nwith better results (+ 4.6%). We report the results of adding new features,\\nsuch as sentiment strength, rhetorical signals, domain-id (source-id and date),\\nand key-phrases in both single-label and multi-label event classification\\nscenarios.\\n',\n",
              " '  Natural language processing is a prompt research area across the country.\\nParsing is one of the very crucial tool in language analysis system which aims\\nto forecast the structural relationship among the words in a given sentence.\\nMany researchers have already developed so many language tools but the accuracy\\nis not meet out the human expectation level, thus the research is still exists.\\nMachine translation is one of the major application area under Natural Language\\nProcessing. While translation between one language to another language, the\\nstructure identification of a sentence play a key role. This paper introduces\\nthe hybrid way to solve the identification of relationship among the given\\nwords in a sentence. In existing system is implemented using rule based\\napproach, which is not suited in huge amount of data. The machine learning\\napproaches is suitable for handle larger amount of data and also to get better\\naccuracy via learning and training the system. The proposed approach takes a\\nTamil sentence as an input and produce the result of a dependency relation as a\\ntree like structure using hybrid approach. This proposed tool is very helpful\\nfor researchers and act as an odd-on improve the quality of existing\\napproaches.\\n',\n",
              " '  In this paper, we present the implementation of an automatic Sign Language\\n(SL) sign annotation framework based on a formal logic, the Propositional\\nDynamic Logic (PDL). Our system relies heavily on the use of a specific variant\\nof PDL, the Propositional Dynamic Logic for Sign Language (PDLSL), which lets\\nus describe SL signs as formulae and corpora videos as labeled transition\\nsystems (LTSs). Here, we intend to show how a generic annotation system can be\\nconstructed upon these underlying theoretical principles, regardless of the\\ntracking technologies available or the input format of corpora. With this in\\nmind, we generated a development framework that adapts the system to specific\\nuse cases. Furthermore, we present some results obtained by our application\\nwhen adapted to one distinct case, 2D corpora analysis with pre-processed\\ntracking information. We also present some insights on how such a technology\\ncan be used to analyze 3D real-time data, captured with a depth device.\\n',\n",
              " '  This paper explores the use of Propositional Dynamic Logic (PDL) as a\\nsuitable formal framework for describing Sign Language (SL), the language of\\ndeaf people, in the context of natural language processing. SLs are visual,\\ncomplete, standalone languages which are just as expressive as oral languages.\\nSigns in SL usually correspond to sequences of highly specific body postures\\ninterleaved with movements, which make reference to real world objects,\\ncharacters or situations. Here we propose a formal representation of SL signs,\\nthat will help us with the analysis of automatically-collected hand tracking\\ndata from French Sign Language (FSL) video corpora. We further show how such a\\nrepresentation could help us with the design of computer aided SL verification\\ntools, which in turn would bring us closer to the development of an automatic\\nrecognition system for these languages.\\n',\n",
              " '  Weibo, as the largest social media service in China, has billions of messages\\ngenerated every day. The huge number of messages contain rich sentimental\\ninformation. In order to analyze the emotional changes in accordance with time\\nand space, this paper presents an Emotion Analysis Platform (EAP), which\\nexplores the emotional distribution of each province, so that can monitor the\\nglobal pulse of each province in China. The massive data of Weibo and the\\nreal-time requirements make the building of EAP challenging. In order to solve\\nthe above problems, emoticons, emotion lexicon and emotion-shifting rules are\\nadopted in EAP to analyze the emotion of each tweet. In order to verify the\\neffectiveness of the platform, case study on the Sichuan earthquake is done,\\nand the analysis result of the platform accords with the fact. In order to\\nanalyze from quantity, we manually annotate a test set and conduct experiment\\non it. The experimental results show that the macro-Precision of EAP reaches\\n80% and the EAP works effectively.\\n',\n",
              " '  Machine translation (MT) research in Indian languages is still in its\\ninfancy. Not much work has been done in proper transliteration of name entities\\nin this domain. In this paper we address this issue. We have used English-Hindi\\nlanguage pair for our experiments and have used a hybrid approach. At first we\\nhave processed English words using a rule based approach which extracts\\nindividual phonemes from the words and then we have applied statistical\\napproach which converts the English into its equivalent Hindi phoneme and in\\nturn the corresponding Hindi word. Through this approach we have attained\\n83.40% accuracy.\\n',\n",
              " '  Evaluation plays a crucial role in development of Machine translation\\nsystems. In order to judge the quality of an existing MT system i.e. if the\\ntranslated output is of human translation quality or not, various automatic\\nmetrics exist. We here present the implementation results of different metrics\\nwhen used on Hindi language along with their comparisons, illustrating how\\neffective are these metrics on languages like Hindi (free word order language).\\n',\n",
              " '  This article reports the evaluation of the integration of data from a\\nsyntactic-semantic lexicon, the Lexicon-Grammar of French, into a syntactic\\nparser. We show that by changing the set of labels for verbs and predicational\\nnouns, we can improve the performance on French of a non-lexicalized\\nprobabilistic parser.\\n',\n",
              " '  Wordnets are semantic networks containing nouns, verbs, adjectives, and\\nadverbs organized according to linguistic principles, by means of semantic\\nrelations. In this work, we adopt a complex network perspective to perform a\\ncomparative analysis of the English and Polish wordnets. We determine their\\nsimilarities and show that the networks exhibit some of the typical\\ncharacteristics observed in other real-world networks. We analyse interlingual\\nrelations between both wordnets and deliberate over the problem of mapping the\\nPolish lexicon onto the English one.\\n',\n",
              " '  Text is the main method of communicating information in the digital age.\\nMessages, blogs, news articles, reviews, and opinionated information abound on\\nthe Internet. People commonly purchase products online and post their opinions\\nabout purchased items. This feedback is displayed publicly to assist others\\nwith their purchasing decisions, creating the need for a mechanism with which\\nto extract and summarize useful information for enhancing the decision-making\\nprocess. Our contribution is to improve the accuracy of extraction by combining\\ndifferent techniques from three major areas, named Data Mining, Natural\\nLanguage Processing techniques and Ontologies. The proposed framework\\nsequentially mines products aspects and users opinions, groups representative\\naspects by similarity, and generates an output summary. This paper focuses on\\nthe task of extracting product aspects and users opinions by extracting all\\npossible aspects and opinions from reviews using natural language, ontology,\\nand frequent (tag) sets. The proposed framework, when compared with an existing\\nbaseline model, yielded promising results.\\n',\n",
              " '  We present the creation of an English-Swedish FrameNet-based grammar in\\nGrammatical Framework. The aim of this research is to make existing framenets\\ncomputationally accessible for multilingual natural language applications via a\\ncommon semantic grammar API, and to facilitate the porting of such grammar to\\nother languages. In this paper, we describe the abstract syntax of the semantic\\ngrammar while focusing on its automatic extraction possibilities. We have\\nextracted a shared abstract syntax from ~58,500 annotated sentences in Berkeley\\nFrameNet (BFN) and ~3,500 annotated sentences in Swedish FrameNet (SweFN). The\\nabstract syntax defines 769 frame-specific valence patterns that cover 77.8%\\nexamples in BFN and 74.9% in SweFN belonging to the shared set of 471 frames.\\nAs a side result, we provide a unified method for comparing semantic and\\nsyntactic valence patterns across framenets.\\n',\n",
              " '  The ability to accurately represent sentences is central to language\\nunderstanding. We describe a convolutional architecture dubbed the Dynamic\\nConvolutional Neural Network (DCNN) that we adopt for the semantic modelling of\\nsentences. The network uses Dynamic k-Max Pooling, a global pooling operation\\nover linear sequences. The network handles input sentences of varying length\\nand induces a feature graph over the sentence that is capable of explicitly\\ncapturing short and long-range relations. The network does not rely on a parse\\ntree and is easily applicable to any language. We test the DCNN in four\\nexperiments: small scale binary and multi-class sentiment prediction, six-way\\nquestion classification and Twitter sentiment prediction by distant\\nsupervision. The network achieves excellent performance in the first three\\ntasks and a greater than 25% error reduction in the last task with respect to\\nthe strongest baseline.\\n',\n",
              " '  Stemming is a pre-processing step in Text Mining applications as well as a\\nvery common requirement of Natural Language processing functions. Stemming is\\nthe process for reducing inflected words to their stem. The main purpose of\\nstemming is to reduce different grammatical forms / word forms of a word like\\nits noun, adjective, verb, adverb etc. to its root form. Stemming is widely\\nuses in Information Retrieval system and reduces the size of index files. We\\ncan say that the goal of stemming is to reduce inflectional forms and sometimes\\nderivationally related forms of a word to a common base form. In this paper we\\nhave discussed different stemming algorithm for non-Indian and Indian language,\\nmethods of stemming, accuracy and errors.\\n',\n",
              " '  For more than forty years now, modern theories of literature (Compagnon,\\n1979) insist on the role of paraphrases, rewritings, citations, reciprocal\\nborrowings and mutual contributions of any kinds. The notions of\\nintertextuality, transtextuality, hypertextuality/hypotextuality, were\\nintroduced in the seventies and eighties to approach these phenomena. The\\ncareful analysis of these references is of particular interest in evaluating\\nthe distance that the creator voluntarily introduces with his/her masters.\\nPhoebus is collaborative project that makes computer scientists from the\\nUniversity Pierre and Marie Curie (LIP6-UPMC) collaborate with the literary\\nteams of Paris-Sorbonne University with the aim to develop efficient tools for\\nliterary studies that take advantage of modern computer science techniques. In\\nthis context, we have developed a piece of software that automatically detects\\nand explores networks of textual reuses in classical literature. This paper\\ndescribes the principles on which is based this program, the significant\\nresults that have already been obtained and the perspectives for the near\\nfuture.\\n',\n",
              " '  Pagination - the process of determining where to break an article across\\npages in a multi-article layout is a common layout challenge for most\\ncommercially printed newspapers and magazines. To date, no one has created an\\nalgorithm that determines a minimal pagination break point based on the content\\nof the article. Existing approaches for automatic multi-article layout focus\\nexclusively on maximizing content (number of articles) and optimizing aesthetic\\npresentation (e.g., spacing between articles). However, disregarding the\\nsemantic information within the article can lead to overly aggressive cutting,\\nthereby eliminating key content and potentially confusing the reader, or\\nsetting too generous of a break point, thereby leaving in superfluous content\\nand making automatic layout more difficult. This is one of the remaining\\nchallenges on the path from manual layouts to fully automated processes that\\nstill ensure article content quality. In this work, we present a new approach\\nto calculating a document minimal break point for the task of pagination. Our\\napproach uses a statistical language model to predict minimal break points\\nbased on the semantic content of an article. We then compare 4 novel candidate\\napproaches, and 4 baselines (currently in use by layout algorithms). Results\\nfrom this experiment show that one of our approaches strongly outperforms the\\nbaselines and alternatives. Results from a second study suggest that humans are\\nnot able to agree on a single \"best\" break point. Therefore, this work shows\\nthat a semantic-based lower bound break point prediction is necessary for ideal\\nautomated document synthesis within a real-world context.\\n',\n",
              " '  We introduce a novel approach for building language models based on a\\nsystematic, recursive exploration of skip n-gram models which are interpolated\\nusing modified Kneser-Ney smoothing. Our approach generalizes language models\\nas it contains the classical interpolation with lower order models as a special\\ncase. In this paper we motivate, formalize and present our approach. In an\\nextensive empirical experiment over English text corpora we demonstrate that\\nour generalized language models lead to a substantial reduction of perplexity\\nbetween 3.1% and 12.7% in comparison to traditional language models using\\nmodified Kneser-Ney smoothing. Furthermore, we investigate the behaviour over\\nthree other languages and a domain specific corpus where we observed consistent\\nimprovements. Finally, we also show that the strength of our approach lies in\\nits ability to cope in particular with sparse training data. Using a very small\\ntraining data set of only 736 KB text we yield improvements of even 25.7%\\nreduction of perplexity.\\n',\n",
              " \"  Metrics for measuring the comparability of corpora or texts need to be\\ndeveloped and evaluated systematically. Applications based on a corpus, such as\\ntraining Statistical MT systems in specialised narrow domains, require finding\\na reasonable balance between the size of the corpus and its consistency, with\\ncontrolled and benchmarked levels of comparability for any newly added\\nsections. In this article we propose a method that can meta-evaluate\\ncomparability metrics by calculating monolingual comparability scores\\nseparately on the 'source' and 'target' sides of parallel corpora. The range of\\nscores on the source side is then correlated (using Pearson's r coefficient)\\nwith the range of 'target' scores; the higher the correlation - the more\\nreliable is the metric. The intuition is that a good metric should yield the\\nsame distance between different domains in different languages. Our method\\ngives consistent results for the same metrics on different data sets, which\\nindicates that it is reliable and can be used for metric comparison or for\\noptimising settings of parametrised metrics.\\n\",\n",
              " '  Most categorical models of meaning use a functor from the syntactic category\\nto the semantic category. When semantic information is available, the problem\\nof grammar induction can therefore be defined as finding preimages of the\\nsemantic types under this forgetful functor, lifting the information flow from\\nthe semantic level to a valid reduction at the syntactic level. We study the\\ncomplexity of grammar induction, and show that for a variety of type systems,\\nincluding pivotal and compact closed categories, the grammar induction problem\\nis NP-complete. Our approach could be extended to linguistic type systems such\\nas autonomous or bi-closed categories.\\n',\n",
              " '  Evaluation plays a vital role in checking the quality of MT output. It is\\ndone either manually or automatically. Manual evaluation is very time consuming\\nand subjective, hence use of automatic metrics is done most of the times. This\\npaper evaluates the translation quality of different MT Engines for\\nHindi-English (Hindi data is provided as input and English is obtained as\\noutput) using various automatic metrics like BLEU, METEOR etc. Further the\\ncomparison automatic evaluation results with Human ranking have also been\\ngiven.\\n',\n",
              " '  Stanford typed dependencies are a widely desired representation of natural\\nlanguage sentences, but parsing is one of the major computational bottlenecks\\nin text analysis systems. In light of the evolving definition of the Stanford\\ndependencies and developments in statistical dependency parsing algorithms,\\nthis paper revisits the question of Cer et al. (2010): what is the tradeoff\\nbetween accuracy and speed in obtaining Stanford dependencies in particular? We\\nalso explore the effects of input representations on this tradeoff:\\npart-of-speech tags, the novel use of an alternative dependency representation\\nas input, and distributional representaions of words. We find that direct\\ndependency parsing is a more viable solution than it was found to be in the\\npast. An accompanying software release can be found at:\\nhttp://www.ark.cs.cmu.edu/TBSD\\n',\n",
              " '  Building computers able to answer questions on any subject is a long standing\\ngoal of artificial intelligence. Promising progress has recently been achieved\\nby methods that learn to map questions to logical forms or database queries.\\nSuch approaches can be effective but at the cost of either large amounts of\\nhuman-labeled data or by defining lexicons and grammars tailored by\\npractitioners. In this paper, we instead take the radical approach of learning\\nto map questions to vectorial feature representations. By mapping answers into\\nthe same space one can query any knowledge base independent of its schema,\\nwithout requiring any grammar or lexicon. Our method is trained with a new\\noptimization procedure combining stochastic gradient descent followed by a\\nfine-tuning step using the weak supervision provided by blending automatically\\nand collaboratively generated resources. We empirically demonstrate that our\\nmodel can capture meaningful signals from its noisy supervision leading to\\nmajor improvements over paralex, the only existing method able to be trained on\\nsimilar weakly labeled data.\\n',\n",
              " '  In this article, we have introduced the first parallel corpus of Persian with\\nmore than 10 other European languages. This article describes primary steps\\ntoward preparing a Basic Language Resources Kit (BLARK) for Persian. Up to now,\\nwe have proposed morphosyntactic specification of Persian based on\\nEAGLE/MULTEXT guidelines and specific resources of MULTEXT-East. The article\\nintroduces Persian Language, with emphasis on its orthography and\\nmorphosyntactic features, then a new Part-of-Speech categorization and\\northography for Persian in digital environments is proposed. Finally, the\\ncorpus and related statistic will be analyzed.\\n',\n",
              " '  We present a novel technique for learning semantic representations, which\\nextends the distributional hypothesis to multilingual data and joint-space\\nembeddings. Our models leverage parallel data and learn to strongly align the\\nembeddings of semantically equivalent sentences, while maintaining sufficient\\ndistance between those of dissimilar sentences. The models do not rely on word\\nalignments or any syntactic information and are successfully applied to a\\nnumber of diverse languages. We extend our approach to learn semantic\\nrepresentations at the document level, too. We evaluate these models on two\\ncross-lingual document classification tasks, outperforming the prior state of\\nthe art. Through qualitative analysis and the study of pivoting effects we\\ndemonstrate that our representations are semantically plausible and can capture\\nsemantic relationships across languages without parallel data.\\n',\n",
              " '  We present a method to leverage radical for learning Chinese character\\nembedding. Radical is a semantic and phonetic component of Chinese character.\\nIt plays an important role as characters with the same radical usually have\\nsimilar semantic meaning and grammatical usage. However, existing Chinese\\nprocessing algorithms typically regard word or character as the basic unit but\\nignore the crucial radical information. In this paper, we fill this gap by\\nleveraging radical for learning continuous representation of Chinese character.\\nWe develop a dedicated neural architecture to effectively learn character\\nembedding and apply it on Chinese character similarity judgement and Chinese\\nword segmentation. Experiment results show that our radical-enhanced method\\noutperforms existing embedding learning algorithms on both tasks.\\n',\n",
              " '  Farsi, also known as Persian, is the official language of Iran and Tajikistan\\nand one of the two main languages spoken in Afghanistan. Farsi enjoys a unified\\nArabic script as its writing system. In this paper we briefly introduce the\\nwriting standards of Farsi and highlight problems one would face when analyzing\\nFarsi electronic texts, especially during development of Farsi corpora\\nregarding to transcription and encoding of Farsi e-texts. The pointes mentioned\\nmay sounds easy but they are crucial when developing and processing written\\ncorpora of Farsi.\\n',\n",
              " '  This paper develops a compositional vector-based semantics of subject and\\nobject relative pronouns within a categorical framework. Frobenius algebras are\\nused to formalise the operations required to model the semantics of relative\\npronouns, including passing information between the relative clause and the\\nmodified noun phrase, as well as copying, combining, and discarding parts of\\nthe relative clause. We develop two instantiations of the abstract semantics,\\none based on a truth-theoretic approach and one based on corpus statistics.\\n',\n",
              " '  In this work we present a morphological analysis of Bishnupriya Manipuri\\nlanguage, an Indo-Aryan language spoken in the north eastern India. As of now,\\nthere is no computational work available for the language. Finite state\\nmorphology is one of the successful approaches applied in a wide variety of\\nlanguages over the year. Therefore we adapted the finite state approach to\\nanalyse morphology of the Bishnupriya Manipuri language.\\n',\n",
              " '  Most state-of-the-art approaches for named-entity recognition (NER) use semi\\nsupervised information in the form of word clusters and lexicons. Recently\\nneural network-based language models have been explored, as they as a byproduct\\ngenerate highly informative vector representations for words, known as word\\nembeddings. In this paper we present two contributions: a new form of learning\\nword embeddings that can leverage information from relevant lexicons to improve\\nthe representations, and the first system to use neural word embeddings to\\nachieve state-of-the-art results on named-entity recognition in both CoNLL and\\nOntonotes NER. Our system achieves an F1 score of 90.90 on the test set for\\nCoNLL 2003---significantly better than any previous system trained on public\\ndata, and matching a system employing massive private industrial query-log\\ndata.\\n',\n",
              " '  The IDSgrep structural query system for Han character dictionaries is\\npresented. This system includes a data model and syntax for describing the\\nspatial structure of Han characters using Extended Ideographic Description\\nSequences (EIDSes) based on the Unicode IDS syntax; a language for querying\\nEIDS databases, designed to suit the needs of font developers and foreign\\nlanguage learners; a bit vector index inspired by Bloom filters for faster\\nquery operations; a freely available implementation; and format translation\\nfrom popular third-party IDS and XML character databases. Experimental results\\nare included, with a comparison to other software used for similar\\napplications.\\n',\n",
              " '  Linguists and psychologists have long been studying cross-linguistic\\ntransfer, the influence of native language properties on linguistic performance\\nin a foreign language. In this work we provide empirical evidence for this\\nprocess in the form of a strong correlation between language similarities\\nderived from structural features in English as Second Language (ESL) texts and\\nequivalent similarities obtained from the typological features of the native\\nlanguages. We leverage this finding to recover native language typological\\nsimilarity structure directly from ESL text, and perform prediction of\\ntypological features in an unsupervised fashion with respect to the target\\nlanguages. Our method achieves 72.2% accuracy on the typology prediction task,\\na result that is highly competitive with equivalent methods that rely on\\ntypological resources.\\n',\n",
              " '  While previous sentiment analysis research has concentrated on the\\ninterpretation of explicitly stated opinions and attitudes, this work initiates\\nthe computational study of a type of opinion implicature (i.e.,\\nopinion-oriented inference) in text. This paper described a rule-based\\nframework for representing and analyzing opinion implicatures which we hope\\nwill contribute to deeper automatic interpretation of subjective language. In\\nthe course of understanding implicatures, the system recognizes implicit\\nsentiments (and beliefs) toward various events and entities in the sentence,\\noften attributed to different sources (holders) and of mixed polarities; thus,\\nit produces a richer interpretation than is typical in opinion analysis.\\n',\n",
              " '  Many successful approaches to semantic parsing build on top of the syntactic\\nanalysis of text, and make use of distributional representations or statistical\\nmodels to match parses to ontology-specific queries. This paper presents a\\nnovel deep learning architecture which provides a semantic parsing system\\nthrough the union of two neural models of language semantics. It allows for the\\ngeneration of ontology-specific queries from natural language statements and\\nquestions without the need for parsing, which makes it especially suitable to\\ngrammatically malformed or syntactically atypical text, such as tweets, as well\\nas permitting the development of semantic parsers for resource-poor languages.\\n',\n",
              " '  In this paper we propose a general framework for topic-specific summarization\\nof large text corpora and illustrate how it can be used for the analysis of\\nnews databases. Our framework, concise comparative summarization (CCS), is\\nbuilt on sparse classification methods. CCS is a lightweight and flexible tool\\nthat offers a compromise between simple word frequency based methods currently\\nin wide use and more heavyweight, model-intensive methods such as latent\\nDirichlet allocation (LDA). We argue that sparse methods have much to offer for\\ntext analysis and hope CCS opens the door for a new branch of research in this\\nimportant field. For a particular topic of interest (e.g., China or energy),\\nCSS automatically labels documents as being either on- or off-topic (usually\\nvia keyword search), and then uses sparse classification methods to predict\\nthese labels with the high-dimensional counts of all the other words and\\nphrases in the documents. The resulting small set of phrases found as\\npredictive are then harvested as the summary. To validate our tool, we, using\\nnews articles from the New York Times international section, designed and\\nconducted a human survey to compare the different summarizers with human\\nunderstanding. We demonstrate our approach with two case studies, a media\\nanalysis of the framing of \"Egypt\" in the New York Times throughout the Arab\\nSpring and an informal comparison of the New York Times\\' and Wall Street\\nJournal\\'s coverage of \"energy.\" Overall, we find that the Lasso with $L^2$\\nnormalization can be effectively and usefully used to summarize large corpora,\\nregardless of document size.\\n',\n",
              " '  We develop a model for the stability and maintenance of phonological\\ncategories. Examples of phonological categories are vowel sounds such as \"i\"\\nand \"e\". We model such categories as consisting of collections of labeled\\nexemplars that language users store in their memory. Each exemplar is a\\ndetailed memory of an instance of the linguistic entity in question. Starting\\nfrom an exemplar-level model we derive integro-differential equations for the\\nlong-term evolution of the density of exemplars in different portions of\\nphonetic space. Using these latter equations we investigate under what\\nconditions two phonological categories merge or not. Our main conclusion is\\nthat for the preservation of distinct phonological categories, it is necessary\\nthat anomalous speech tokens of a given category are discarded, and not merely\\nstored in memory as an exemplar of another category.\\n',\n",
              " '  We describe a contextual parser for the Robot Commands Treebank, a new\\ncrowdsourced resource. In contrast to previous semantic parsers that select the\\nmost-probable parse, we consider the different problem of parsing using\\nadditional situational context to disambiguate between different readings of a\\nsentence. We show that multiple semantic analyses can be searched using dynamic\\nprogramming via interaction with a spatial planner, to guide the parsing\\nprocess. We are able to parse sentences in near linear-time by ruling out\\nanalyses early on that are incompatible with spatial context. We report a 34%\\nupper bound on accuracy, as our planner correctly processes spatial context for\\n3,394 out of 10,000 sentences. However, our parser achieves a 96.53%\\nexact-match score for parsing within the subset of sentences recognized by the\\nplanner, compared to 82.14% for a non-contextual parser.\\n',\n",
              " \"  We present an approach to the extraction of family relations from literary\\nnarrative, which incorporates a technique for utterance attribution proposed\\nrecently by Elson and McKeown (2010). In our work this technique is used in\\ncombination with the detection of vocatives - the explicit forms of address\\nused by the characters in a novel. We take advantage of the fact that certain\\nvocatives indicate family relations between speakers. The extracted relations\\nare then propagated using a set of rules. We report the results of the\\napplication of our method to Jane Austen's Pride and Prejudice.\\n\",\n",
              " '  In this work, we employ quantitative methods from the realm of statistics and\\nmachine learning to develop novel methodologies for author attribution and\\ntextual analysis. In particular, we develop techniques and software suitable\\nfor applications to Classical study, and we illustrate the efficacy of our\\napproach in several interesting open questions in the field. We apply our\\nnumerical analysis techniques to questions of authorship attribution in the\\ncase of the Greek tragedian Euripides, to instances of intertextuality and\\ninfluence in the poetry of the Roman statesman Seneca the Younger, and to cases\\nof \"interpolated\" text with respect to the histories of Livy.\\n',\n",
              " '  Named Entities (NEs) are often written with no orthographic changes across\\ndifferent languages that share a common alphabet. We show that this can be\\nleveraged so as to improve named entity recognition (NER) by using unsupervised\\nword clusters from secondary languages as features in state-of-the-art\\ndiscriminative NER systems. We observe significant increases in performance,\\nfinding that person and location identification is particularly improved, and\\nthat phylogenetically close languages provide more valuable features than more\\ndistant languages.\\n',\n",
              " '  We present a probabilistic model that simultaneously learns alignments and\\ndistributed representations for bilingual data. By marginalizing over word\\nalignments the model captures a larger semantic context than prior work relying\\non hard alignments. The advantage of this approach is demonstrated in a\\ncross-lingual classification task, where we outperform the prior published\\nstate of the art.\\n',\n",
              " '  It is now widely recognized that ontologies, are one of the fundamental\\ncornerstones of knowledge-based systems. What is lacking, however, is a\\ncurrently accepted strategy of how to build ontology; what kinds of the\\nresources and techniques are indispensables to optimize the expenses and the\\ntime on the one hand and the amplitude, the completeness, the robustness of en\\nontology on the other hand. The paper offers a semi-automatic ontology\\nconstruction method from text corpora in the domain of radiological protection.\\nThis method is composed from next steps: 1) text annotation with part-of-speech\\ntags; 2) revelation of the significant linguistic structures and forming the\\ntemplates; 3) search of text fragments corresponding to these templates; 4)\\nbasic ontology instantiation process\\n',\n",
              " '  Conjuring up our thoughts, language reflects statistical patterns of word\\nco-occurrences which in turn come to describe how we perceive the world.\\nWhether counting how frequently nouns and verbs combine in Google search\\nqueries, or extracting eigenvectors from term document matrices made up of\\nWikipedia lines and Shakespeare plots, the resulting latent semantics capture\\nnot only the associative links which form concepts, but also spatial dimensions\\nembedded within the surface structure of language. As both the shape and\\nmovements of objects have been found to be associated with phonetic contrasts\\nalready in toddlers, this study explores whether articulatory and acoustic\\nparameters may likewise differentiate the latent semantics of action verbs.\\nSelecting 3 x 20 emotion, face, and hand related verbs known to activate\\npremotor areas in the brain, their mutual cosine similarities were computed\\nusing latent semantic analysis LSA, and the resulting adjacency matrices were\\ncompared based on two different large scale text corpora; HAWIK and TASA.\\nApplying hierarchical clustering to identify common structures across the two\\ntext corpora, the verbs largely divide into combined mouth and hand movements\\nversus emotional expressions. Transforming the verbs into their constituent\\nphonemes, the clustered small and large size movements appear differentiated by\\nfront versus back vowels corresponding to increasing levels of arousal. Whereas\\nthe clustered emotional verbs seem characterized by sequences of close versus\\nopen jaw produced phonemes, generating up- or downwards shifts in formant\\nfrequencies that may influence their perceived valence. Suggesting, that the\\nlatent semantics of action verbs reflect parameters of intensity and emotional\\npolarity that appear correlated with the articulatory contrasts and acoustic\\ncharacteristics of phonemes\\n',\n",
              " '  Word sense disambiguation (WSD) is a problem in the field of computational\\nlinguistics given as finding the intended sense of a word (or a set of words)\\nwhen it is activated within a certain context. WSD was recently addressed as a\\ncombinatorial optimization problem in which the goal is to find a sequence of\\nsenses that maximize the semantic relatedness among the target words. In this\\narticle, a novel algorithm for solving the WSD problem called D-Bees is\\nproposed which is inspired by bee colony optimization (BCO)where artificial bee\\nagents collaborate to solve the problem. The D-Bees algorithm is evaluated on a\\nstandard dataset (SemEval 2007 coarse-grained English all-words task corpus)and\\nis compared to simulated annealing, genetic algorithms, and two ant colony\\noptimization techniques (ACO). It will be observed that the BCO and ACO\\napproaches are on par.\\n',\n",
              " '  The strength with which a statement is made can have a significant impact on\\nthe audience. For example, international relations can be strained by how the\\nmedia in one country describes an event in another; and papers can be rejected\\nbecause they overstate or understate their findings. It is thus important to\\nunderstand the effects of statement strength. A first step is to be able to\\ndistinguish between strong and weak statements. However, even this problem is\\nunderstudied, partly due to a lack of data. Since strength is inherently\\nrelative, revisions of texts that make claims are a natural source of data on\\nstrength differences. In this paper, we introduce a corpus of sentence-level\\nrevisions from academic writing. We also describe insights gained from our\\nannotation efforts for this task.\\n',\n",
              " '  While many lexica annotated with words polarity are available for sentiment\\nanalysis, very few tackle the harder task of emotion analysis and are usually\\nquite limited in coverage. In this paper, we present a novel approach for\\nextracting - in a totally automated way - a high-coverage and high-precision\\nlexicon of roughly 37 thousand terms annotated with emotion scores, called\\nDepecheMood. Our approach exploits in an original way \\'crowd-sourced\\' affective\\nannotation implicitly provided by readers of news articles from rappler.com. By\\nproviding new state-of-the-art performances in unsupervised settings for\\nregression and classification tasks, even using a na\\\\\"{\\\\i}ve approach, our\\nexperiments show the beneficial impact of harvesting social media data for\\naffective lexicon building.\\n',\n",
              " '  This paper presents preliminary results of Croatian syllable networks\\nanalysis. Syllable network is a network in which nodes are syllables and links\\nbetween them are constructed according to their connections within words. In\\nthis paper we analyze networks of syllables generated from texts collected from\\nthe Croatian Wikipedia and Blogs. As a main tool we use complex network\\nanalysis methods which provide mechanisms that can reveal new patterns in a\\nlanguage structure. We aim to show that syllable networks have much higher\\nclustering coefficient in comparison to Erd\\\\\"os-Renyi random networks. The\\nresults indicate that Croatian syllable networks exhibit certain properties of\\na small world networks. Furthermore, we compared Croatian syllable networks\\nwith Portuguese and Chinese syllable networks and we showed that they have\\nsimilar properties.\\n',\n",
              " '  In this work we present our expert system of Automatic reading or speech\\nsynthesis based on a text written in Standard Arabic, our work is carried out\\nin two great stages: the creation of the sound data base, and the\\ntransformation of the written text into speech (Text To Speech TTS). This\\ntransformation is done firstly by a Phonetic Orthographical Transcription (POT)\\nof any written Standard Arabic text with the aim of transforming it into his\\ncorresponding phonetics sequence, and secondly by the generation of the voice\\nsignal which corresponds to the chain transcribed. We spread out the different\\nof conception of the system, as well as the results obtained compared to others\\nworks studied to realize TTS based on Standard Arabic.\\n',\n",
              " '  Minimum error rate training (MERT) is a widely used training procedure for\\nstatistical machine translation. A general problem of this approach is that the\\nsearch space is easy to converge to a local optimum and the acquired weight set\\nis not in accord with the real distribution of feature functions. This paper\\nintroduces coordinate system selection (RSS) into the search algorithm for\\nMERT. Contrary to previous approaches in which every dimension only corresponds\\nto one independent feature function, we create several coordinate systems by\\nmoving one of the dimensions to a new direction. The basic idea is quite simple\\nbut critical that the training procedure of MERT should be based on a\\ncoordinate system formed by search directions but not directly on feature\\nfunctions. Experiments show that by selecting coordinate systems with tuning\\nset results, better results can be obtained without any other language\\nknowledge.\\n',\n",
              " '  In this paper we present the comparison of the linguistic networks from\\nliterature and blog texts. The linguistic networks are constructed from texts\\nas directed and weighted co-occurrence networks of words. Words are nodes and\\nlinks are established between two nodes if they are directly co-occurring\\nwithin the sentence. The comparison of the networks structure is performed at\\nglobal level (network) in terms of: average node degree, average shortest path\\nlength, diameter, clustering coefficient, density and number of components.\\nFurthermore, we perform analysis on the local level (node) by comparing the\\nrank plots of in and out degree, strength and selectivity. The\\nselectivity-based results point out that there are differences between the\\nstructure of the networks constructed from literature and blogs.\\n',\n",
              " '  In both quantum mechanics and corpus linguistics based on vector spaces, the\\nnotion of entanglement provides a means for the various subsystems to\\ncommunicate with each other. In this paper we examine a number of\\nimplementations of the categorical framework of Coecke, Sadrzadeh and Clark\\n(2010) for natural language, from an entanglement perspective. Specifically,\\nour goal is to better understand in what way the level of entanglement of the\\nrelational tensors (or the lack of it) affects the compositional structures in\\npractical situations. Our findings reveal that a number of proposals for verb\\nconstruction lead to almost separable tensors, a fact that considerably\\nsimplifies the interactions between the words. We examine the ramifications of\\nthis fact, and we show that the use of Frobenius algebras mitigates the\\npotential problems to a great extent. Finally, we briefly examine a machine\\nlearning method that creates verb tensors exhibiting a sufficient level of\\nentanglement.\\n',\n",
              " '  This paper presents a novel combinational phonetic algorithm for Sindhi\\nLanguage, to be used in developing Sindhi Spell Checker which has yet not been\\ndeveloped prior to this work. The compound textual forms and glyphs of Sindhi\\nlanguage presents a substantial challenge for developing Sindhi spell checker\\nsystem and generating similar suggestion list for misspelled words. In order to\\nimplement such a system, phonetic based Sindhi language rules and patterns must\\nbe considered into account for increasing the accuracy and efficiency. The\\nproposed system is developed with a blend between Phonetic based SoundEx\\nalgorithm and ShapeEx algorithm for pattern or glyph matching, generating\\naccurate and efficient suggestion list for incorrect or misspelled Sindhi\\nwords. A table of phonetically similar sounding Sindhi characters for SoundEx\\nalgorithm is also generated along with another table containing similar glyph\\nor shape based character groups for ShapeEx algorithm. Both these are first\\never attempt of any such type of categorization and representation for Sindhi\\nLanguage.\\n',\n",
              " '  Requests are at the core of many social media systems such as question &\\nanswer sites and online philanthropy communities. While the success of such\\nrequests is critical to the success of the community, the factors that lead\\ncommunity members to satisfy a request are largely unknown. Success of a\\nrequest depends on factors like who is asking, how they are asking, when are\\nthey asking, and most critically what is being requested, ranging from small\\nfavors to substantial monetary donations. We present a case study of altruistic\\nrequests in an online community where all requests ask for the very same\\ncontribution and do not offer anything tangible in return, allowing us to\\ndisentangle what is requested from textual and social factors. Drawing from\\nsocial psychology literature, we extract high-level social features from text\\nthat operationalize social relations between recipient and donor and\\ndemonstrate that these extracted relations are predictive of success. More\\nspecifically, we find that clearly communicating need through the narrative is\\nessential and that that linguistic indications of gratitude, evidentiality, and\\ngeneralized reciprocity, as well as high status of the asker further increase\\nthe likelihood of success. Building on this understanding, we develop a model\\nthat can predict the success of unseen requests, significantly improving over\\nseveral baselines. We link these findings to research in psychology on helping\\nbehavior, providing a basis for further analysis of success in social media\\nsystems.\\n',\n",
              " '  We provide a method for automatically detecting change in language across\\ntime through a chronologically trained neural language model. We train the\\nmodel on the Google Books Ngram corpus to obtain word vector representations\\nspecific to each year, and identify words that have changed significantly from\\n1900 to 2009. The model identifies words such as \"cell\" and \"gay\" as having\\nchanged during that time period. The model simultaneously identifies the\\nspecific years during which such words underwent change.\\n',\n",
              " '  We provide a simple but novel supervised weighting scheme for adjusting term\\nfrequency in tf-idf for sentiment analysis and text classification. We compare\\nour method to baseline weighting schemes and find that it outperforms them on\\nmultiple benchmarks. The method is robust and works well on both snippets and\\nlonger documents.\\n',\n",
              " '  We describe INAUT, a controlled natural language dedicated to collaborative\\nupdate of a knowledge base on maritime navigation and to automatic generation\\nof coast pilot books (Instructions nautiques) of the French National\\nHydrographic and Oceanographic Service SHOM. INAUT is based on French language\\nand abundantly uses georeferenced entities. After describing the structure of\\nthe overall system, giving details on the language and on its generation, and\\ndiscussing the three major applications of INAUT (document production,\\ninteraction with ENCs and collaborative updates of the knowledge base), we\\nconclude with future extensions and open problems.\\n',\n",
              " '  This paper studies the properties of the Croatian texts via complex networks.\\nWe present network properties of normal and shuffled Croatian texts for\\ndifferent shuffling principles: on the sentence level and on the text level. In\\nboth experiments we preserved the vocabulary size, word and sentence frequency\\ndistributions. Additionally, in the first shuffling approach we preserved the\\nsentence structure of the text and the number of words per sentence. Obtained\\nresults showed that degree rank distributions exhibit no substantial deviation\\nin shuffled networks, and strength rank distributions are preserved due to the\\nsame word frequencies. Therefore, standard approach to study the structure of\\nlinguistic co-occurrence networks showed no clear difference among the\\ntopologies of normal and shuffled texts. Finally, we showed that the in- and\\nout- selectivity values from shuffled texts are constantly below selectivity\\nvalues calculated from normal texts. Our results corroborate that the node\\nselectivity measure can capture structural differences between original and\\nshuffled Croatian texts.\\n',\n",
              " '  In recent years, new developments in the area of lexicography have altered\\nnot only the management, processing and publishing of lexicographical data, but\\nalso created new types of products such as electronic dictionaries and\\nthesauri. These expand the range of possible uses of lexical data and support\\nusers with more flexibility, for instance in assisting human translation. In\\nthis article, we give a short and easy-to-understand introduction to the\\nproblematic nature of the storage, display and interpretation of lexical data.\\nWe then describe the main methods and specifications used to build and\\nrepresent lexical data. This paper is targeted for the following groups of\\npeople: linguists, lexicographers, IT specialists, computer linguists and all\\nothers who wish to learn more about the modelling, representation and\\nvisualization of lexical knowledge. This paper is written in two languages:\\nFrench and German.\\n',\n",
              " '  Many machine learning algorithms require the input to be represented as a\\nfixed-length feature vector. When it comes to texts, one of the most common\\nfixed-length features is bag-of-words. Despite their popularity, bag-of-words\\nfeatures have two major weaknesses: they lose the ordering of the words and\\nthey also ignore semantics of the words. For example, \"powerful,\" \"strong\" and\\n\"Paris\" are equally distant. In this paper, we propose Paragraph Vector, an\\nunsupervised algorithm that learns fixed-length feature representations from\\nvariable-length pieces of texts, such as sentences, paragraphs, and documents.\\nOur algorithm represents each document by a dense vector which is trained to\\npredict words in the document. Its construction gives our algorithm the\\npotential to overcome the weaknesses of bag-of-words models. Empirical results\\nshow that Paragraph Vectors outperform bag-of-words models as well as other\\ntechniques for text representations. Finally, we achieve new state-of-the-art\\nresults on several text classification and sentiment analysis tasks.\\n',\n",
              " '  This paper presents preliminary results of Croatian syllable networks\\nanalysis. Syllable network is a network in which nodes are syllables and links\\nbetween them are constructed according to their connections within words. In\\nthis paper we analyze networks of syllables generated from texts collected from\\nthe Croatian Wikipedia and Blogs. As a main tool we use complex network\\nanalysis methods which provide mechanisms that can reveal new patterns in a\\nlanguage structure. We aim to show that syllable networks have much higher\\nclustering coefficient in comparison to Erd\\\\\"os-Renyi random networks. The\\nresults indicate that Croatian syllable networks exhibit certain properties of\\na small world networks. Furthermore, we compared Croatian syllable networks\\nwith Portuguese and Chinese syllable networks and we showed that they have\\nsimilar properties.\\n',\n",
              " '  We present a natural language modelization method which is strongely relying\\non mathematics. This method, called \"Formal Semantics,\" has been initiated by\\nthe American linguist Richard M. Montague in the 1970\\'s. It uses mathematical\\ntools such as formal languages and grammars, first-order logic, type theory and\\n$\\\\lambda$-calculus. Our goal is to have the reader discover both Montagovian\\nformal semantics and the mathematical tools that he used in his method.\\n  -----\\n  Nous pr\\\\\\'esentons une m\\\\\\'ethode de mod\\\\\\'elisation de la langue naturelle qui\\nest fortement bas\\\\\\'ee sur les math\\\\\\'ematiques. Cette m\\\\\\'ethode, appel\\\\\\'ee\\n{\\\\guillemotleft}s\\\\\\'emantique formelle{\\\\guillemotright}, a \\\\\\'et\\\\\\'e initi\\\\\\'ee par\\nle linguiste am\\\\\\'ericain Richard M. Montague dans les ann\\\\\\'ees 1970. Elle\\nutilise des outils math\\\\\\'ematiques tels que les langages et grammaires formels,\\nla logique du 1er ordre, la th\\\\\\'eorie de types et le $\\\\lambda$-calcul. Nous\\nnous proposons de faire d\\\\\\'ecouvrir au lecteur tant la s\\\\\\'emantique formelle de\\nMontague que les outils math\\\\\\'ematiques dont il s\\'est servi.\\n',\n",
              " '  This paper presents a scalable method for integrating compositional\\nmorphological representations into a vector-based probabilistic language model.\\nOur approach is evaluated in the context of log-bilinear language models,\\nrendered suitably efficient for implementation inside a machine translation\\ndecoder by factoring the vocabulary. We perform both intrinsic and extrinsic\\nevaluations, presenting results on a range of languages which demonstrate that\\nour model learns morphological representations that both perform well on word\\nsimilarity tasks and lead to substantial reductions in perplexity. When used\\nfor translation into morphologically rich languages with large vocabularies,\\nour models obtain improvements of up to 1.2 BLEU points relative to a baseline\\nsystem using back-off n-gram models.\\n',\n",
              " '  We present an extended, thematically reinforced version of Gabrilovich and\\nMarkovitch\\'s Explicit Semantic Analysis (ESA), where we obtain thematic\\ninformation through the category structure of Wikipedia. For this we first\\ndefine a notion of categorical tfidf which measures the relevance of terms in\\ncategories. Using this measure as a weight we calculate a maximal spanning tree\\nof the Wikipedia corpus considered as a directed graph of pages and categories.\\nThis tree provides us with a unique path of \"most related categories\" between\\neach page and the top of the hierarchy. We reinforce tfidf of words in a page\\nby aggregating it with categorical tfidfs of the nodes of these paths, and\\ndefine a thematically reinforced ESA semantic relatedness measure which is more\\nrobust than standard ESA and less sensitive to noise caused by out-of-context\\nwords. We apply our method to the French Wikipedia corpus, evaluate it through\\na text classification on a 37.5 MB corpus of 20 French newsgroups and obtain a\\nprecision increase of 9-10% compared with standard ESA.\\n',\n",
              " '  In this paper, we propose an unsupervised method to identify noun sense\\nchanges based on rigorous analysis of time-varying text data available in the\\nform of millions of digitized books. We construct distributional thesauri based\\nnetworks from data at different time points and cluster each of them separately\\nto obtain word-centric sense clusters corresponding to the different time\\npoints. Subsequently, we compare these sense clusters of two different time\\npoints to find if (i) there is birth of a new sense or (ii) if an older sense\\nhas got split into more than one sense or (iii) if a newer sense has been\\nformed from the joining of older senses or (iv) if a particular sense has died.\\nWe conduct a thorough evaluation of the proposed methodology both manually as\\nwell as through comparison with WordNet. Manual evaluation indicates that the\\nalgorithm could correctly identify 60.4% birth cases from a set of 48 randomly\\npicked samples and 57% split/join cases from a set of 21 randomly picked\\nsamples. Remarkably, in 44% cases the birth of a novel sense is attested by\\nWordNet, while in 46% cases and 43% cases split and join are respectively\\nconfirmed by WordNet. Our approach can be applied for lexicography, as well as\\nfor applications like word sense disambiguation or semantic search.\\n',\n",
              " '  In this article, we investigate the structure of Croatian linguistic\\nco-occurrence networks. We examine the change of network structure properties\\nby systematically varying the co-occurrence window sizes, the corpus sizes and\\nremoving stopwords. In a co-occurrence window of size $n$ we establish a link\\nbetween the current word and $n-1$ subsequent words. The results point out that\\nthe increase of the co-occurrence window size is followed by a decrease in\\ndiameter, average path shortening and expectedly condensing the average\\nclustering coefficient. The same can be noticed for the removal of the\\nstopwords. Finally, since the size of texts is reflected in the network\\nproperties, our results suggest that the corpus influence can be reduced by\\nincreasing the co-occurrence window size.\\n',\n",
              " '  The trimming scheme with a prefixed cutoff portion is known as a method of\\nimproving the robustness of statistical models such as multivariate Gaussian\\nmixture models (MG- MMs) in small scale tests by alleviating the impacts of\\noutliers. However, when this method is applied to real- world data, such as\\nnoisy speech processing, it is hard to know the optimal cut-off portion to\\nremove the outliers and sometimes removes useful data samples as well. In this\\npaper, we propose a new method based on measuring the dispersion degree (DD) of\\nthe training data to avoid this problem, so as to realise automatic robust\\nestimation for MGMMs. The DD model is studied by using two different measures.\\nFor each one, we theoretically prove that the DD of the data samples in a\\ncontext of MGMMs approximately obeys a specific (chi or chi-square)\\ndistribution. The proposed method is evaluated on a real-world application with\\na moderately-sized speaker recognition task. Experiments show that the proposed\\nmethod can significantly improve the robustness of the conventional training\\nmethod of GMMs for speaker recognition.\\n',\n",
              " '  Traditional learning-based coreference resolvers operate by training the\\nmention-pair model for determining whether two mentions are coreferent or not.\\nThough conceptually simple and easy to understand, the mention-pair model is\\nlinguistically rather unappealing and lags far behind the heuristic-based\\ncoreference models proposed in the pre-statistical NLP era in terms of\\nsophistication. Two independent lines of recent research have attempted to\\nimprove the mention-pair model, one by acquiring the mention-ranking model to\\nrank preceding mentions for a given anaphor, and the other by training the\\nentity-mention model to determine whether a preceding cluster is coreferent\\nwith a given mention. We propose a cluster-ranking approach to coreference\\nresolution, which combines the strengths of the mention-ranking model and the\\nentity-mention model, and is therefore theoretically more appealing than both\\nof these models. In addition, we seek to improve cluster rankers via two\\nextensions: (1) lexicalization and (2) incorporating knowledge of anaphoricity\\nby jointly modeling anaphoricity determination and coreference resolution.\\nExperimental results on the ACE data sets demonstrate the superior performance\\nof cluster rankers to competing approaches as well as the effectiveness of our\\ntwo extensions.\\n',\n",
              " '  Dual decomposition, and more generally Lagrangian relaxation, is a classical\\nmethod for combinatorial optimization; it has recently been applied to several\\ninference problems in natural language processing (NLP). This tutorial gives an\\noverview of the technique. We describe example algorithms, describe formal\\nguarantees for the method, and describe practical issues in implementing the\\nalgorithms. While our examples are predominantly drawn from the NLP literature,\\nthe material should be of general relevance to inference problems in machine\\nlearning. A central theme of this tutorial is that Lagrangian relaxation is\\nnaturally applied in conjunction with a broad class of combinatorial\\nalgorithms, allowing inference in models that go significantly beyond previous\\nwork on Lagrangian relaxation for inference in graphical models.\\n',\n",
              " '  Chinese characters have a complex and hierarchical graphical structure\\ncarrying both semantic and phonetic information. We use this structure to\\nenhance the text model and obtain better results in standard NLP operations.\\nFirst of all, to tackle the problem of graphical variation we define\\nallographic classes of characters. Next, the relation of inclusion of a\\nsubcharacter in a characters, provides us with a directed graph of allographic\\nclasses. We provide this graph with two weights: semanticity (semantic relation\\nbetween subcharacter and character) and phoneticity (phonetic relation) and\\ncalculate \"most semantic subcharacter paths\" for each character. Finally,\\nadding the information contained in these paths to unigrams we claim to\\nincrease the efficiency of text mining methods. We evaluate our method on a\\ntext classification task on two corpora (Chinese and Japanese) of a total of 18\\nmillion characters and get an improvement of 3% on an already high baseline of\\n89.6% precision, obtained by a linear SVM classifier. Other possible\\napplications and perspectives of the system are discussed.\\n',\n",
              " '  Although the parallel corpus has an irreplaceable role in machine\\ntranslation, its scale and coverage is still beyond the actual needs.\\nNon-parallel corpus resources on the web have an inestimable potential value in\\nmachine translation and other natural language processing tasks. This article\\nproposes a semi-supervised transductive learning method for expanding the\\ntraining corpus in statistical machine translation system by extracting\\nparallel sentences from the non-parallel corpus. This method only requires a\\nsmall amount of labeled corpus and a large unlabeled corpus to build a\\nhigh-performance classifier, especially for when there is short of labeled\\ncorpus. The experimental results show that by combining the non-parallel corpus\\nalignment and the semi-supervised transductive learning method, we can more\\neffectively use their respective strengths to improve the performance of\\nmachine translation system.\\n',\n",
              " '  Economic issues related to the information processing techniques are very\\nimportant. The development of such technologies is a major asset for developing\\ncountries like Cambodia and Laos, and emerging ones like Vietnam, Malaysia and\\nThailand. The MotAMot project aims to computerize an under-resourced language:\\nKhmer, spoken mainly in Cambodia. The main goal of the project is the\\ndevelopment of a multilingual lexical system targeted for Khmer. The\\nmacrostructure is a pivot one with each word sense of each language linked to a\\npivot axi. The microstructure comes from a simplification of the explanatory\\nand combinatory dictionary. The lexical system has been initialized with data\\ncoming mainly from the conversion of the French-Khmer bilingual dictionary of\\nDenis Richer from Word to XML format. The French part was completed with\\npronunciation and parts-of-speech coming from the FeM French-english-Malay\\ndictionary. The Khmer headwords noted in IPA in the Richer dictionary were\\nconverted to Khmer writing with OpenFST, a finite state transducer tool. The\\nresulting resource is available online for lookup, editing, download and remote\\nprogramming via a REST API on a Jibiki platform.\\n',\n",
              " '  This paper relates work done during the DiLAF project. It consists in\\nconverting 5 bilingual African language-French dictionaries originally in Word\\nformat into XML following the LMF model. The languages processed are Bambara,\\nHausa, Kanuri, Tamajaq and Songhai-zarma, still considered as under-resourced\\nlanguages concerning Natural Language Processing tools. Once converted, the\\ndictionaries are available online on the Jibiki platform for lookup and\\nmodification. The DiLAF project is first presented. A description of each\\ndictionary follows. Then, the conversion methodology from .doc format to XML\\nfiles is presented. A specific point on the usage of Unicode follows. Then,\\neach step of the conversion into XML and LMF is detailed. The last part\\npresents the Jibiki lexical resources management platform used for the project.\\n',\n",
              " '  The technique of building of networks of hierarchies of terms based on the\\nanalysis of chosen text corpora is offered. The technique is based on the\\nmethodology of horizontal visibility graphs. Constructed and investigated\\nlanguage network, formed on the basis of electronic preprints arXiv on topics\\nof information retrieval.\\n',\n",
              " '  The Swiss avalanche bulletin is produced twice a day in four languages. Due\\nto the lack of time available for manual translation, a fully automated\\ntranslation system is employed, based on a catalogue of predefined phrases and\\npredetermined rules of how these phrases can be combined to produce sentences.\\nThe system is able to automatically translate such sentences from German into\\nthe target languages French, Italian and English without subsequent\\nproofreading or correction. Our catalogue of phrases is limited to a small\\nsublanguage. The reduction of daily translation costs is expected to offset the\\ninitial development costs within a few years. After being operational for two\\nwinter seasons, we assess here the quality of the produced texts based on an\\nevaluation where participants rate real danger descriptions from both origins,\\nthe catalogue of phrases versus the manually written and translated texts. With\\na mean recognition rate of 55%, users can hardly distinguish between the two\\ntypes of texts, and give similar ratings with respect to their language\\nquality. Overall, the output from the catalogue system can be considered\\nvirtually equivalent to a text written by avalanche forecasters and then\\nmanually translated by professional translators. Furthermore, forecasters\\ndeclared that all relevant situations were captured by the system with\\nsufficient accuracy and within the limited time available.\\n',\n",
              " '  We present NaturalOWL, a natural language generation system that produces\\ntexts describing individuals or classes of OWL ontologies. Unlike simpler OWL\\nverbalizers, which typically express a single axiom at a time in controlled,\\noften not entirely fluent natural language primarily for the benefit of domain\\nexperts, we aim to generate fluent and coherent multi-sentence texts for\\nend-users. With a system like NaturalOWL, one can publish information in OWL on\\nthe Web, along with automatically produced corresponding texts in multiple\\nlanguages, making the information accessible not only to computer programs and\\ndomain experts, but also end-users. We discuss the processing stages of\\nNaturalOWL, the optional domain-dependent linguistic resources that the system\\ncan use at each stage, and why they are useful. We also present trials showing\\nthat when the domain-dependent llinguistic resources are available, NaturalOWL\\nproduces significantly better texts compared to a simpler verbalizer, and that\\nthe resources can be created with relatively light effort.\\n',\n",
              " '  Name matching between multiple natural languages is an important step in\\ncross-enterprise integration applications and data mining. It is difficult to\\ndecide whether or not two syntactic values (names) from two heterogeneous data\\nsources are alternative designation of the same semantic entity (person), this\\nprocess becomes more difficult with Arabic language due to several factors\\nincluding spelling and pronunciation variation, dialects and special vowel and\\nconsonant distinction and other linguistic characteristics. This paper proposes\\na new framework for name matching between the Arabic language and other\\nlanguages. The framework uses a dictionary based on a new proposed version of\\nthe Soundex algorithm to encapsulate the recognition of special features of\\nArabic names. The framework proposes a new proximity matching algorithm to suit\\nthe high importance of order sensitivity in Arabic name matching. New\\nperformance evaluation metrics are proposed as well. The framework is\\nimplemented and verified empirically in several case studies demonstrating\\nsubstantial improvements compared to other well-known techniques found in\\nliterature.\\n',\n",
              " '  This paper re-investigates a lexical acquisition system initially developed\\nfor French.We show that, interestingly, the architecture of the system\\nreproduces and implements the main components of Optimality Theory. However, we\\nformulate the hypothesis that some of its limitations are mainly due to a poor\\nrepresentation of the constraints used. Finally, we show how a better\\nrepresentation of the constraints used would yield better results.\\n',\n",
              " '  This paper reports about our work in the ICON 2013 NLP TOOLS CONTEST on Named\\nEntity Recognition. We submitted runs for Bengali, English, Hindi, Marathi,\\nPunjabi, Tamil and Telugu. A statistical HMM (Hidden Markov Models) based model\\nhas been used to implement our system. The system has been trained and tested\\non the NLP TOOLS CONTEST: ICON 2013 datasets. Our system obtains F-measures of\\n0.8599, 0.7704, 0.7520, 0.4289, 0.5455, 0.4466, and 0.4003 for Bengali,\\nEnglish, Hindi, Marathi, Punjabi, Tamil and Telugu respectively.\\n',\n",
              " '  We present a novel framework for learning to interpret and generate language\\nusing only perceptual context as supervision. We demonstrate its capabilities\\nby developing a system that learns to sportscast simulated robot soccer games\\nin both English and Korean without any language-specific prior knowledge.\\nTraining employs only ambiguous supervision consisting of a stream of\\ndescriptive textual comments and a sequence of events extracted from the\\nsimulation trace. The system simultaneously establishes correspondences between\\nindividual comments and the events that they describe while building a\\ntranslation model that supports both parsing and generation. We also present a\\nnovel algorithm for learning which events are worth describing. Human\\nevaluations of the generated commentaries indicate they are of reasonable\\nquality and in some cases even on par with those produced by humans for our\\nlimited domain.\\n',\n",
              " '  This paper discusses the problem of marrying structural similarity with\\nsemantic relatedness for Information Extraction from text. Aiming at accurate\\nrecognition of relations, we introduce local alignment kernels and explore\\nvarious possibilities of using them for this task. We give a definition of a\\nlocal alignment (LA) kernel based on the Smith-Waterman score as a sequence\\nsimilarity measure and proceed with a range of possibilities for computing\\nsimilarity between elements of sequences. We show how distributional similarity\\nmeasures obtained from unlabeled data can be incorporated into the learning\\ntask as semantic knowledge. Our experiments suggest that the LA kernel yields\\npromising results on various biomedical corpora outperforming two baselines by\\na large margin. Additional series of experiments have been conducted on the\\ndata sets of seven general relation types, where the performance of the LA\\nkernel is comparable to the current state-of-the-art results.\\n',\n",
              " '  Semantic composition is the task of understanding the meaning of text by\\ncomposing the meanings of the individual words in the text. Semantic\\ndecomposition is the task of understanding the meaning of an individual word by\\ndecomposing it into various aspects (factors, constituents, components) that\\nare latent in the meaning of the word. We take a distributional approach to\\nsemantics, in which a word is represented by a context vector. Much recent work\\nhas considered the problem of recognizing compositions and decompositions, but\\nwe tackle the more difficult generation problem. For simplicity, we focus on\\nnoun-modifier bigrams and noun unigrams. A test for semantic composition is,\\ngiven context vectors for the noun and modifier in a noun-modifier bigram (\"red\\nsalmon\"), generate a noun unigram that is synonymous with the given bigram\\n(\"sockeye\"). A test for semantic decomposition is, given a context vector for a\\nnoun unigram (\"snifter\"), generate a noun-modifier bigram that is synonymous\\nwith the given unigram (\"brandy glass\"). With a vocabulary of about 73,000\\nunigrams from WordNet, there are 73,000 candidate unigram compositions for a\\nbigram and 5,300,000,000 (73,000 squared) candidate bigram decompositions for a\\nunigram. We generate ranked lists of potential solutions in two passes. A fast\\nunsupervised learning algorithm generates an initial list of candidates and\\nthen a slower supervised learning algorithm refines the list. We evaluate the\\ncandidate solutions by comparing them to WordNet synonym sets. For\\ndecomposition (unigram to bigram), the top 100 most highly ranked bigrams\\ninclude a WordNet synonym of the given unigram 50.7% of the time. For\\ncomposition (bigram to unigram), the top 100 most highly ranked unigrams\\ninclude a WordNet synonym of the given bigram 77.8% of the time.\\n',\n",
              " \"  Several messages express opinions about events, products, and services,\\npolitical views or even their author's emotional state and mood. Sentiment\\nanalysis has been used in several applications including analysis of the\\nrepercussions of events in social networks, analysis of opinions about products\\nand services, and simply to better understand aspects of social communication\\nin Online Social Networks (OSNs). There are multiple methods for measuring\\nsentiments, including lexical-based approaches and supervised machine learning\\nmethods. Despite the wide use and popularity of some methods, it is unclear\\nwhich method is better for identifying the polarity (i.e., positive or\\nnegative) of a message as the current literature does not provide a method of\\ncomparison among existing methods. Such a comparison is crucial for\\nunderstanding the potential limitations, advantages, and disadvantages of\\npopular methods in analyzing the content of OSNs messages. Our study aims at\\nfilling this gap by presenting comparisons of eight popular sentiment analysis\\nmethods in terms of coverage (i.e., the fraction of messages whose sentiment is\\nidentified) and agreement (i.e., the fraction of identified sentiments that are\\nin tune with ground truth). We develop a new method that combines existing\\napproaches, providing the best coverage results and competitive agreement. We\\nalso present a free Web service called iFeel, which provides an open API for\\naccessing and comparing results across different sentiment methods for a given\\ntext.\\n\",\n",
              " '  The use of Structured English as a computation independent knowledge\\nrepresentation format for non-technical users in business rules representation\\nhas been proposed in OMGs Semantics and Business Vocabulary Representation\\n(SBVR). In the legal domain we face a similar problem. Formal representation\\nlanguages, such as OASIS LegalRuleML and legal ontologies (LKIF, legal OWL2\\nontologies etc.) support the technical knowledge engineer and the automated\\nreasoning. But, they can be hardly used directly by the legal domain experts\\nwho do not have a computer science background. In this paper we adapt the SBVR\\nStructured English approach for the legal domain and implement a\\nproof-of-concept, called KR4IPLaw, which enables legal domain experts to\\nrepresent their knowledge in Structured English in a computational independent\\nand hence, for them, more usable way. The benefit of this approach is that the\\nunderlying pre-defined semantics of the Structured English approach makes\\ntransformations into formal languages such as OASIS LegalRuleML and OWL2\\nontologies possible. We exemplify our approach in the domain of patent law.\\n',\n",
              " '  In this paper, we propose a novel neural network model called RNN\\nEncoder-Decoder that consists of two recurrent neural networks (RNN). One RNN\\nencodes a sequence of symbols into a fixed-length vector representation, and\\nthe other decodes the representation into another sequence of symbols. The\\nencoder and decoder of the proposed model are jointly trained to maximize the\\nconditional probability of a target sequence given a source sequence. The\\nperformance of a statistical machine translation system is empirically found to\\nimprove by using the conditional probabilities of phrase pairs computed by the\\nRNN Encoder-Decoder as an additional feature in the existing log-linear model.\\nQualitatively, we show that the proposed model learns a semantically and\\nsyntactically meaningful representation of linguistic phrases.\\n',\n",
              " \"  Sentence extraction based summarization methods has some limitations as it\\ndoesn't go into the semantics of the document. Also, it lacks the capability of\\nsentence generation which is intuitive to humans. Here we present a novel\\nmethod to summarize text documents taking the process to semantic levels with\\nthe use of WordNet and other resources, and using a technique for sentence\\ngeneration. We involve semantic role labeling to get the semantic\\nrepresentation of text and use of segmentation to form clusters of the related\\npieces of text. Picking out the centroids and sentence generation completes the\\ntask. We evaluate our system against human composed summaries and also present\\nan evaluation done by humans to measure the quality attributes of our\\nsummaries.\\n\",\n",
              " '  \"How to generate a sentence\" is the most critical and difficult problem in\\nall the natural language processing technologies. In this paper, we present a\\nnew approach to explain the generation process of a sentence from the\\nperspective of mathematics. Our method is based on the premise that in our\\nbrain a sentence is a part of a word network which is formed by many word\\nnodes. Experiments show that the probability of the entire sentence can be\\nobtained by the probabilities of single words and the probabilities of the\\nco-occurrence of word pairs, which indicate that human use the synthesis method\\nto generate a sentence.\\n',\n",
              " '  It has been proved that large scale realistic Knowledge Based Machine\\nTranslation applications require acquisition of huge knowledge about language\\nand about the world. This knowledge is encoded in computational grammars,\\nlexicons and domain models. Another approach which avoids the need for\\ncollecting and analyzing massive knowledge, is the Example Based approach,\\nwhich is the topic of this paper. We show through the paper that using Example\\nBased in its native form is not suitable for translating into Arabic. Therefore\\na modification to the basic approach is presented to improve the accuracy of\\nthe translation process. The basic idea of the new approach is to improve the\\ntechnique by which template-based approaches select the appropriate templates.\\n',\n",
              " '  Development of a proper names pronunciation lexicon is usually a manual\\neffort which can not be avoided. Grapheme to phoneme (G2P) conversion modules,\\nin literature, are usually rule based and work best for non-proper names in a\\nparticular language. Proper names are foreign to a G2P module. We follow an\\noptimization approach to enable automatic construction of proper names\\npronunciation lexicon. The idea is to construct a small orthogonal set of words\\n(basis) which can span the set of names in a given database. We propose two\\nalgorithms for the construction of this basis. The transcription lexicon of all\\nthe proper names in a database can be produced by the manual transcription of\\nonly the small set of basis words. We first construct a cost function and show\\nthat the minimization of the cost function results in a basis. We derive\\nconditions for convergence of this cost function and validate them\\nexperimentally on a very large proper name database. Experiments show the\\ntranscription can be achieved by transcribing a set of small number of basis\\nwords. The algorithms proposed are generic and independent of language; however\\nperformance is better if the proper names have same origin, namely, same\\nlanguage or geographical region.\\n',\n",
              " \"  Tree-structured recursive neural networks (TreeRNNs) for sentence meaning\\nhave been successful for many applications, but it remains an open question\\nwhether the fixed-length representations that they learn can support tasks as\\ndemanding as logical deduction. We pursue this question by evaluating whether\\ntwo such models---plain TreeRNNs and tree-structured neural tensor networks\\n(TreeRNTNs)---can correctly learn to identify logical relationships such as\\nentailment and contradiction using these representations. In our first set of\\nexperiments, we generate artificial data from a logical grammar and use it to\\nevaluate the models' ability to learn to handle basic relational reasoning,\\nrecursive structures, and quantification. We then evaluate the models on the\\nmore natural SICK challenge data. Both models perform competitively on the SICK\\ndata and generalize well in all three experiments on simulated data, suggesting\\nthat they can learn suitable representations for logical inference in natural\\nlanguage.\\n\",\n",
              " '  IsiZulu is one of the eleven official languages of South Africa and roughly\\nhalf the population can speak it. It is the first (home) language for over 10\\nmillion people in South Africa. Only a few computational resources exist for\\nisiZulu and its related Nguni languages, yet the imperative for tool\\ndevelopment exists. We focus on natural language generation, and the grammar\\noptions and preferences in particular, which will inform verbalization of\\nknowledge representation languages and could contribute to machine translation.\\nThe verbalization pattern specification shows that the grammar rules are\\nelaborate and there are several options of which one may have preference. We\\ndevised verbalization patterns for subsumption, basic disjointness, existential\\nand universal quantification, and conjunction. This was evaluated in a survey\\namong linguists and non-linguists. Some differences between linguists and\\nnon-linguists can be observed, with the former much more in agreement, and\\npreferences depend on the overall structure of the sentence, such as singular\\nfor subsumption and plural in other cases.\\n',\n",
              " '  Protein-protein interaction extraction is the key precondition of the\\nconstruction of protein knowledge network, and it is very important for the\\nresearch in the biomedicine. This paper extracted directional protein-protein\\ninteraction from the biological text, using the SVM-based method. Experiments\\nwere evaluated on the LLL05 corpus with good results. The results show that\\ndependency features are import for the protein-protein interaction extraction\\nand features related to the interaction word are effective for the interaction\\ndirection judgment. At last, we analyzed the effects of different features and\\nplaned for the next step.\\n',\n",
              " '  We propose a new method for learning word representations using hierarchical\\nregularization in sparse coding inspired by the linguistic study of word\\nmeanings. We show an efficient learning algorithm based on stochastic proximal\\nmethods that is significantly faster than previous approaches, making it\\npossible to perform hierarchical sparse coding on a corpus of billions of word\\ntokens. Experiments on various benchmark tasks---word similarity ranking,\\nanalogies, sentence completion, and sentiment analysis---demonstrate that the\\nmethod outperforms or is competitive with state-of-the-art methods. Our word\\nrepresentations are available at\\n\\\\url{http://www.ark.cs.cmu.edu/dyogatam/wordvecs/}.\\n',\n",
              " \"  Recent developments in controlled natural language editors for knowledge\\nengineering (KE) have given rise to expectations that they will make KE tasks\\nmore accessible and perhaps even enable non-engineers to build knowledge bases.\\nThis exploratory research focussed on novices and experts in knowledge\\nengineering during their attempts to learn a controlled natural language (CNL)\\nknown as OWL Simplified English and use it to build a small knowledge base.\\nParticipants' behaviours during the task were observed through eye-tracking and\\nscreen recordings. This was an attempt at a more ambitious user study than in\\nprevious research because we used a naturally occurring text as the source of\\ndomain knowledge, and left them without guidance on which information to\\nselect, or how to encode it. We have identified a number of skills\\n(competencies) required for this difficult task and key problems that authors\\nface.\\n\",\n",
              " '  This paper presents a currently bilingual but potentially multilingual\\nFrameNet-based grammar library implemented in Grammatical Framework. The\\ncontribution of this paper is two-fold. First, it offers a methodological\\napproach to automatically generate the grammar based on semantico-syntactic\\nvalence patterns extracted from FrameNet-annotated corpora. Second, it provides\\na proof of concept for two use cases illustrating how the acquired multilingual\\ngrammar can be exploited in different CNL applications in the domains of arts\\nand tourism.\\n',\n",
              " '  The paper presents a FrameNet-based information extraction and knowledge\\nrepresentation framework, called FrameNet-CNL. The framework is used on natural\\nlanguage documents and represents the extracted knowledge in a tailor-made\\nFrame-ontology from which unambiguous FrameNet-CNL paraphrase text can be\\ngenerated automatically in multiple languages. This approach brings together\\nthe fields of information extraction and CNL, because a source text can be\\nconsidered belonging to FrameNet-CNL, if information extraction parser produces\\nthe correct knowledge representation as a result. We describe a\\nstate-of-the-art information extraction parser used by a national news agency\\nand speculate that FrameNet-CNL eventually could shape the natural language\\nsubset used for writing the newswire articles.\\n',\n",
              " '  One of the main challenges for building the Semantic web is Ontology\\nAuthoring. Controlled Natural Languages CNLs offer a user friendly means for\\nnon-experts to author ontologies. This paper provides a snapshot of the\\nstate-of-the-art for the core CNLs for ontology authoring and reviews their\\nrespective evaluations.\\n',\n",
              " '  Sentiment analysis of Twitter data is performed. The researcher has made the\\nfollowing contributions via this paper: (1) an innovative method for deriving\\nsentiment score dictionaries using an existing sentiment dictionary as seed\\nwords is explored, and (2) an analysis of clustered tweet sentiment scores\\nbased on tweet length is performed.\\n',\n",
              " '  Controlled natural languages for industrial application are often regarded as\\na response to the challenges of translation and multilingual communication.\\nThis paper presents a quite different approach taken by Koenig & Bauer AG,\\nwhere the main goal was the improvement of the authoring process for technical\\ndocumentation. Most importantly, this paper explores the notion of a controlled\\nlanguage and demonstrates how style guides can emerge from non-linguistic\\nconsiderations. Moreover, it shows the transition from loose language\\nrecommendations into precise and prescriptive rules and investigates whether\\nsuch rules can be regarded as a full-fledged controlled language.\\n',\n",
              " '  This paper presents a system which learns to answer questions on a broad\\nrange of topics from a knowledge base using few hand-crafted features. Our\\nmodel learns low-dimensional embeddings of words and knowledge base\\nconstituents; these representations are used to score natural language\\nquestions against candidate answers. Training our system using pairs of\\nquestions and structured representations of their answers, and pairs of\\nquestion paraphrases, yields competitive results on a competitive benchmark of\\nthe literature.\\n',\n",
              " '  Todays world is a world of Internet, almost all work can be done with the\\nhelp of it, from simple mobile phone recharge to biggest business deals can be\\ndone with the help of this technology. People spent their most of the times on\\nsurfing on the Web it becomes a new source of entertainment, education,\\ncommunication, shopping etc. Users not only use these websites but also give\\ntheir feedback and suggestions that will be useful for other users. In this way\\na large amount of reviews of users are collected on the Web that needs to be\\nexplored, analyse and organized for better decision making. Opinion Mining or\\nSentiment Analysis is a Natural Language Processing and Information Extraction\\ntask that identifies the users views or opinions explained in the form of\\npositive, negative or neutral comments and quotes underlying the text. Aspect\\nbased opinion mining is one of the level of Opinion mining that determines the\\naspect of the given reviews and classify the review for each feature. In this\\npaper an aspect based opinion mining system is proposed to classify the reviews\\nas positive, negative and neutral for each feature. Negation is also handled in\\nthe proposed system. Experimental results using reviews of products show the\\neffectiveness of the system.\\n',\n",
              " '  Capturing the compositional process which maps the meaning of words to that\\nof documents is a central challenge for researchers in Natural Language\\nProcessing and Information Retrieval. We introduce a model that is able to\\nrepresent the meaning of documents by embedding them in a low dimensional\\nvector space, while preserving distinctions of word and sentence order crucial\\nfor capturing nuanced semantics. Our model is based on an extended Dynamic\\nConvolution Neural Network, which learns convolution filters at both the\\nsentence and document level, hierarchically learning to capture and compose low\\nlevel lexical features into high level semantic concepts. We demonstrate the\\neffectiveness of this model on a range of document modelling tasks, achieving\\nstrong results with no feature engineering and with a more compact model.\\nInspired by recent advances in visualising deep convolution networks for\\ncomputer vision, we present a novel visualisation technique for our document\\nnetworks which not only provides insight into their learning process, but also\\ncan be interpreted to produce a compelling automatic summarisation system for\\ntexts.\\n',\n",
              " '  In todays digital world automated Machine Translation of one language to\\nanother has covered a long way to achieve different kinds of success stories.\\nWhereas Babel Fish supports a good number of foreign languages and only Hindi\\nfrom Indian languages, the Google Translator takes care of about 10 Indian\\nlanguages. Though most of the Automated Machine Translation Systems are doing\\nwell but handling Indian languages needs a major care while handling the local\\nproverbs/ idioms. Most of the Machine Translation system follows the direct\\ntranslation approach while translating one Indian language to other. Our\\nresearch at KMIT R&D Lab found that handling the local proverbs/idioms is not\\ngiven enough attention by the earlier research work. This paper focuses on two\\nof the majorly spoken Indian languages Marathi and Telugu, and translation\\nbetween them. Handling proverbs and idioms of both the languages have been\\ngiven a special care, and the research outcome shows a significant achievement\\nin this direction.\\n',\n",
              " '  In this paper, we describe methods for handling multilingual\\nnon-compositional constructions in the framework of GF. We specifically look at\\nmethods to detect and extract non-compositional phrases from parallel texts and\\npropose methods to handle such constructions in GF grammars. We expect that the\\nmethods to handle non-compositional constructions will enrich CNLs by providing\\nmore flexibility in the design of controlled languages. We look at two specific\\nuse cases of non-compositional constructions: a general-purpose method to\\ndetect and extract multilingual multiword expressions and a procedure to\\nidentify nominal compounds in German. We evaluate our procedure for multiword\\nexpressions by performing a qualitative analysis of the results. For the\\nexperiments on nominal compounds, we incorporate the detected compounds in a\\nfull SMT pipeline and evaluate the impact of our method in machine translation\\nprocess.\\n',\n",
              " '  In this paper, we investigate and experiment the notion of error correction\\nmemory applied to error correction in technical texts. The main purpose is to\\ninduce relatively generic correction patterns associated with more contextual\\ncorrection recommendations, based on previously memorized and analyzed\\ncorrections. The notion of error correction memory is developed within the\\nframework of the LELIE project and illustrated on the case of fuzzy lexical\\nitems, which is a major problem in technical texts.\\n',\n",
              " '  Inspired by embedded programming languages, an embedded CNL (controlled\\nnatural language) is a proper fragment of an entire natural language (its host\\nlanguage), but it has a parser that recognizes the entire host language. This\\nmakes it possible to process out-of-CNL input and give useful feedback to\\nusers, instead of just reporting syntax errors. This extended abstract explains\\nthe main concepts of embedded CNL implementation in GF (Grammatical Framework),\\nwith examples from machine translation and some other ongoing work.\\n',\n",
              " '  In this paper we describe our contribution to the PoliInformatics 2014\\nChallenge on the 2007-2008 financial crisis. We propose a state of the art\\ntechnique to extract information from texts and provide different\\nrepresentations, giving first a static overview of the domain and then a\\ndynamic representation of its main evolutions. We show that this strategy\\nprovides a practical solution to some recent theories in social sciences that\\nare facing a lack of methods and tools to automatically extract information\\nfrom natural language texts.\\n',\n",
              " '  A method for authorship attribution based on function word adjacency networks\\n(WANs) is introduced. Function words are parts of speech that express\\ngrammatical relationships between other words but do not carry lexical meaning\\non their own. In the WANs in this paper, nodes are function words and directed\\nedges stand in for the likelihood of finding the sink word in the ordered\\nvicinity of the source word. WANs of different authors can be interpreted as\\ntransition probabilities of a Markov chain and are therefore compared in terms\\nof their relative entropies. Optimal selection of WAN parameters is studied and\\nattribution accuracy is benchmarked across a diverse pool of authors and\\nvarying text lengths. This analysis shows that, since function words are\\nindependent of content, their use tends to be specific to an author and that\\nthe relational data captured by function WANs is a good summary of stylometric\\nfingerprints. Attribution accuracy is observed to exceed the one achieved by\\nmethods that rely on word frequencies alone. Further combining WANs with\\nmethods that rely on word frequencies alone, results in larger attribution\\naccuracy, indicating that both sources of information encode different aspects\\nof authorial styles.\\n',\n",
              " \"  Within the categorical compositional distributional model of meaning, we\\nprovide semantic interpretations for the subject and object roles of the\\npossessive relative pronoun `whose'. This is done in terms of Frobenius\\nalgebras over compact closed categories. These algebras and their diagrammatic\\nlanguage expose how meanings of words in relative clauses interact with each\\nother. We show how our interpretation is related to Montague-style semantics\\nand provide a truth-theoretic interpretation. We also show how vector spaces\\nprovide a concrete interpretation and provide preliminary corpus-based\\nexperimental evidence. In a prequel to this paper, we used similar methods and\\ndealt with the case of subject and object relative pronouns.\\n\",\n",
              " '  The semantics of determiner phrases, be they definite de- scriptions,\\nindefinite descriptions or quantified noun phrases, is often as- sumed to be a\\nfully solved question: common nouns are properties, and determiners are\\ngeneralised quantifiers that apply to two predicates: the property\\ncorresponding to the common noun and the one corresponding to the verb phrase.\\nWe first present a criticism of this standard view. Firstly, the semantics of\\ndeterminers does not follow the syntactical structure of the sentence. Secondly\\nthe standard interpretation of the indefinite article cannot ac- count for\\nnominal sentences. Thirdly, the standard view misses the linguis- tic asymmetry\\nbetween the two properties of a generalised quantifier. In the sequel, we\\npropose a treatment of determiners and quantifiers as Hilbert terms in a richly\\ntyped system that we initially developed for lexical semantics, using a many\\nsorted logic for semantical representations. We present this semantical\\nframework called the Montagovian generative lexicon and show how these terms\\nbetter match the syntactical structure and avoid the aforementioned problems of\\nthe standard approach. Hilbert terms rather differ from choice functions in\\nthat there is one polymorphic operator and not one operator per formula. They\\nalso open an intriguing connection between the logic for meaning assembly, the\\ntyped lambda calculus handling compositionality and the many-sorted logic for\\nsemantical representations. Furthermore epsilon terms naturally introduce\\ntype-judgements and confirm the claim that type judgment are a form of\\npresupposition.\\n',\n",
              " '  While language competition models of diachronic language shift are\\nincreasingly sophisticated, drawing on sociolinguistic components like variable\\nlanguage prestige, distance from language centers and intermediate bilingual\\ntransitionary populations, in one significant way they fall short. They fail to\\nconsider contact-based outcomes resulting in mixed language practices, e.g.\\noutcome scenarios such as creoles or unmarked code switching as an emergent\\ncommunicative norm. On these lines something very interesting is uncovered in\\nIndia, where traditionally there have been monolingual Hindi speakers and\\nHindi/English bilinguals, but virtually no monolingual English speakers. While\\nthe Indian census data reports a sharp increase in the proportion of\\nHindi/English bilinguals, we argue that the number of Hindi/English bilinguals\\nin India is inaccurate, given a new class of urban individuals speaking a mixed\\nlect of Hindi and English, popularly known as \"Hinglish\". Based on\\npredator-prey, sociolinguistic theories, salient local ecological factors and\\nthe rural-urban divide in India, we propose a new mathematical model of\\ninteracting monolingual Hindi speakers, Hindi/English bilinguals and Hinglish\\nspeakers. The model yields globally asymptotic stable states of coexistence, as\\nwell as bilingual extinction. To validate our model, sociolinguistic data from\\ndifferent Indian classes are contrasted with census reports: We see that\\npurported urban Hindi/English bilinguals are unable to maintain fluent Hindi\\nspeech and instead produce Hinglish, whereas rural speakers evidence\\nmonolingual Hindi. Thus we present evidence for the first time where an\\nunrecognized mixed lect involving English but not \"English\", has possibly taken\\nover a sizeable faction of a large global population.\\n',\n",
              " \"  With Zipf's law being originally and most famously observed for word\\nfrequency, it is surprisingly limited in its applicability to human language,\\nholding over no more than three to four orders of magnitude before hitting a\\nclear break in scaling. Here, building on the simple observation that phrases\\nof one or more words comprise the most coherent units of meaning in language,\\nwe show empirically that Zipf's law for phrases extends over as many as nine\\norders of rank magnitude. In doing so, we develop a principled and scalable\\nstatistical mechanical method of random text partitioning, which opens up a\\nrich frontier of rigorous text analysis via a rank ordering of mixed length\\nphrases.\\n\",\n",
              " '  Text classification is a task of automatic classification of text into one of\\nthe predefined categories. The problem of text classification has been widely\\nstudied in different communities like natural language processing, data mining\\nand information retrieval. Text classification is an important constituent in\\nmany information management tasks like topic identification, spam filtering,\\nemail routing, language identification, genre classification, readability\\nassessment etc. The performance of text classification improves notably when\\nphrase patterns are used. The use of phrase patterns helps in capturing\\nnon-local behaviours and thus helps in the improvement of text classification\\ntask. Phrase structure extraction is the first step to continue with the phrase\\npattern identification. In this survey, detailed study of phrase structure\\nlearning methods have been carried out. This will enable future work in several\\nNLP tasks, which uses syntactic information from phrase structure like grammar\\ncheckers, question answering, information extraction, machine translation, text\\nclassification. The paper also provides different levels of classification and\\ndetailed comparison of the phrase structure learning methods.\\n',\n",
              " \"  We present a first step towards a framework for defining and manipulating\\nnormative documents or contracts described as Contract-Oriented (C-O) Diagrams.\\nThese diagrams provide a visual representation for such texts, giving the\\npossibility to express a signatory's obligations, permissions and prohibitions,\\nwith or without timing constraints, as well as the penalties resulting from the\\nnon-fulfilment of a contract. This work presents a CNL for verbalising C-O\\nDiagrams, a web-based tool allowing editing in this CNL, and another for\\nvisualising and manipulating the diagrams interactively. We then show how these\\nproof-of-concept tools can be used by applying them to a small example.\\n\",\n",
              " '  The purpose of speech emotion recognition system is to classify speakers\\nutterances into different emotional states such as disgust, boredom, sadness,\\nneutral and happiness. Speech features that are commonly used in speech emotion\\nrecognition rely on global utterance level prosodic features. In our work, we\\nevaluate the impact of frame level feature extraction. The speech samples are\\nfrom Berlin emotional database and the features extracted from these utterances\\nare energy, different variant of mel frequency cepstrum coefficients, velocity\\nand acceleration features.\\n',\n",
              " '  While most topic modeling algorithms model text corpora with unigrams, human\\ninterpretation often relies on inherent grouping of terms into phrases. As\\nsuch, we consider the problem of discovering topical phrases of mixed lengths.\\nExisting work either performs post processing to the inference results of\\nunigram-based topic models, or utilizes complex n-gram-discovery topic models.\\nThese methods generally produce low-quality topical phrases or suffer from poor\\nscalability on even moderately-sized datasets. We propose a different approach\\nthat is both computationally efficient and effective. Our solution combines a\\nnovel phrase mining framework to segment a document into single and multi-word\\nphrases, and a new topic model that operates on the induced document partition.\\nOur approach discovers high quality topical phrases with negligible extra cost\\nto the bag-of-words topic model in a variety of datasets including research\\npublication titles, abstracts, reviews, and news articles.\\n',\n",
              " '  In this paper we present an ongoing research investigating the possibility\\nand potential of integrating frame semantics, particularly FrameNet, in the\\nGrammatical Framework (GF) application grammar development. An important\\ncomponent of GF is its Resource Grammar Library (RGL) that encapsulates the\\nlow-level linguistic knowledge about morphology and syntax of currently more\\nthan 20 languages facilitating rapid development of multilingual applications.\\nIn the ideal case, porting a GF application grammar to a new language would\\nonly require introducing the domain lexicon - translation equivalents that are\\ninterlinked via common abstract terms. While it is possible for a highly\\nrestricted CNL, developing and porting a less restricted CNL requires above\\naverage linguistic knowledge about the particular language, and above average\\nGF experience. Specifying a lexicon is mostly straightforward in the case of\\nnouns (incl. multi-word units), however, verbs are the most complex category\\n(in terms of both inflectional paradigms and argument structure), and adding\\nthem to a GF application grammar is not a straightforward task. In this paper\\nwe are focusing on verbs, investigating the possibility of creating a\\nmultilingual FrameNet-based GF library. We propose an extension to the current\\nRGL, allowing GF application developers to define clauses on the semantic\\nlevel, thus leaving the language-specific syntactic mapping to this extension.\\nWe demonstrate our approach by reengineering the MOLTO Phrasebook application\\ngrammar.\\n',\n",
              " '  The speech feature extraction has been a key focus in robust speech\\nrecognition research; it significantly affects the recognition performance. In\\nthis paper, we first study a set of different features extraction methods such\\nas linear predictive coding (LPC), mel frequency cepstral coefficient (MFCC)\\nand perceptual linear prediction (PLP) with several features normalization\\ntechniques like rasta filtering and cepstral mean subtraction (CMS). Based on\\nthis, a comparative evaluation of these features is performed on the task of\\ntext independent speaker identification using a combination between gaussian\\nmixture models (GMM) and linear and non-linear kernels based on support vector\\nmachine (SVM).\\n',\n",
              " '  The computational handling of Modern Standard Arabic is a challenge in the\\nfield of natural language processing due to its highly rich morphology.\\nHowever, several authors have pointed out that the Arabic morphological system\\nis in fact extremely regular. The existing Arabic morphological analyzers have\\nexploited this regularity to variable extent, yet we believe there is still\\nsome scope for improvement. Taking inspiration in traditional Arabic prosody,\\nwe have designed and implemented a compact and simple morphological system\\nwhich in our opinion takes further advantage of the regularities encountered in\\nthe Arabic morphological system. The output of the system is a large-scale\\nlexicon of inflected forms that has subsequently been used to create an Online\\nInterface for a morphological analyzer of Arabic verbs. The Jabalin Online\\nInterface is available at http://elvira.lllf.uam.es/jabalin/, hosted at the\\nLLI-UAM lab. The generation system is also available under a GNU GPL 3 license.\\n',\n",
              " '  Deep neural networks (DNNs) are now a central component of nearly all\\nstate-of-the-art speech recognition systems. Building neural network acoustic\\nmodels requires several design decisions including network architecture, size,\\nand training loss function. This paper offers an empirical investigation on\\nwhich aspects of DNN acoustic model design are most important for speech\\nrecognition system performance. We report DNN classifier performance and final\\nspeech recognizer word error rates, and compare DNNs using several metrics to\\nquantify factors influencing differences in task performance. Our first set of\\nexperiments use the standard Switchboard benchmark corpus, which contains\\napproximately 300 hours of conversational telephone speech. We compare standard\\nDNNs to convolutional networks, and present the first experiments using\\nlocally-connected, untied neural networks for acoustic modeling. We\\nadditionally build systems on a corpus of 2,100 hours of training data by\\ncombining the Switchboard and Fisher corpora. This larger corpus allows us to\\nmore thoroughly examine performance of large DNN models -- with up to ten times\\nmore parameters than those typically used in speech recognition systems. Our\\nresults suggest that a relatively simple DNN architecture and optimization\\ntechnique produces strong results. These findings, along with previous work,\\nhelp establish a set of best practices for building DNN hybrid speech\\nrecognition systems with maximum likelihood training. Our experiments in DNN\\noptimization additionally serve as a case study for training DNNs with\\ndiscriminative loss functions for speech tasks, as well as DNN classifiers more\\ngenerally.\\n',\n",
              " '  In this paper, we tackle the problem of the translation of proper names. We\\nintroduce our hypothesis according to which proper names can be translated more\\noften than most people seem to think. Then, we describe the construction of a\\nparallel multilingual corpus used to illustrate our point. We eventually\\nevaluate both the advantages and limits of this corpus in our study.\\n',\n",
              " '  WordRep is a benchmark collection for the research on learning distributed\\nword representations (or word embeddings), released by Microsoft Research. In\\nthis paper, we describe the details of the WordRep collection and show how to\\nuse it in different types of machine learning research related to word\\nembedding. Specifically, we describe how the evaluation tasks in WordRep are\\nselected, how the data are sampled, and how the evaluation tool is built. We\\nthen compare several state-of-the-art word representations on WordRep, report\\ntheir evaluation performance, and make discussions on the results. After that,\\nwe discuss new potential research topics that can be supported by WordRep, in\\naddition to algorithm comparison. We hope that this paper can help people gain\\ndeeper understanding of WordRep, and enable more interesting research on\\nlearning distributed word representations and related topics.\\n',\n",
              " '  Neural network techniques are widely applied to obtain high-quality\\ndistributed representations of words, i.e., word embeddings, to address text\\nmining, information retrieval, and natural language processing tasks. Recently,\\nefficient methods have been proposed to learn word embeddings from context that\\ncaptures both semantic and syntactic relationships between words. However, it\\nis challenging to handle unseen words or rare words with insufficient context.\\nIn this paper, inspired by the study on word recognition process in cognitive\\npsychology, we propose to take advantage of seemingly less obvious but\\nessentially important morphological knowledge to address these challenges. In\\nparticular, we introduce a novel neural network architecture called KNET that\\nleverages both contextual information and morphological word similarity built\\nbased on morphological knowledge to learn word embeddings. Meanwhile, the\\nlearning architecture is also able to refine the pre-defined morphological\\nknowledge and obtain more accurate word similarity. Experiments on an\\nanalogical reasoning task and a word similarity task both demonstrate that the\\nproposed KNET framework can greatly enhance the effectiveness of word\\nembeddings.\\n',\n",
              " \"  This paper presents an overview of `Lexpresso', a Controlled Natural Language\\ndeveloped at the Defence Science & Technology Organisation as a bidirectional\\nnatural language interface to a high-level information fusion system. The paper\\ndescribes Lexpresso's main features including lexical coverage, expressiveness\\nand range of linguistic syntactic and semantic structures. It also touches on\\nits tight integration with a formal semantic formalism and tentatively\\nclassifies it against the PENS system.\\n\",\n",
              " '  An inter-rater agreement study is performed for readability assessment in\\nBengali. A 1-7 rating scale was used to indicate different levels of\\nreadability. We obtained moderate to fair agreement among seven independent\\nannotators on 30 text passages written by four eminent Bengali authors. As a by\\nproduct of our study, we obtained a readability-annotated ground truth dataset\\nin Bengali. .\\n',\n",
              " '  Machine translation is the process of translating text from one language to\\nanother. In this paper, Statistical Machine Translation is done on Assamese and\\nEnglish language by taking their respective parallel corpus. A statistical\\nphrase based translation toolkit Moses is used here. To develop the language\\nmodel and to align the words we used two another tools IRSTLM, GIZA\\nrespectively. BLEU score is used to check our translation system performance,\\nhow good it is. A difference in BLEU scores is obtained while translating\\nsentences from Assamese to English and vice-versa. Since Indian languages are\\nmorphologically very rich hence translation is relatively harder from English\\nto Assamese resulting in a low BLEU score. A statistical transliteration system\\nis also introduced with our translation system to deal basically with proper\\nnouns, OOV (out of vocabulary) words which are not present in our corpus.\\n',\n",
              " '  Machine Translation is the challenging problem for Indian languages. Every\\nday we can see some machine translators being developed, but getting a high\\nquality automatic translation is still a very distant dream . The correct\\ntranslated sentence for Hindi language is rarely found. In this paper, we are\\nemphasizing on English-Hindi language pair, so in order to preserve the correct\\nMT output we present a ranking system, which employs some machine learning\\ntechniques and morphological features. In ranking no human intervention is\\nrequired. We have also validated our results by comparing it with human\\nranking.\\n',\n",
              " '  Named Entity Recognition is always important when dealing with major Natural\\nLanguage Processing tasks such as information extraction, question-answering,\\nmachine translation, document summarization etc so in this paper we put forward\\na survey of Named Entities in Indian Languages with particular reference to\\nAssamese. There are various rule-based and machine learning approaches\\navailable for Named Entity Recognition. At the very first of the paper we give\\nan idea of the available approaches for Named Entity Recognition and then we\\ndiscuss about the related research in this field. Assamese like other Indian\\nlanguages is agglutinative and suffers from lack of appropriate resources as\\nNamed Entity Recognition requires large data sets, gazetteer list, dictionary\\netc and some useful feature like capitalization as found in English cannot be\\nfound in Assamese. Apart from this we also describe some of the issues faced in\\nAssamese while doing Named Entity Recognition.\\n',\n",
              " '  In this paper we present a fundamental lexical semantics of Sinhala language\\nand a Hidden Markov Model (HMM) based Part of Speech (POS) Tagger for Sinhala\\nlanguage. In any Natural Language processing task, Part of Speech is a very\\nvital topic, which involves analysing of the construction, behaviour and the\\ndynamics of the language, which the knowledge could utilized in computational\\nlinguistics analysis and automation applications. Though Sinhala is a\\nmorphologically rich and agglutinative language, in which words are inflected\\nwith various grammatical features, tagging is very essential for further\\nanalysis of the language. Our research is based on statistical based approach,\\nin which the tagging process is done by computing the tag sequence probability\\nand the word-likelihood probability from the given corpus, where the linguistic\\nknowledge is automatically extracted from the annotated corpus. The current\\ntagger could reach more than 90% of accuracy for known words.\\n',\n",
              " '  In this paper we analyse the selectivity measure calculated from the complex\\nnetwork in the task of the automatic keyword extraction. Texts, collected from\\ndifferent web sources (portals, forums), are represented as directed and\\nweighted co-occurrence complex networks of words. Words are nodes and links are\\nestablished between two nodes if they are directly co-occurring within the\\nsentence. We test different centrality measures for ranking nodes - keyword\\ncandidates. The promising results are achieved using the selectivity measure.\\nThen we propose an approach which enables extracting word pairs according to\\nthe values of the in/out selectivity and weight measures combined with\\nfiltering.\\n',\n",
              " '  Named Entity Disambiaguation (NED) is a central task for applications dealing\\nwith natural language text. Assume that we have a graph based knowledge base\\n(subsequently referred as Knowledge Graph) where nodes represent various real\\nworld entities such as people, location, organization and concepts. Given data\\nsources such as social media streams and web pages Entity Linking is the task\\nof mapping named entities that are extracted from the data to those present in\\nthe Knowledge Graph. This is an inherently difficult task due to several\\nreasons. Almost all these data sources are generated without any formal\\nontology; the unstructured nature of the input, limited context and the\\nambiguity involved when multiple entities are mapped to the same name make this\\na hard task. This report looks at two state of the art systems employing two\\ndistinctive approaches: graph based Accurate Online Disambiguation of Entities\\n(AIDA) and Mined Evidence Named Entity Disambiguation (MENED), which employs a\\nstatistical inference approach. We compare both approaches using the data set\\nand queries provided by the Knowledge Base Population (KBP) track at 2011 NIST\\nText Analytics Conference (TAC). This report begins with an overview of the\\nrespective approaches, followed by detailed description of the experimental\\nsetup. It concludes with our findings from the benchmarking exercise.\\n',\n",
              " '  Preliminary report on network based keyword extraction for Croatian is an\\nunsupervised method for keyword extraction from the complex network. We build\\nour approach with a new network measure the node selectivity, motivated by the\\nresearch of the graph based centrality approaches. The node selectivity is\\ndefined as the average weight distribution on the links of the single node. We\\nextract nodes (keyword candidates) based on the selectivity value. Furthermore,\\nwe expand extracted nodes to word-tuples ranked with the highest in/out\\nselectivity values. Selectivity based extraction does not require linguistic\\nknowledge while it is purely derived from statistical and structural\\ninformation en-compassed in the source text which is reflected into the\\nstructure of the network. Obtained sets are evaluated on a manually annotated\\nkeywords: for the set of extracted keyword candidates average F1 score is\\n24,63%, and average F2 score is 21,19%; for the exacted words-tuples candidates\\naverage F1 score is 25,9% and average F2 score is 24,47%.\\n',\n",
              " '  We model and compute the probability distribution of the letters in random\\ngenerated words in a language by using the theory of set partitions, Young\\ntableaux and graph theoretical representation methods. This has been of\\ninterest for several application areas such as network systems, bioinformatics,\\ninternet search, data mining and computacional linguistics.\\n',\n",
              " '  We describe our ongoing research that centres on the application of natural\\nlanguage processing (NLP) to software engineering and systems development\\nactivities. In particular, this paper addresses the use of NLP in the\\nrequirements analysis and systems design processes. We have developed a\\nprototype toolset that can assist the systems analyst or software engineer to\\nselect and verify terms relevant to a project. In this paper we describe the\\nprocesses employed by the system to extract and classify objects of interest\\nfrom requirements documents. These processes are illustrated using a small\\nexample.\\n',\n",
              " '  We analyze a word embedding method in supervised tasks. It maps words on a\\nsphere such that words co-occurring in similar contexts lie closely. The\\nsimilarity of contexts is measured by the distribution of substitutes that can\\nfill them. We compared word embeddings, including more recent representations,\\nin Named Entity Recognition (NER), Chunking, and Dependency Parsing. We examine\\nour framework in multilingual dependency parsing as well. The results show that\\nthe proposed method achieves as good as or better results compared to the other\\nword embeddings in the tasks we investigate. It achieves state-of-the-art\\nresults in multilingual dependency parsing. Word embeddings in 7 languages are\\navailable for public use.\\n',\n",
              " '  In context of document classification, where in a corpus of documents their\\nlabel tags are readily known, an opportunity lies in utilizing label\\ninformation to learn document representation spaces with better discriminative\\nproperties. To this end, in this paper application of a Variational Bayesian\\nSupervised Nonnegative Matrix Factorization (supervised vbNMF) with\\nlabel-driven sparsity structure of coefficients is proposed for learning of\\ndiscriminative nonsubtractive latent semantic components occuring in TF-IDF\\ndocument representations. Constraints are such that the components pursued are\\nmade to be frequently occuring in a small set of labels only, making it\\npossible to yield document representations with distinctive label-specific\\nsparse activation patterns. A simple measure of quality of this kind of\\nsparsity structure, dubbed inter-label sparsity, is introduced and\\nexperimentally brought into tight connection with classification performance.\\nRepresenting a great practical convenience, inter-label sparsity is shown to be\\neasily controlled in supervised vbNMF by a single parameter.\\n',\n",
              " \"  We propose an approach to Longobardi's parametric comparison method (PCM) via\\nthe theory of error-correcting codes. One associates to a collection of\\nlanguages to be analyzed with the PCM a binary (or ternary) code with one code\\nwords for each language in the family and each word consisting of the binary\\nvalues of the syntactic parameters of the language, with the ternary case\\nallowing for an additional parameter state that takes into account phenomena of\\nentailment of parameters. The code parameters of the resulting code can be\\ncompared with some classical bounds in coding theory: the asymptotic bound, the\\nGilbert-Varshamov bound, etc. The position of the code parameters with respect\\nto some of these bounds provides quantitative information on the variability of\\nsyntactic parameters within and across historical-linguistic families. While\\ncomputations carried out for languages belonging to the same family yield codes\\nbelow the GV curve, comparisons across different historical families can give\\nexamples of isolated codes lying above the asymptotic bound.\\n\",\n",
              " '  Previous attempts at RST-style discourse segmentation typically adopt\\nfeatures centered on a single token to predict whether to insert a boundary\\nbefore that token. In contrast, we develop a discourse segmenter utilizing a\\nset of pairing features, which are centered on a pair of adjacent tokens in the\\nsentence, by equally taking into account the information from both tokens.\\nMoreover, we propose a novel set of global features, which encode\\ncharacteristics of the segmentation as a whole, once we have an initial\\nsegmentation. We show that both the pairing and global features are useful on\\ntheir own, and their combination achieved an $F_1$ of 92.6% of identifying\\nin-sentence discourse boundaries, which is a 17.8% error-rate reduction over\\nthe state-of-the-art performance, approaching 95% of human performance. In\\naddition, similar improvement is observed across different classification\\nframeworks.\\n',\n",
              " '  In this paper, we describe the architecture of a web-based predictive text\\neditor being developed for the controlled natural language PENG$^{ASP)$. This\\ncontrolled language can be used to write non-monotonic specifications that have\\nthe same expressive power as Answer Set Programs. In order to support the\\nwriting process of these specifications, the predictive text editor\\ncommunicates asynchronously with the controlled natural language processor that\\ngenerates lookahead categories and additional auxiliary information for the\\nauthor of a specification text. The text editor can display multiple sets of\\nlookahead categories simultaneously for different possible sentence\\ncompletions, anaphoric expressions, and supports the addition of new content\\nwords to the lexicon.\\n',\n",
              " '  We present a novel approach for recognizing what we call targetable named\\nentities; that is, named entities in a targeted set (e.g, movies, books, TV\\nshows). Unlike many other NER systems that need to retrain their statistical\\nmodels as new entities arrive, our approach does not require such retraining,\\nwhich makes it more adaptable for types of entities that are frequently\\nupdated. For this preliminary study, we focus on one entity type, movie title,\\nusing data collected from Twitter. Our system is tested on two evaluation sets,\\none including only entities corresponding to movies in our training set, and\\nthe other excluding any of those entities. Our final model shows F1-scores of\\n76.19% and 78.70% on these evaluation sets, which gives strong evidence that\\nour approach is completely unbiased to any par- ticular set of entities found\\nduring training.\\n',\n",
              " '  MindMapping is a well-known technique used in note taking, which encourages\\nlearning and studying. MindMapping has been manually adopted to help present\\nknowledge and concepts in a visual form. Unfortunately, there is no reliable\\nautomated approach to generate MindMaps from Natural Language text. This work\\nfirstly introduces MindMap Multilevel Visualization concept which is to jointly\\nvisualize and summarize textual information. The visualization is achieved\\npictorially across multiple levels using semantic information (i.e. ontology),\\nwhile the summarization is achieved by the information in the highest levels as\\nthey represent abstract information in the text. This work also presents the\\nfirst automated approach that takes a text input and generates a MindMap\\nvisualization out of it. The approach could visualize text documents in\\nmultilevel MindMaps, in which a high-level MindMap node could be expanded into\\nchild MindMaps. \\\\ignore{ As far as we know, this is the first work that view\\nMindMapping as a new approach to jointly summarize and visualize textual\\ninformation.} The proposed method involves understanding of the input text and\\nconverting it into intermediate Detailed Meaning Representation (DMR). The DMR\\nis then visualized with two modes; Single level or Multiple levels, which is\\nconvenient for larger text. The generated MindMaps from both approaches were\\nevaluated based on Human Subject experiments performed on Amazon Mechanical\\nTurk with various parameter settings.\\n',\n",
              " '  Comment on \"Approaching human language with complex networks\" by Cong & Liu\\n',\n",
              " \"  Identifying concepts and relationships in biomedical text enables knowledge\\nto be applied in computational analyses. Many biological natural language\\nprocess (BioNLP) projects attempt to address this challenge, but the state of\\nthe art in BioNLP still leaves much room for improvement. Progress in BioNLP\\nresearch depends on large, annotated corpora for evaluating information\\nextraction systems and training machine learning models. Traditionally, such\\ncorpora are created by small numbers of expert annotators often working over\\nextended periods of time. Recent studies have shown that workers on microtask\\ncrowdsourcing platforms such as Amazon's Mechanical Turk (AMT) can, in\\naggregate, generate high-quality annotations of biomedical text. Here, we\\ninvestigated the use of the AMT in capturing disease mentions in PubMed\\nabstracts. We used the NCBI Disease corpus as a gold standard for refining and\\nbenchmarking our crowdsourcing protocol. After several iterations, we arrived\\nat a protocol that reproduced the annotations of the 593 documents in the\\ntraining set of this gold standard with an overall F measure of 0.872\\n(precision 0.862, recall 0.883). The output can also be tuned to optimize for\\nprecision (max = 0.984 when recall = 0.269) or recall (max = 0.980 when\\nprecision = 0.436). Each document was examined by 15 workers, and their\\nannotations were merged based on a simple voting method. In total 145 workers\\ncombined to complete all 593 documents in the span of 1 week at a cost of $.06\\nper abstract per worker. The quality of the annotations, as judged with the F\\nmeasure, increases with the number of workers assigned to each task such that\\nthe system can be tuned to balance cost against quality. These results\\ndemonstrate that microtask crowdsourcing can be a valuable tool for generating\\nwell-annotated corpora in BioNLP.\\n\",\n",
              " '  Linguistic norms emerge in human communities because people imitate each\\nother. A shared linguistic system provides people with the benefits of shared\\nknowledge and coordinated planning. Once norms are in place, why would they\\never change? This question, echoing broad questions in the theory of social\\ndynamics, has particular force in relation to language. By definition, an\\ninnovator is in the minority when the innovation first occurs. In some areas of\\nsocial dynamics, important minorities can strongly influence the majority\\nthrough their power, fame, or use of broadcast media. But most linguistic\\nchanges are grassroots developments that originate with ordinary people. Here,\\nwe develop a novel model of communicative behavior in communities, and identify\\na mechanism for arbitrary innovations by ordinary people to have a good chance\\nof being widely adopted.\\n  To imitate each other, people must form a mental representation of what other\\npeople do. Each time they speak, they must also decide which form to produce\\nthemselves. We introduce a new decision function that enables us to smoothly\\nexplore the space between two types of behavior: probability matching (matching\\nthe probabilities of incoming experience) and regularization (producing some\\nforms disproportionately often). Using Monte Carlo methods, we explore the\\ninteractions amongst the degree of regularization, the distribution of biases\\nin a network, and the network position of the innovator. We identify two\\nregimes for the widespread adoption of arbritrary innovations, viewed as\\ninformational cascades in the network. With moderate regularization of\\nexperienced input, average people (not well-connected people) are the most\\nlikely source of successful innovations. Our results shed light on a major\\noutstanding puzzle in the theory of language change. The framework also holds\\npromise for understanding the dynamics of other social norms.\\n',\n",
              " '  In this paper, we describe the problem of cognate identification and its\\nrelation to phylogenetic inference. We introduce subsequence based features for\\ndiscriminating cognates from non-cognates. We show that subsequence based\\nfeatures perform better than the state-of-the-art string similarity measures\\nfor the purpose of cognate identification. We use the cognate judgments for the\\npurpose of phylogenetic inference and observe that these classifiers infer a\\ntree which is close to the gold standard tree. The contribution of this paper\\nis the use of subsequence features for cognate identification and to employ the\\ncognate judgments for phylogenetic inference.\\n',\n",
              " '  Most controlled natural languages (CNLs) are processed with the help of a\\npipeline architecture that relies on different software components. We\\ninvestigate in this paper in an experimental way how well answer set\\nprogramming (ASP) is suited as a unifying framework for parsing a CNL, deriving\\na formal representation for the resulting syntax trees, and for reasoning with\\nthat representation. We start from a list of input tokens in ASP notation and\\nshow how this input can be transformed into a syntax tree using an ASP grammar\\nand then into reified ASP rules in form of a set of facts. These facts are then\\nprocessed by an ASP meta-interpreter that allows us to infer new knowledge.\\n',\n",
              " '  We present a method to perform first-pass large vocabulary continuous speech\\nrecognition using only a neural network and language model. Deep neural network\\nacoustic models are now commonplace in HMM-based speech recognition systems,\\nbut building such systems is a complex, domain-specific task. Recent work\\ndemonstrated the feasibility of discarding the HMM sequence modeling framework\\nby directly predicting transcript text from audio. This paper extends this\\napproach in two ways. First, we demonstrate that a straightforward recurrent\\nneural network architecture can achieve a high level of accuracy. Second, we\\npropose and evaluate a modified prefix-search decoding algorithm. This approach\\nto decoding enables first-pass speech recognition with a language model,\\ncompletely unaided by the cumbersome infrastructure of HMM-based systems.\\nExperiments on the Wall Street Journal corpus demonstrate fairly competitive\\nword error rates, and the importance of bi-directional network recurrence.\\n',\n",
              " '  Real-word spelling correction differs from non-word spelling correction in\\nits aims and its challenges. Here we show that the central problem in real-word\\nspelling correction is detection. Methods from non-word spelling correction,\\nwhich focus instead on selection among candidate corrections, do not address\\ndetection adequately, because detection is either assumed in advance or heavily\\nconstrained. As we demonstrate in this paper, merely discriminating between the\\nintended word and a random close variation of it within the context of a\\nsentence is a task that can be performed with high accuracy using\\nstraightforward models. Trigram models are sufficient in almost all cases. The\\ndifficulty comes when every word in the sentence is a potential error, with a\\nlarge set of possible candidate corrections. Despite their strengths, trigram\\nmodels cannot reliably find true errors without introducing many more, at least\\nnot when used in the obvious sequential way without added structure. The\\ndetection task exposes weakness not visible in the selection task.\\n',\n",
              " '  We present SimLex-999, a gold standard resource for evaluating distributional\\nsemantic models that improves on existing resources in several important ways.\\nFirst, in contrast to gold standards such as WordSim-353 and MEN, it explicitly\\nquantifies similarity rather than association or relatedness, so that pairs of\\nentities that are associated but not actually similar [Freud, psychology] have\\na low rating. We show that, via this focus on similarity, SimLex-999\\nincentivizes the development of models with a different, and arguably wider\\nrange of applications than those which reflect conceptual association. Second,\\nSimLex-999 contains a range of concrete and abstract adjective, noun and verb\\npairs, together with an independent rating of concreteness and (free)\\nassociation strength for each pair. This diversity enables fine-grained\\nanalyses of the performance of models on concepts of different types, and\\nconsequently greater insight into how architectures can be improved. Further,\\nunlike existing gold standard evaluations, for which automatic approaches have\\nreached or surpassed the inter-annotator agreement ceiling, state-of-the-art\\nmodels perform well below this ceiling on SimLex-999. There is therefore plenty\\nof scope for SimLex-999 to quantify future improvements to distributional\\nsemantic models, guiding the development of the next generation of\\nrepresentation-learning architectures.\\n',\n",
              " '  In this work, we present an application of the recently proposed unsupervised\\nkeyword extraction algorithm RAKE to a corpus of Polish legal texts from the\\nfield of public procurement. RAKE is essentially a language and domain\\nindependent method. Its only language-specific input is a stoplist containing a\\nset of non-content words. The performance of the method heavily depends on the\\nchoice of such a stoplist, which should be domain adopted. Therefore, we\\ncomplement RAKE algorithm with an automatic approach to selecting non-content\\nwords, which is based on the statistical properties of term distribution.\\n',\n",
              " '  The use of short text messages in social media and instant messaging has\\nbecome a popular communication channel during the last years. This rising\\npopularity has caused an increment in messaging threats such as spam, phishing\\nor malware as well as other threats. The processing of these short text message\\nthreats could pose additional challenges such as the presence of lexical\\nvariants, SMS-like contractions or advanced obfuscations which can degrade the\\nperformance of traditional filtering solutions. By using a real-world SMS data\\nset from a large telecommunications operator from the US and a social media\\ncorpus, in this paper we analyze the effectiveness of machine learning filters\\nbased on linguistic and behavioral patterns in order to detect short text spam\\nand abusive users in the network. We have also explored different ways to deal\\nwith short text message challenges such as tokenization and entity detection by\\nusing text normalization and substring clustering techniques. The obtained\\nresults show the validity of the proposed solution by enhancing baseline\\napproaches.\\n',\n",
              " '  Ferrer-i-Cancho (2015) presents a mathematical model of both the synchronic\\nand diachronic nature of word order based on the assumption that memory costs\\nare a never decreasing function of distance and a few very general linguistic\\nassumptions. However, even these minimal and seemingly obvious assumptions are\\nnot as safe as they appear in light of recent typological and psycholinguistic\\nevidence. The interaction of word order and memory has further depths to be\\nexplored.\\n',\n",
              " '  We report on a series of experiments with convolutional neural networks (CNN)\\ntrained on top of pre-trained word vectors for sentence-level classification\\ntasks. We show that a simple CNN with little hyperparameter tuning and static\\nvectors achieves excellent results on multiple benchmarks. Learning\\ntask-specific vectors through fine-tuning offers further gains in performance.\\nWe additionally propose a simple modification to the architecture to allow for\\nthe use of both task-specific and static vectors. The CNN models discussed\\nherein improve upon the state of the art on 4 out of 7 tasks, which include\\nsentiment analysis and question classification.\\n',\n",
              " '  We provide a comparative study between neural word representations and\\ntraditional vector spaces based on co-occurrence counts, in a number of\\ncompositional tasks. We use three different semantic spaces and implement seven\\ntensor-based compositional models, which we then test (together with simpler\\nadditive and multiplicative approaches) in tasks involving verb disambiguation\\nand sentence similarity. To check their scalability, we additionally evaluate\\nthe spaces using simple compositional methods on larger-scale tasks with less\\nconstrained language: paraphrase detection and dialogue act tagging. In the\\nmore constrained tasks, co-occurrence vectors are competitive, although choice\\nof compositional method is important; on the larger-scale tasks, they are\\noutperformed by neural word embeddings, which show robust, stable performance\\nacross the tasks.\\n',\n",
              " '  This paper provides a method for improving tensor-based compositional\\ndistributional models of meaning by the addition of an explicit disambiguation\\nstep prior to composition. In contrast with previous research where this\\nhypothesis has been successfully tested against relatively simple compositional\\nmodels, in our work we use a robust model trained with linear regression. The\\nresults we get in two experiments show the superiority of the prior\\ndisambiguation method and suggest that the effectiveness of this approach is\\nmodel-independent.\\n',\n",
              " '  This paper presents categorization of Croatian texts using Non-Standard Words\\n(NSW) as features. Non-Standard Words are: numbers, dates, acronyms,\\nabbreviations, currency, etc. NSWs in Croatian language are determined\\naccording to Croatian NSW taxonomy. For the purpose of this research, 390 text\\ndocuments were collected and formed the SKIPEZ collection with 6 classes:\\nofficial, literary, informative, popular, educational and scientific. Text\\ncategorization experiment was conducted on three different representations of\\nthe SKIPEZ collection: in the first representation, the frequencies of NSWs are\\nused as features; in the second representation, the statistic measures of NSWs\\n(variance, coefficient of variation, standard deviation, etc.) are used as\\nfeatures; while the third representation combines the first two feature sets.\\nNaive Bayes, CN2, C4.5, kNN, Classification Trees and Random Forest algorithms\\nwere used in text categorization experiments. The best categorization results\\nare achieved using the first feature set (NSW frequencies) with the\\ncategorization accuracy of 87%. This suggests that the NSWs should be\\nconsidered as features in highly inflectional languages, such as Croatian. NSW\\nbased features reduce the dimensionality of the feature space without standard\\nlemmatization procedures, and therefore the bag-of-NSWs should be considered\\nfor further Croatian texts categorization experiments.\\n',\n",
              " '  We present STIR (STrongly Incremental Repair detection), a system that\\ndetects speech repairs and edit terms on transcripts incrementally with minimal\\nlatency. STIR uses information-theoretic measures from n-gram models as its\\nprincipal decision features in a pipeline of classifiers detecting the\\ndifferent stages of repairs. Results on the Switchboard disfluency tagged\\ncorpus show utterance-final accuracy on a par with state-of-the-art incremental\\nrepair detection methods, but with better incremental accuracy, faster\\ntime-to-detection and less computational overhead. We evaluate its performance\\nusing incremental metrics and propose new repair processing evaluation\\nstandards.\\n',\n",
              " \"  In this empirical study, I compare various tree distance measures --\\noriginally developed in computational biology for the purpose of tree\\ncomparison -- for the purpose of parser evaluation. I will control for the\\nparser setting by comparing the automatically generated parse trees from the\\nstate-of-the-art parser Charniak, 2000) with the gold-standard parse trees. The\\narticle describes two different tree distance measures (RF and QD) along with\\nits variants (GRF and GQD) for the purpose of parser evaluation. The article\\nwill argue that RF measure captures similar information as the standard EvalB\\nmetric (Sekine and Collins, 1997) and the tree edit distance (Zhang and Shasha,\\n1989) applied by Tsarfaty et al. (2011). Finally, the article also provides\\nempirical evidence by reporting high correlations between the different tree\\ndistances and EvalB metric's scores.\\n\",\n",
              " '  Neural machine translation is a recently proposed approach to machine\\ntranslation. Unlike the traditional statistical machine translation, the neural\\nmachine translation aims at building a single neural network that can be\\njointly tuned to maximize the translation performance. The models proposed\\nrecently for neural machine translation often belong to a family of\\nencoder-decoders and consists of an encoder that encodes a source sentence into\\na fixed-length vector from which a decoder generates a translation. In this\\npaper, we conjecture that the use of a fixed-length vector is a bottleneck in\\nimproving the performance of this basic encoder-decoder architecture, and\\npropose to extend this by allowing a model to automatically (soft-)search for\\nparts of a source sentence that are relevant to predicting a target word,\\nwithout having to form these parts as a hard segment explicitly. With this new\\napproach, we achieve a translation performance comparable to the existing\\nstate-of-the-art phrase-based system on the task of English-to-French\\ntranslation. Furthermore, qualitative analysis reveals that the\\n(soft-)alignments found by the model agree well with our intuition.\\n',\n",
              " '  The authors of (Cho et al., 2014a) have shown that the recently introduced\\nneural network translation systems suffer from a significant drop in\\ntranslation quality when translating long sentences, unlike existing\\nphrase-based translation systems. In this paper, we propose a way to address\\nthis issue by automatically segmenting an input sentence into phrases that can\\nbe easily translated by the neural network translation model. Once each segment\\nhas been independently translated by the neural machine translation model, the\\ntranslated clauses are concatenated to form a final translation. Empirical\\nresults show a significant improvement in translation quality for long\\nsentences.\\n',\n",
              " '  Neural machine translation is a relatively new approach to statistical\\nmachine translation based purely on neural networks. The neural machine\\ntranslation models often consist of an encoder and a decoder. The encoder\\nextracts a fixed-length representation from a variable-length input sentence,\\nand the decoder generates a correct translation from this representation. In\\nthis paper, we focus on analyzing the properties of the neural machine\\ntranslation using two models; RNN Encoder--Decoder and a newly proposed gated\\nrecursive convolutional neural network. We show that the neural machine\\ntranslation performs relatively well on short sentences without unknown words,\\nbut its performance degrades rapidly as the length of the sentence and the\\nnumber of unknown words increase. Furthermore, we find that the proposed gated\\nrecursive convolutional network learns a grammatical structure of a sentence\\nautomatically.\\n',\n",
              " '  The paper deals with word sense induction from lexical co-occurrence graphs.\\nWe construct such graphs on large Russian corpora and then apply this data to\\ncluster Mail.ru Search results according to meanings of the query. We compare\\ndifferent methods of performing such clustering and different source corpora.\\nModels of applying distributional semantics to big linguistic data are\\ndescribed.\\n',\n",
              " \"  This report describes an NLP assistant for the collaborative development\\nenvironment Clide, that supports the development of NLP applications by\\nproviding easy access to some common NLP data structures. The assistant\\nvisualizes text fragments and their dependencies by displaying the semantic\\ngraph of a sentence, the coreference chain of a paragraph and mined triples\\nthat are extracted from a paragraph's semantic graphs and linked using its\\ncoreference chain. Using this information and a logic programming library, we\\ncreate an NLP database which is used by a series of queries to mine the\\ntriples. The algorithm is tested by translating a natural language text\\ndescribing a graph to an actual graph that is shown as an annotation in the\\ntext editor.\\n\",\n",
              " '  We investigate the predictive power behind the language of food on social\\nmedia. We collect a corpus of over three million food-related posts from\\nTwitter and demonstrate that many latent population characteristics can be\\ndirectly predicted from this data: overweight rate, diabetes rate, political\\nleaning, and home geographical location of authors. For all tasks, our\\nlanguage-based models significantly outperform the majority-class baselines.\\nPerformance is further improved with more complex natural language processing,\\nsuch as topic modeling. We analyze which textual features have most predictive\\npower for these datasets, providing insight into the connections between the\\nlanguage of food, geographic locale, and community characteristics. Lastly, we\\ndesign and implement an online system for real-time query and visualization of\\nthe dataset. Visualization tools, such as geo-referenced heatmaps,\\nsemantics-preserving wordclouds and temporal histograms, allow us to discover\\nmore complex, global patterns mirrored in the language of food.\\n',\n",
              " '  We study the complexity of approximating solution structure of the bijective\\nweighted sentence alignment problem of DeNero and Klein (2008). In particular,\\nwe consider the complexity of finding an alignment that has a significant\\noverlap with an optimal alignment. We discuss ways of representing the solution\\nfor the general weighted sentence alignment as well as phrases-to-words\\nalignment problem, and show that computing a string which agrees with the\\noptimal sentence partition on more than half (plus an arbitrarily small\\npolynomial fraction) positions for the phrases-to-words alignment is NP-hard.\\nFor the general weighted sentence alignment we obtain such bound from the\\nagreement on a little over 2/3 of the bits. Additionally, we generalize the\\nHamming distance approximation of a solution structure to approximating it with\\nrespect to the edit distance metric, obtaining similar lower bounds.\\n',\n",
              " '  Automatic Multi-Word Term (MWT) extraction is a very important issue to many\\napplications, such as information retrieval, question answering, and text\\ncategorization. Although many methods have been used for MWT extraction in\\nEnglish and other European languages, few studies have been applied to Arabic.\\nIn this paper, we propose a novel, hybrid method which combines linguistic and\\nstatistical approaches for Arabic Multi-Word Term extraction. The main\\ncontribution of our method is to consider contextual information and both\\ntermhood and unithood for association measures at the statistical filtering\\nstep. In addition, our technique takes into account the problem of MWT\\nvariation in the linguistic filtering step. The performance of the proposed\\nstatistical measure (NLC-value) is evaluated using an Arabic environment corpus\\nby comparing it with some existing competitors. Experimental results show that\\nour NLC-value measure outperforms the other ones in term of precision for both\\nbi-grams and tri-grams.\\n',\n",
              " \"  Deep Neural Networks (DNNs) are powerful models that have achieved excellent\\nperformance on difficult learning tasks. Although DNNs work well whenever large\\nlabeled training sets are available, they cannot be used to map sequences to\\nsequences. In this paper, we present a general end-to-end approach to sequence\\nlearning that makes minimal assumptions on the sequence structure. Our method\\nuses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to\\na vector of a fixed dimensionality, and then another deep LSTM to decode the\\ntarget sequence from the vector. Our main result is that on an English to\\nFrench translation task from the WMT'14 dataset, the translations produced by\\nthe LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM's\\nBLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did\\nnot have difficulty on long sentences. For comparison, a phrase-based SMT\\nsystem achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM\\nto rerank the 1000 hypotheses produced by the aforementioned SMT system, its\\nBLEU score increases to 36.5, which is close to the previous best result on\\nthis task. The LSTM also learned sensible phrase and sentence representations\\nthat are sensitive to word order and are relatively invariant to the active and\\nthe passive voice. Finally, we found that reversing the order of the words in\\nall source sentences (but not target sentences) improved the LSTM's performance\\nmarkedly, because doing so introduced many short term dependencies between the\\nsource and the target sentence which made the optimization problem easier.\\n\",\n",
              " '  This paper presents a new model of WordNet that is used to disambiguate the\\ncorrect sense of polysemy word based on the clue words. The related words for\\neach sense of a polysemy word as well as single sense word are referred to as\\nthe clue words. The conventional WordNet organizes nouns, verbs, adjectives and\\nadverbs together into sets of synonyms called synsets each expressing a\\ndifferent concept. In contrast to the structure of WordNet, we developed a new\\nmodel of WordNet that organizes the different senses of polysemy words as well\\nas the single sense words based on the clue words. These clue words for each\\nsense of a polysemy word as well as for single sense word are used to\\ndisambiguate the correct meaning of the polysemy word in the given context\\nusing knowledge based Word Sense Disambiguation (WSD) algorithms. The clue word\\ncan be a noun, verb, adjective or adverb.\\n',\n",
              " '  This paper describes adaptations for EaFi, a parser for easy-first parsing of\\ndiscontinuous constituents, to adapt it to multiple languages as well as make\\nuse of the unlabeled data that was provided as part of the SPMRL shared task\\n2014.\\n',\n",
              " \"  Natural languages are full of rules and exceptions. One of the most famous\\nquantitative rules is Zipf's law which states that the frequency of occurrence\\nof a word is approximately inversely proportional to its rank. Though this\\n`law' of ranks has been found to hold across disparate texts and forms of data,\\nanalyses of increasingly large corpora over the last 15 years have revealed the\\nexistence of two scaling regimes. These regimes have thus far been explained by\\na hypothesis suggesting a separability of languages into core and non-core\\nlexica. Here, we present and defend an alternative hypothesis, that the two\\nscaling regimes result from the act of aggregating texts. We observe that text\\nmixing leads to an effective decay of word introduction, which we show provides\\naccurate predictions of the location and severity of breaks in scaling. Upon\\nexamining large corpora from 10 languages in the Project Gutenberg eBooks\\ncollection (eBooks), we find emphatic empirical support for the universality of\\nour claim.\\n\",\n",
              " '  There is a broad range of BioNLP tasks for which active learning (AL) can\\nsignificantly reduce annotation costs and a specific AL algorithm we have\\ndeveloped is particularly effective in reducing annotation costs for these\\ntasks. We have previously developed an AL algorithm called ClosestInitPA that\\nworks best with tasks that have the following characteristics: redundancy in\\ntraining material, burdensome annotation costs, Support Vector Machines (SVMs)\\nwork well for the task, and imbalanced datasets (i.e. when set up as a binary\\nclassification problem, one class is substantially rarer than the other). Many\\nBioNLP tasks have these characteristics and thus our AL algorithm is a natural\\napproach to apply to BioNLP tasks.\\n',\n",
              " '  Nowadays peoples are actively involved in giving comments and reviews on\\nsocial networking websites and other websites like shopping websites, news\\nwebsites etc. large number of people everyday share their opinion on the web,\\nresults is a large number of user data is collected .users also find it trivial\\ntask to read all the reviews and then reached into the decision. It would be\\nbetter if these reviews are classified into some category so that the user\\nfinds it easier to read. Opinion Mining or Sentiment Analysis is a natural\\nlanguage processing task that mines information from various text forms such as\\nreviews, news, and blogs and classify them on the basis of their polarity as\\npositive, negative or neutral. But, from the last few years, user content in\\nHindi language is also increasing at a rapid rate on the Web. So it is very\\nimportant to perform opinion mining in Hindi language as well. In this paper a\\nHindi language opinion mining system is proposed. The system classifies the\\nreviews as positive, negative and neutral for Hindi language. Negation is also\\nhandled in the proposed system. Experimental results using reviews of movies\\nshow the effectiveness of the system\\n',\n",
              " '  The rendering of Sanskrit poetry from text to speech is a problem that has\\nnot been solved before. One reason may be the complications in the language\\nitself. We present unique algorithms based on extensive empirical analysis, to\\nsynthesize speech from a given text input of Sanskrit verses. Using a\\npre-recorded audio units database which is itself tremendously reduced in size\\ncompared to the colossal size that would otherwise be required, the algorithms\\nwork on producing the best possible, tunefully rendered chanting of the given\\nverse. His would enable the visually impaired and those with reading\\ndisabilities to easily access the contents of Sanskrit verses otherwise\\navailable only in writing.\\n',\n",
              " '  Comprehensively searching for words in Sanskrit E-text is a non-trivial\\nproblem because words could change their forms in different contexts. One such\\ncontext is sandhi or euphonic conjunctions, which cause a word to change owing\\nto the presence of adjacent letters or words. The change wrought by these\\npossible conjunctions can be so significant in Sanskrit that a simple search\\nfor the word in its given form alone can significantly reduce the success level\\nof the search. This work presents a representational schema that represents\\nletters in a binary format and reduces Paninian rules of euphonic conjunctions\\nto simple bit set-unset operations. The work presents an efficient algorithm to\\nprocess vowel-based sandhis using this schema. It further presents another\\nalgorithm that uses the sandhi processor to generate the possible transformed\\nword forms of a given word to use in a comprehensive word search.\\n',\n",
              " '  Searching for words in Sanskrit E-text is a problem that is accompanied by\\ncomplexities introduced by features of Sanskrit such as euphonic conjunctions\\nor sandhis. A word could occur in an E-text in a transformed form owing to the\\noperation of rules of sandhi. Simple word search would not yield these\\ntransformed forms of the word. Further, there is no search engine in the\\nliterature that can comprehensively search for words in Sanskrit E-texts taking\\neuphonic conjunctions into account. This work presents an optimal binary\\nrepresentational schema for letters of the Sanskrit alphabet along with\\nalgorithms to efficiently process the sandhi rules of Sanskrit grammar. The\\nwork further presents an algorithm that uses the sandhi processing algorithm to\\nperform a comprehensive word search on E-text.\\n',\n",
              " \"  Consumers' purchase decisions are increasingly influenced by user-generated\\nonline reviews. Accordingly, there has been growing concern about the potential\\nfor posting deceptive opinion spam fictitious reviews that have been\\ndeliberately written to sound authentic, to deceive the readers. Existing\\napproaches mainly focus on developing automatic supervised learning based\\nmethods to help users identify deceptive opinion spams.\\n  This work, we used the LSI and Sprinkled LSI technique to reduce the\\ndimension for deception detection. We make our contribution to demonstrate what\\nLSI is capturing in latent semantic space and reveal how deceptive opinions can\\nbe recognized automatically from truthful opinions. Finally, we proposed a\\nvoting scheme which integrates different approaches to further improve the\\nclassification performance.\\n\",\n",
              " '  Twitter with over 500 million users globally, generates over 100,000 tweets\\nper minute . The 140 character limit per tweet, perhaps unintentionally,\\nencourages users to use shorthand notations and to strip spellings to their\\nbare minimum \"syllables\" or elisions e.g. \"srsly\". The analysis of twitter\\nmessages which typically contain misspellings, elisions, and grammatical\\nerrors, poses a challenge to established Natural Language Processing (NLP)\\ntools which are generally designed with the assumption that the data conforms\\nto the basic grammatical structure commonly used in English language. In order\\nto make sense of Twitter messages it is necessary to first transform them into\\na canonical form, consistent with the dictionary or grammar. This process,\\nperformed at the level of individual tokens (\"words\"), is called lexical\\nnormalisation. This paper investigates various techniques for lexical\\nnormalisation of Twitter data and presents the findings as the techniques are\\napplied to process raw data from Twitter.\\n',\n",
              " '  We investigate properties of evolving linguistic networks defined by the\\nword-adjacency relation. Such networks belong to the category of networks with\\naccelerated growth but their shortest path length appears to reveal the network\\nsize dependence of different functional form than the ones known so far. We\\nthus compare the networks created from literary texts with their artificial\\nsubstitutes based on different variants of the Dorogovtsev-Mendes model and\\nobserve that none of them is able to properly simulate the novel asymptotics of\\nthe shortest path length. Then, we identify the local chain-like linear growth\\ninduced by grammar and style as a missing element in this model and extend it\\nby incorporating such effects. It is in this way that a satisfactory agreement\\nwith the empirical result is obtained.\\n',\n",
              " '  A crowdsourcing translation approach is an effective tool for globalization\\nof site content, but it is also an important source of parallel linguistic\\ndata. For the given site, processed with a crowdsourcing system, a\\nsentence-aligned corpus can be fetched, which covers a very narrow domain of\\nterminology and language patterns - a site-specific domain. These data can be\\nused for training and estimation of site-specific statistical machine\\ntranslation engine\\n',\n",
              " '  We describe a unified and coherent syntactic framework for supporting a\\nsemantically-informed syntactic approach to statistical machine translation.\\nSemantically enriched syntactic tags assigned to the target-language training\\ntexts improved translation quality. The resulting system significantly\\noutperformed a linguistically naive baseline model (Hiero), and reached the\\nhighest scores yet reported on the NIST 2009 Urdu-English translation task.\\nThis finding supports the hypothesis (posed by many researchers in the MT\\ncommunity, e.g., in DARPA GALE) that both syntactic and semantic information\\nare critical for improving translation quality---and further demonstrates that\\nlarge gains can be achieved for low-resource languages with different word\\norder than English.\\n',\n",
              " \"  According to Zipf's meaning-frequency law, words that are more frequent tend\\nto have more meanings. Here it is shown that a linear dependency between the\\nfrequency of a form and its number of meanings is found in a family of models\\nof Zipf's law for word frequencies. This is evidence for a weak version of the\\nmeaning-frequency law. Interestingly, that weak law (a) is not an inevitable of\\nproperty of the assumptions of the family and (b) is found at least in the\\nnarrow regime where those models exhibit Zipf's law for word frequencies.\\n\",\n",
              " \"  In this paper, the performance of two dependency parsers, namely Stanford and\\nMinipar, on biomedical texts has been reported. The performance of te parsers\\nto assignm dependencies between two biomedical concepts that are already proved\\nto be connected is not satisfying. Both Stanford and Minipar, being statistical\\nparsers, fail to assign dependency relation between two connected concepts if\\nthey are distant by at least one clause. Minipar's performance, in terms of\\nprecision, recall and the F-score of the attachment score (e.g., correctly\\nidentified head in a dependency), to parse biomedical text is also measured\\ntaking the Stanford's as a gold standard. The results suggest that Minipar is\\nnot suitable yet to parse biomedical texts. In addition, a qualitative\\ninvestigation reveals that the difference between working principles of the\\nparsers also play a vital role for Minipar's degraded performance.\\n\",\n",
              " '  We investigate ways in which to improve the interpretability of LDA topic\\nmodels by better analyzing and visualizing their outputs. We focus on examining\\nwhat we refer to as topic similarity networks: graphs in which nodes represent\\nlatent topics in text collections and links represent similarity among topics.\\nWe describe efficient and effective approaches to both building and labeling\\nsuch networks. Visualizations of topic models based on these networks are shown\\nto be a powerful means of exploring, characterizing, and summarizing large\\ncollections of unstructured text documents. They help to \"tease out\"\\nnon-obvious connections among different sets of documents and provide insights\\ninto how topics form larger themes. We demonstrate the efficacy and\\npracticality of these approaches through two case studies: 1) NSF grants for\\nbasic research spanning a 14 year period and 2) the entire English portion of\\nWikipedia.\\n',\n",
              " '  Semi-supervised classification is an interesting idea where classification\\nmodels are learned from both labeled and unlabeled data. It has several\\nadvantages over supervised classification in natural language processing\\ndomain. For instance, supervised classification exploits only labeled data that\\nare expensive, often difficult to get, inadequate in quantity, and require\\nhuman experts for annotation. On the other hand, unlabeled data are inexpensive\\nand abundant. Despite the fact that many factors limit the wide-spread use of\\nsemi-supervised classification, it has become popular since its level of\\nperformance is empirically as good as supervised classification. This study\\nexplores the possibilities and achievements as well as complexity and\\nlimitations of semi-supervised classification for several natural langue\\nprocessing tasks like parsing, biomedical information processing, text\\nclassification, and summarization.\\n',\n",
              " '  Contemporary research on computational processing of linguistic metaphors is\\ndivided into two main branches: metaphor recognition and metaphor\\ninterpretation. We take a different line of research and present an automated\\nmethod for generating conceptual metaphors from linguistic data. Given the\\ngenerated conceptual metaphors, we find corresponding linguistic metaphors in\\ncorpora. In this paper, we describe our approach and its evaluation using\\nEnglish and Russian data.\\n',\n",
              " '  We explore the idea that authoring a piece of text is an act of maximizing\\none\\'s expected utility. To make this idea concrete, we consider the societally\\nimportant decisions of the Supreme Court of the United States. Extensive past\\nwork in quantitative political science provides a framework for empirically\\nmodeling the decisions of justices and how they relate to text. We incorporate\\ninto such a model texts authored by amici curiae (\"friends of the court\"\\nseparate from the litigants) who seek to weigh in on the decision, then\\nexplicitly model their goals in a random utility model. We demonstrate the\\nbenefits of this approach in improved vote prediction and the ability to\\nperform counterfactual analysis.\\n',\n",
              " '  This paper describes performance of CRF based systems for Named Entity\\nRecognition (NER) in Indian language as a part of ICON 2013 shared task. In\\nthis task we have considered a set of language independent features for all the\\nlanguages. Only for English a language specific feature, i.e. capitalization,\\nhas been added. Next the use of gazetteer is explored for Bengali, Hindi and\\nEnglish. The gazetteers are built from Wikipedia and other sources. Test\\nresults show that the system achieves the highest F measure of 88% for English\\nand the lowest F measure of 69% for both Tamil and Telugu. Note that for the\\nleast performing two languages no gazetteer was used. NER in Bengali and Hindi\\nfinds accuracy (F measure) of 87% and 79%, respectively.\\n',\n",
              " '  Nearly all Statistical Parametric Speech Synthesizers today use Mel Cepstral\\ncoefficients as the vocal tract parameterization of the speech signal. Mel\\nCepstral coefficients were never intended to work in a parametric speech\\nsynthesis framework, but as yet, there has been little success in creating a\\nbetter parameterization that is more suited to synthesis. In this paper, we use\\ndeep learning algorithms to investigate a data-driven parameterization\\ntechnique that is designed for the specific requirements of synthesis. We\\ncreate an invertible, low-dimensional, noise-robust encoding of the Mel Log\\nSpectrum by training a tapered Stacked Denoising Autoencoder (SDA). This SDA is\\nthen unwrapped and used as the initialization for a Multi-Layer Perceptron\\n(MLP). The MLP is fine-tuned by training it to reconstruct the input at the\\noutput layer. This MLP is then split down the middle to form encoding and\\ndecoding networks. These networks produce a parameterization of the Mel Log\\nSpectrum that is intended to better fulfill the requirements of synthesis.\\nResults are reported for experiments conducted using this resulting\\nparameterization with the ClusterGen speech synthesizer.\\n',\n",
              " '  Machine Translation is one of the major oldest and the most active research\\narea in Natural Language Processing. Currently, Statistical Machine Translation\\n(SMT) dominates the Machine Translation research. Statistical Machine\\nTranslation is an approach to Machine Translation which uses models to learn\\ntranslation patterns directly from data, and generalize them to translate a new\\nunseen text. The SMT approach is largely language independent, i.e. the models\\ncan be applied to any language pair. Statistical Machine Translation (SMT)\\nattempts to generate translations using statistical methods based on bilingual\\ntext corpora. Where such corpora are available, excellent results can be\\nattained translating similar texts, but such corpora are still not available\\nfor many language pairs. Statistical Machine Translation systems, in general,\\nhave difficulty in handling the morphology on the source or the target side\\nespecially for morphologically rich languages. Errors in morphology or syntax\\nin the target language can have severe consequences on meaning of the sentence.\\nThey change the grammatical function of words or the understanding of the\\nsentence through the incorrect tense information in verb. Baseline SMT also\\nknown as Phrase Based Statistical Machine Translation (PBSMT) system does not\\nuse any linguistic information and it only operates on surface word form.\\nRecent researches shown that adding linguistic information helps to improve the\\naccuracy of the translation with less amount of bilingual corpora. Adding\\nlinguistic information can be done using the Factored Statistical Machine\\nTranslation system through pre-processing steps. This paper investigates about\\nhow English side pre-processing is used to improve the accuracy of\\nEnglish-Tamil SMT system.\\n',\n",
              " '  The Linguistic Annotation Framework (LAF) provides a general, extensible\\nstand-off markup system for corpora. This paper discusses LAF-Fabric, a new\\ntool to analyse LAF resources in general with an extension to process the\\nHebrew Bible in particular. We first walk through the history of the Hebrew\\nBible as text database in decennium-wide steps. Then we describe how LAF-Fabric\\nmay serve as an analysis tool for this corpus. Finally, we describe three\\nanalytic projects/workflows that benefit from the new LAF representation:\\n  1) the study of linguistic variation: extract cooccurrence data of common\\nnouns between the books of the Bible (Martijn Naaijer); 2) the study of the\\ngrammar of Hebrew poetry in the Psalms: extract clause typology (Gino Kalkman);\\n3) construction of a parser of classical Hebrew by Data Oriented Parsing:\\ngenerate tree structures from the database (Andreas van Cranenburgh).\\n',\n",
              " '  We present an open source morphological analyzer for Japanese nouns, verbs\\nand adjectives. The system builds upon the morphological analyzing capabilities\\nof MeCab to incorporate finer details of classification such as politeness,\\ntense, mood and voice attributes. We implemented our analyzer in the form of a\\nfinite state transducer using the open source finite state compiler FOMA\\ntoolkit. The source code and tool is available at\\nhttps://bitbucket.org/skylander/yc-nlplab/.\\n',\n",
              " '  Neural language models learn word representations that capture rich\\nlinguistic and conceptual information. Here we investigate the embeddings\\nlearned by neural machine translation models. We show that translation-based\\nembeddings outperform those learned by cutting-edge monolingual models at\\nsingle-language tasks requiring knowledge of conceptual similarity and/or\\nsyntactic role. The findings suggest that, while monolingual models learn\\ninformation about how concepts are related, neural-translation models better\\ncapture their true ontological status.\\n',\n",
              " '  The article describes the original method of creating a dictionary of\\nabbreviations based on the Google Books Ngram Corpus. The dictionary of\\nabbreviations is designed for Russian, yet as its methodology is universal it\\ncan be applied to any language. The dictionary can be used to define the\\nfunction of the period during text segmentation in various applied systems of\\ntext processing. The article describes difficulties encountered in the process\\nof its construction as well as the ways to overcome them. A model of evaluating\\na probability of first and second type errors (extraction accuracy and\\nfullness) is constructed. Certain statistical data for the use of abbreviations\\nare provided.\\n',\n",
              " '  This paper proposes a methodology to prepare corpora in Arabic language from\\nonline social network (OSN) and review site for Sentiment Analysis (SA) task.\\nThe paper also proposes a methodology for generating a stopword list from the\\nprepared corpora. The aim of the paper is to investigate the effect of removing\\nstopwords on the SA task. The problem is that the stopwords lists generated\\nbefore were on Modern Standard Arabic (MSA) which is not the common language\\nused in OSN. We have generated a stopword list of Egyptian dialect and a\\ncorpus-based list to be used with the OSN corpora. We compare the efficiency of\\ntext classification when using the generated lists along with previously\\ngenerated lists of MSA and combining the Egyptian dialect list with the MSA\\nlist. The text classification was performed using Na\\\\\"ive Bayes and Decision\\nTree classifiers and two feature selection approaches, unigrams and bigram. The\\nexperiments show that the general lists containing the Egyptian dialects words\\ngive better performance than using lists of MSA stopwords only.\\n',\n",
              " '  This paper explores the use of machine learning approaches, or more\\nspecifically, four supervised learning Methods, namely Decision Tree(C 4.5),\\nK-Nearest Neighbour (KNN), Na\\\\\"ive Bays (NB), and Support Vector Machine (SVM)\\nfor categorization of Bangla web documents. This is a task of automatically\\nsorting a set of documents into categories from a predefined set. Whereas a\\nwide range of methods have been applied to English text categorization,\\nrelatively few studies have been conducted on Bangla language text\\ncategorization. Hence, we attempt to analyze the efficiency of those four\\nmethods for categorization of Bangla documents. In order to validate, Bangla\\ncorpus from various websites has been developed and used as examples for the\\nexperiment. For Bangla, empirical results support that all four methods produce\\nsatisfactory performance with SVM attaining good result in terms of high\\ndimensional and relatively noisy document feature vectors.\\n',\n",
              " '  Word alignment is an important natural language processing task that\\nindicates the correspondence between natural languages. Recently, unsupervised\\nlearning of log-linear models for word alignment has received considerable\\nattention as it combines the merits of generative and discriminative\\napproaches. However, a major challenge still remains: it is intractable to\\ncalculate the expectations of non-local features that are critical for\\ncapturing the divergence between natural languages. We propose a contrastive\\napproach that aims to differentiate observed training examples from noises. It\\nnot only introduces prior knowledge to guide unsupervised learning but also\\ncancels out partition functions. Based on the observation that the probability\\nmass of log-linear models for word alignment is usually highly concentrated, we\\npropose to use top-n alignments to approximate the expectations with respect to\\nposterior distributions. This allows for efficient and accurate calculation of\\nexpectations of non-local features. Experiments show that our approach achieves\\nsignificant improvements over state-of-the-art unsupervised word alignment\\nmethods.\\n',\n",
              " \"  Statistics pedagogy values using a variety of examples. Thanks to text\\nresources on the Web, and since statistical packages have the ability to\\nanalyze string data, it is now easy to use language-based examples in a\\nstatistics class. Three such examples are discussed here. First, many types of\\nwordplay (e.g., crosswords and hangman) involve finding words with letters that\\nsatisfy a certain pattern. Second, linguistics has shown that idiomatic pairs\\nof words often appear together more frequently than chance. For example, in the\\nBrown Corpus, this is true of the phrasal verb to throw up (p-value=7.92E-10.)\\nThird, a pangram contains all the letters of the alphabet at least once. These\\nare searched for in Charles Dickens' A Christmas Carol, and their lengths are\\ncompared to the expected value given by the unequal probability coupon\\ncollector's problem as well as simulations.\\n\",\n",
              " '  We propose a spatial diffuseness feature for deep neural network (DNN)-based\\nautomatic speech recognition to improve recognition accuracy in reverberant and\\nnoisy environments. The feature is computed in real-time from multiple\\nmicrophone signals without requiring knowledge or estimation of the direction\\nof arrival, and represents the relative amount of diffuse noise in each time\\nand frequency bin. It is shown that using the diffuseness feature as an\\nadditional input to a DNN-based acoustic model leads to a reduced word error\\nrate for the REVERB challenge corpus, both compared to logmelspec features\\nextracted from noisy signals, and features enhanced by spectral subtraction.\\n',\n",
              " '  Hybrid approaches for automatic vowelization of Arabic texts are presented in\\nthis article. The process is made up of two modules. In the first one, a\\nmorphological analysis of the text words is performed using the open source\\nmorphological Analyzer AlKhalil Morpho Sys. Outputs for each word analyzed out\\nof context, are its different possible vowelizations. The integration of this\\nAnalyzer in our vowelization system required the addition of a lexical database\\ncontaining the most frequent words in Arabic language. Using a statistical\\napproach based on two hidden Markov models (HMM), the second module aims to\\neliminate the ambiguities. Indeed, for the first HMM, the unvowelized Arabic\\nwords are the observed states and the vowelized words are the hidden states.\\nThe observed states of the second HMM are identical to those of the first, but\\nthe hidden states are the lists of possible diacritics of the word without its\\nArabic letters. Our system uses Viterbi algorithm to select the optimal path\\namong the solutions proposed by Al Khalil Morpho Sys. Our approach opens an\\nimportant way to improve the performance of automatic vowelization of Arabic\\ntexts for other uses in automatic natural language processing.\\n',\n",
              " \"  Euphonic conjunctions (sandhis) form a very important aspect of Sanskrit\\nmorphology and phonology. The traditional and modern methods of studying about\\neuphonic conjunctions in Sanskrit follow different methodologies. The former\\ninvolves a rigorous study of the Paninian system embodied in Panini's\\nAshtadhyayi, while the latter usually involves the study of a few important\\nsandhi rules with the use of examples. The former is not suitable for\\nbeginners, and the latter, not sufficient to gain a comprehensive understanding\\nof the operation of sandhi rules. This is so since there are not only numerous\\nsandhi rules and exceptions, but also complex precedence rules involved. The\\nneed for a new ontology for sandhi-tutoring was hence felt. This work presents\\na comprehensive ontology designed to enable a student-user to learn in stages\\nall about euphonic conjunctions and the relevant aphorisms of Sanskrit grammar\\nand to test and evaluate the progress of the student-user. The ontology forms\\nthe basis of a multimedia sandhi tutor that was given to different categories\\nof users including Sanskrit scholars for extensive and rigorous testing.\\n\",\n",
              " \"  With the acceptance of Western culture and science, Traditional Chinese\\nMedicine (TCM) has become a controversial issue in China. So, it's important to\\nstudy the public's sentiment and opinion on TCM. The rapid development of\\nonline social network, such as twitter, make it convenient and efficient to\\nsample hundreds of millions of people for the aforementioned sentiment study.\\nTo the best of our knowledge, the present work is the first attempt that\\napplies sentiment analysis to the domain of TCM on Sina Weibo (a twitter-like\\nmicroblogging service in China). In our work, firstly we collect tweets topic\\nabout TCM from Sina Weibo, and label the tweets as supporting TCM and opposing\\nTCM automatically based on user tag. Then, a support vector machine classifier\\nhas been built to predict the sentiment of TCM tweets without labels. Finally,\\nwe present a method to adjust the classifier result. The performance of\\nF-measure attained with our method is 97%.\\n\",\n",
              " '  The increasing diversity of languages used on the web introduces a new level\\nof complexity to Information Retrieval (IR) systems. We can no longer assume\\nthat textual content is written in one language or even the same language\\nfamily. In this paper, we demonstrate how to build massive multilingual\\nannotators with minimal human expertise and intervention. We describe a system\\nthat builds Named Entity Recognition (NER) annotators for 40 major languages\\nusing Wikipedia and Freebase. Our approach does not require NER human annotated\\ndatasets or language specific resources like treebanks, parallel corpora, and\\northographic rules. The novelty of approach lies therein - using only language\\nagnostic techniques, while achieving competitive performance.\\n  Our method learns distributed word representations (word embeddings) which\\nencode semantic and syntactic features of words in each language. Then, we\\nautomatically generate datasets from Wikipedia link structure and Freebase\\nattributes. Finally, we apply two preprocessing stages (oversampling and exact\\nsurface form matching) which do not require any linguistic expertise.\\n  Our evaluation is two fold: First, we demonstrate the system performance on\\nhuman annotated datasets. Second, for languages where no gold-standard\\nbenchmarks are available, we propose a new method, distant evaluation, based on\\nstatistical machine translation.\\n',\n",
              " \"  Natural logic offers a powerful relational conception of meaning that is a\\nnatural counterpart to distributed semantic representations, which have proven\\nvaluable in a wide range of sophisticated language tasks. However, it remains\\nan open question whether it is possible to train distributed representations to\\nsupport the rich, diverse logical reasoning captured by natural logic. We\\naddress this question using two neural network-based models for learning\\nembeddings: plain neural networks and neural tensor networks. Our experiments\\nevaluate the models' ability to learn the basic algebra of natural logic\\nrelations from simulated data and from the WordNet noun graph. The overall\\npositive results are promising for the future of learned distributed\\nrepresentations in the applied modeling of logical semantics.\\n\",\n",
              " '  Long short-term memory (LSTM) based acoustic modeling methods have recently\\nbeen shown to give state-of-the-art performance on some speech recognition\\ntasks. To achieve a further performance improvement, in this research, deep\\nextensions on LSTM are investigated considering that deep hierarchical model\\nhas turned out to be more efficient than a shallow one. Motivated by previous\\nresearch on constructing deep recurrent neural networks (RNNs), alternative\\ndeep LSTM architectures are proposed and empirically evaluated on a large\\nvocabulary conversational telephone speech recognition task. Meanwhile,\\nregarding to multi-GPU devices, the training process for LSTM networks is\\nintroduced and discussed. Experimental results demonstrate that the deep LSTM\\nnetworks benefit from the depth and yield the state-of-the-art performance on\\nthis task.\\n',\n",
              " '  In this paper we provide a quantitative framework for the study of\\nphonological networks (PNs) for the English language by carrying out principled\\ncomparisons to null models, either based on site percolation, randomization\\ntechniques, or network growth models. In contrast to previous work, we mainly\\nfocus on null models that reproduce lower order characteristics of the\\nempirical data. We find that artificial networks matching connectivity\\nproperties of the English PN are exceedingly rare: this leads to the hypothesis\\nthat the word repertoire might have been assembled over time by preferentially\\nintroducing new words which are small modifications of old words. Our null\\nmodels are able to explain the \"power-law-like\" part of the degree\\ndistributions and generally retrieve qualitative features of the PN such as\\nhigh clustering, high assortativity coefficient, and small-world\\ncharacteristics. However, the detailed comparison to expectations from null\\nmodels also points out significant differences, suggesting the presence of\\nadditional constraints in word assembly. Key constraints we identify are the\\navoidance of large degrees, the avoidance of triadic closure, and the avoidance\\nof large non-percolating clusters.\\n',\n",
              " '  This paper proposes the use of dependent types for pragmatic phenomena such\\nas pronoun binding and presupposition resolution as a type-theoretic\\nalternative to formalisms such as Discourse Representation Theory and Dynamic\\nSemantics.\\n',\n",
              " '  We study the performance of Arabic text classification combining various\\ntechniques: (a) tfidf vs. dependency syntax, for feature selection and\\nweighting; (b) class association rules vs. support vector machines, for\\nclassification. The Arabic text is used in two forms: rootified and lightly\\nstemmed. The results we obtain show that lightly stemmed text leads to better\\nperformance than rootified text; that class association rules are better suited\\nfor small feature sets obtained by dependency syntax constraints; and, finally,\\nthat support vector machines are better suited for large feature sets based on\\nmorphological feature selection criteria.\\n',\n",
              " '  This paper describes our resource-building results for an eight-week JHU\\nHuman Language Technology Center of Excellence Summer Camp for Applied Language\\nExploration (SCALE-2009) on Semantically-Informed Machine Translation.\\nSpecifically, we describe the construction of a modality annotation scheme, a\\nmodality lexicon, and two automated modality taggers that were built using the\\nlexicon and annotation scheme. Our annotation scheme is based on identifying\\nthree components of modality: a trigger, a target and a holder. We describe how\\nour modality lexicon was produced semi-automatically, expanding from an initial\\nhand-selected list of modality trigger words and phrases. The resulting\\nexpanded modality lexicon is being made publicly available. We demonstrate that\\none tagger---a structure-based tagger---results in precision around 86%\\n(depending on genre) for tagging of a standard LDC data set. In a machine\\ntranslation application, using the structure-based tagger to annotate English\\nmodalities on an English-Urdu training corpus improved the translation quality\\nscore for Urdu by 0.3 Bleu points in the face of sparse training data.\\n',\n",
              " '  We describe a visualization tool that can be used to view the change in\\nmeaning of words over time. The tool makes use of existing (static) word\\nembedding datasets together with a timestamped $n$-gram corpus to create {\\\\em\\ntemporal} word embeddings.\\n',\n",
              " '  The syntactic structure of a sentence can be modeled as a tree where vertices\\nare words and edges indicate syntactic dependencies between words. It is\\nwell-known that those edges normally do not cross when drawn over the sentence.\\nHere a new null hypothesis for the number of edge crossings of a sentence is\\npresented. That null hypothesis takes into account the length of the pair of\\nedges that may cross and predicts the relative number of crossings in random\\ntrees with a small error, suggesting that a ban of crossings or a principle of\\nminimization of crossings are not needed in general to explain the origins of\\nnon-crossing dependencies. Our work paves the way for more powerful null\\nhypotheses to investigate the origins of non-crossing dependencies in nature.\\n',\n",
              " \"  Building machine translation (MT) test sets is a relatively expensive task.\\nAs MT becomes increasingly desired for more and more language pairs and more\\nand more domains, it becomes necessary to build test sets for each case. In\\nthis paper, we investigate using Amazon's Mechanical Turk (MTurk) to make MT\\ntest sets cheaply. We find that MTurk can be used to make test sets much\\ncheaper than professionally-produced test sets. More importantly, in\\nexperiments with multiple MT systems, we find that the MTurk-produced test sets\\nyield essentially the same conclusions regarding system performance as the\\nprofessionally-produced test sets yield.\\n\",\n",
              " '  We explore how to improve machine translation systems by adding more\\ntranslation data in situations where we already have substantial resources. The\\nmain challenge is how to buck the trend of diminishing returns that is commonly\\nencountered. We present an active learning-style data solicitation algorithm to\\nmeet this challenge. We test it, gathering annotations via Amazon Mechanical\\nTurk, and find that we get an order of magnitude increase in performance rates\\nof improvement.\\n',\n",
              " \"  We apply entropy agglomeration (EA), a recently introduced algorithm, to\\ncluster the words of a literary text. EA is a greedy agglomerative procedure\\nthat minimizes projection entropy (PE), a function that can quantify the\\nsegmentedness of an element set. To apply it, the text is reduced to a feature\\nallocation, a combinatorial object to represent the word occurences in the\\ntext's paragraphs. The experiment results demonstrate that EA, despite its\\nreduction and simplicity, is useful in capturing significant relationships\\namong the words in the text. This procedure was implemented in Python and\\npublished as a free software: REBUS.\\n\",\n",
              " '  Applying natural language processing for mining and intelligent information\\naccess to tweets (a form of microblog) is a challenging, emerging research\\narea. Unlike carefully authored news text and other longer content, tweets pose\\na number of new challenges, due to their short, noisy, context-dependent, and\\ndynamic nature. Information extraction from tweets is typically performed in a\\npipeline, comprising consecutive stages of language identification,\\ntokenisation, part-of-speech tagging, named entity recognition and entity\\ndisambiguation (e.g. with respect to DBpedia). In this work, we describe a new\\nTwitter entity disambiguation dataset, and conduct an empirical analysis of\\nnamed entity recognition and disambiguation, investigating how robust a number\\nof state-of-the-art systems are on such noisy texts, what the main sources of\\nerror are, and which problems should be further investigated to improve the\\nstate of the art.\\n',\n",
              " '  Mel Frequency Cepstral Coefficients (MFCCs) are the most popularly used\\nspeech features in most speech and speaker recognition applications. In this\\nwork, we propose a modified Mel filter bank to extract MFCCs from subsampled\\nspeech. We also propose a stronger metric which effectively captures the\\ncorrelation between MFCCs of original speech and MFCC of resampled speech. It\\nis found that the proposed method of filter bank construction performs\\ndistinguishably well and gives recognition performance on resampled speech\\nclose to recognition accuracies on original speech.\\n',\n",
              " '  We describe a paradigm for combining manual and automatic error correction of\\nnoisy structured lexicographic data. Modifications to the structure and\\nunderlying text of the lexicographic data are expressed in a simple,\\ninterpreted programming language. Dictionary Manipulation Language (DML)\\ncommands identify nodes by unique identifiers, and manipulations are performed\\nusing simple commands such as create, move, set text, etc. Corrected lexicons\\nare produced by applying sequences of DML commands to the source version of the\\nlexicon. DML commands can be written manually to repair one-off errors or\\ngenerated automatically to correct recurring problems. We discuss advantages of\\nthe paradigm for the task of editing digital bilingual dictionaries.\\n',\n",
              " '  Dictionaries are often developed using tools that save to Extensible Markup\\nLanguage (XML)-based standards. These standards often allow high-level\\nrepeating elements to represent lexical entries, and utilize descendants of\\nthese repeating elements to represent the structure within each lexical entry,\\nin the form of an XML tree. In many cases, dictionaries are published that have\\nerrors and inconsistencies that are expensive to find manually. This paper\\ndiscusses a method for dictionary writers to quickly audit structural\\nregularity across entries in a dictionary by using statistical language\\nmodeling. The approach learns the patterns of XML nodes that could occur within\\nan XML tree, and then calculates the probability of each XML tree in the\\ndictionary against these patterns to look for entries that diverge from the\\nnorm.\\n',\n",
              " '  Neural Machine Translation (NMT) is a new approach to machine translation\\nthat has shown promising results that are comparable to traditional approaches.\\nA significant weakness in conventional NMT systems is their inability to\\ncorrectly translate very rare words: end-to-end NMTs tend to have relatively\\nsmall vocabularies with a single unk symbol that represents every possible\\nout-of-vocabulary (OOV) word. In this paper, we propose and implement an\\neffective technique to address this problem. We train an NMT system on data\\nthat is augmented by the output of a word alignment algorithm, allowing the NMT\\nsystem to emit, for each OOV word in the target sentence, the position of its\\ncorresponding word in the source sentence. This information is later utilized\\nin a post-processing step that translates every OOV word using a dictionary.\\nOur experiments on the WMT14 English to French translation task show that this\\nmethod provides a substantial improvement of up to 2.8 BLEU points over an\\nequivalent NMT system that does not use this technique. With 37.5 BLEU points,\\nour NMT system is the first to surpass the best result achieved on a WMT14\\ncontest task.\\n',\n",
              " '  We present paired learning and inference algorithms for significantly\\nreducing computation and increasing speed of the vector dot products in the\\nclassifiers that are at the heart of many NLP components. This is accomplished\\nby partitioning the features into a sequence of templates which are ordered\\nsuch that high confidence can often be reached using only a small fraction of\\nall features. Parameter estimation is arranged to maximize accuracy and early\\nconfidence in this sequence. We present experiments in left-to-right\\npart-of-speech tagging on WSJ, demonstrating that we can preserve accuracy\\nabove 97% with over a five-fold reduction in run-time.\\n',\n",
              " '  When digitizing a print bilingual dictionary, whether via optical character\\nrecognition or manual entry, it is inevitable that errors are introduced into\\nthe electronic version that is created. We investigate automating the process\\nof detecting errors in an XML representation of a digitized print dictionary\\nusing a hybrid approach that combines rule-based, feature-based, and language\\nmodel-based methods. We investigate combining methods and show that using\\nrandom forests is a promising approach. We find that in isolation, unsupervised\\nmethods rival the performance of supervised methods. Random forests typically\\nrequire training data so we investigate how we can apply random forests to\\ncombine individual base methods that are themselves unsupervised without\\nrequiring large amounts of training data. Experiments reveal empirically that a\\nrelatively small amount of data is sufficient and can potentially be further\\nreduced through specific selection criteria.\\n',\n",
              " '  Domain ontologies are important information sources for knowledge-based\\nsystems. Yet, building domain ontologies from scratch is known to be a very\\nlabor-intensive process. In this study, we present our semi-automatic approach\\nto building an ontology for the domain of wind energy which is an important\\ntype of renewable energy with a growing share in electricity generation all\\nover the world. Related Wikipedia articles are first processed in an automated\\nmanner to determine the basic concepts of the domain together with their\\nproperties and next the concepts, properties, and relationships are organized\\nto arrive at the ultimate ontology. We also provide pointers to other\\nengineering ontologies which could be utilized together with the proposed wind\\nenergy ontology in addition to its prospective application areas. The current\\nstudy is significant as, to the best of our knowledge, it proposes the first\\nconsiderably wide-coverage ontology for the wind energy domain and the ontology\\nis built through a semi-automatic process which makes use of the related Web\\nresources, thereby reducing the overall cost of the ontology building process.\\n',\n",
              " '  Social media texts are significant information sources for several\\napplication areas including trend analysis, event monitoring, and opinion\\nmining. Unfortunately, existing solutions for tasks such as named entity\\nrecognition that perform well on formal texts usually perform poorly when\\napplied to social media texts. In this paper, we report on experiments that\\nhave the purpose of improving named entity recognition on Turkish tweets, using\\ntwo different annotated data sets. In these experiments, starting with a\\nbaseline named entity recognition system, we adapt its recognition rules and\\nresources to better fit Twitter language by relaxing its capitalization\\nconstraint and by diacritics-based expansion of its lexical resources, and we\\nemploy a simplistic normalization scheme on tweets to observe the effects of\\nthese on the overall named entity recognition performance on Turkish tweets.\\nThe evaluation results of the system with these different settings are provided\\nwith discussions of these results.\\n',\n",
              " '  Parsing the Arabic language is a difficult task given the specificities of\\nthis language and given the scarcity of digital resources (grammars and\\nannotated corpora). In this paper, we suggest a method for Arabic parsing based\\non supervised machine learning. We used the SVMs algorithm to select the\\nsyntactic labels of the sentence. Furthermore, we evaluated our parser\\nfollowing the cross validation method by using the Penn Arabic Treebank. The\\nobtained results are very encouraging.\\n',\n",
              " '  Part-of-speech (POS) tagging is a fundamental component for performing\\nnatural language tasks such as parsing, information extraction, and question\\nanswering. When POS taggers are trained in one domain and applied in\\nsignificantly different domains, their performance can degrade dramatically. We\\npresent a methodology for rapid adaptation of POS taggers to new domains. Our\\ntechnique is unsupervised in that a manually annotated corpus for the new\\ndomain is not necessary. We use suffix information gathered from large amounts\\nof raw text as well as orthographic information to increase the lexical\\ncoverage. We present an experiment in the Biological domain where our POS\\ntagger achieves results comparable to POS taggers specifically trained to this\\ndomain.\\n',\n",
              " \"  How many words (and which ones) are sufficient to define all other words?\\nWhen dictionaries are analyzed as directed graphs with links from defining\\nwords to defined words, they reveal a latent structure. Recursively removing\\nall words that are reachable by definition but that do not define any further\\nwords reduces the dictionary to a Kernel of about 10%. This is still not the\\nsmallest number of words that can define all the rest. About 75% of the Kernel\\nturns out to be its Core, a Strongly Connected Subset of words with a\\ndefinitional path to and from any pair of its words and no word's definition\\ndepending on a word outside the set. But the Core cannot define all the rest of\\nthe dictionary. The 25% of the Kernel surrounding the Core consists of small\\nstrongly connected subsets of words: the Satellites. The size of the smallest\\nset of words that can define all the rest (the graph's Minimum Feedback Vertex\\nSet or MinSet) is about 1% of the dictionary, 15% of the Kernel, and half-Core,\\nhalf-Satellite. But every dictionary has a huge number of MinSets. The Core\\nwords are learned earlier, more frequent, and less concrete than the\\nSatellites, which in turn are learned earlier and more frequent but more\\nconcrete than the rest of the Dictionary. In principle, only one MinSet's words\\nwould need to be grounded through the sensorimotor capacity to recognize and\\ncategorize their referents. In a dual-code sensorimotor-symbolic model of the\\nmental lexicon, the symbolic code could do all the rest via re-combinatory\\ndefinition.\\n\",\n",
              " '  In this article, we describe an approach for automatic detection of\\nnoun-adjective agreement errors in Bulgarian texts by explaining the necessary\\nsteps required to develop a simple Java-based language processing application.\\nFor this purpose, we use the GATE language processing framework, which is\\ncapable of analyzing texts in Bulgarian language and can be embedded in\\nsoftware applications, accessed through a set of Java APIs. In our example\\napplication we also demonstrate how to use the functionality of GATE to perform\\nregular expressions over annotations for detecting agreement errors in simple\\nnoun phrases formed by two words - attributive adjective and a noun, where the\\nattributive adjective precedes the noun. The provided code samples can also be\\nused as a starting point for implementing natural language processing\\nfunctionalities in software applications related to language processing tasks\\nlike detection, annotation and retrieval of word groups meeting a specific set\\nof criteria.\\n',\n",
              " '  Suicide is among the leading causes of death in China. However, technical\\napproaches toward preventing suicide are challenging and remaining under\\ndevelopment. Recently, several actual suicidal cases were preceded by users who\\nposted microblogs with suicidal ideation to Sina Weibo, a Chinese social media\\nnetwork akin to Twitter. It would therefore be desirable to detect suicidal\\nideations from microblogs in real-time, and immediately alert appropriate\\nsupport groups, which may lead to successful prevention. In this paper, we\\npropose a real-time suicidal ideation detection system deployed over Weibo,\\nusing machine learning and known psychological techniques. Currently, we have\\nidentified 53 known suicidal cases who posted suicide notes on Weibo prior to\\ntheir deaths.We explore linguistic features of these known cases using a\\npsychological lexicon dictionary, and train an effective suicidal Weibo post\\ndetection model. 6714 tagged posts and several classifiers are used to verify\\nthe model. By combining both machine learning and psychological knowledge, SVM\\nclassifier has the best performance of different classifiers, yielding an\\nF-measure of 68:3%, a Precision of 78:9%, and a Recall of 60:3%.\\n',\n",
              " '  Acoustic models using probabilistic linear discriminant analysis (PLDA)\\ncapture the correlations within feature vectors using subspaces which do not\\nvastly expand the model. This allows high dimensional and correlated feature\\nspaces to be used, without requiring the estimation of multiple high dimension\\ncovariance matrices. In this letter we extend the recently presented PLDA\\nmixture model for speech recognition through a tied PLDA approach, which is\\nbetter able to control the model size to avoid overfitting. We carried out\\nexperiments using the Switchboard corpus, with both mel frequency cepstral\\ncoefficient features and bottleneck feature derived from a deep neural network.\\nReductions in word error rate were obtained by using tied PLDA, compared with\\nthe PLDA mixture model, subspace Gaussian mixture models, and deep neural\\nnetworks.\\n',\n",
              " '  Standard LDA model suffers the problem that the topic assignment of each word\\nis independent and word correlation hence is neglected. To address this\\nproblem, in this paper, we propose a model called Word Related Latent Dirichlet\\nAllocation (WR-LDA) by incorporating word correlation into LDA topic models.\\nThis leads to new capabilities that standard LDA model does not have such as\\nestimating infrequently occurring words or multi-language topic modeling.\\nExperimental results demonstrate the effectiveness of our model compared with\\nstandard LDA.\\n',\n",
              " ...]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_model = SentenceTransformer(\"thenlper/gte-small\")\n",
        "embeddings = embedding_model.encode(abstracts, show_progress_bar=True)\n",
        "umap_model = UMAP(\n",
        "                n_components=5, min_dist=0.0, metric='cosine', random_state=42\n",
        "              )\n",
        "hdbscan_model = HDBSCAN(\n",
        "    min_cluster_size = 50,\n",
        "    metric = 'euclidean',\n",
        "    cluster_selection_method='eom',\n",
        "    prediction_data=True\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369,
          "referenced_widgets": [
            "fe0709f1c70643d2874600e54f3aad04",
            "51b9744f4d724b599eba7901cd28e2f1",
            "05e830b1b9f44836bde15b5e8c2d9cd7",
            "0a9c4725b97741ada7366f348e727e68",
            "5491f3c7d92b4598adea1f77b12d09dd",
            "98a200d284844d48854a21bb309e4e36",
            "e0be0a34e69f4ced9b7df3818eb94674",
            "64c1f1eaa4a345d69f0784560b21acec",
            "20392996f12f4afa907355a0fd77ba0f",
            "22c268cd8e934257b882bc3700af7615",
            "6a5ec42176264ded84cf4c5ca948bbd7",
            "abe9e25d8e174915b3e5172b9fab47dc",
            "f96768db1b994296bc5efd2c877d6d89",
            "0c946566e742476aaa9bc802ee3dc14b",
            "5f0ef976731b4bb0bd005b943b31536f",
            "806d8cc89cdf44dd9d40ba91bf2e42ac",
            "b2ecd5466438494f8dbc2dfffcedd516",
            "930730e9d5054959b2e7a8b460884275",
            "4eaa5024e1384309a3db6b93a03ee352",
            "4f08a828156e496e8ca2863aec066031",
            "4bc8c1a917e24222977c923c855bf4e5",
            "23dfb06413d84f4da939c518a7acefd8",
            "d80d2639362346ce91d8e908c745c3c0",
            "dbcbcd11dcf14947ae1cc91bcc7116c6",
            "346c277025984002808791df3e19b410",
            "33ef64a4fa324c50a4e13c3b0c469b96",
            "e45f18246a4540df84e925f93b770499",
            "35acc08d8c834676a66a22af387b5ebf",
            "016cdcd4ddee442dba05ccc7841ad949",
            "57ea03eb02c3426690cf7f9c2e7faea6",
            "cb4a149be48948cc91cf4a153be73847",
            "2b6d1d0704be46819917f3c528adc94c",
            "88f37ab9203147de86e51ee9d10845b1",
            "62ffad936be74274bbc117871235fdd5",
            "7a5db3592af2480aa531df442d2108d4",
            "559a8b2ff42f4ea4a4a27091b406ba7e",
            "2f3845fbe07e42848ad69e3dba056858",
            "339f407463544204aade5f7beeb51288",
            "fb390a1dd3d84ff8903f4846e5cdabbe",
            "4fc82d6231e8423893f2a9914fac06de",
            "c36fc67bea70480e9130cf947a6f1988",
            "26992231639b43a1a7405ae37e9499a3",
            "67421ca406154fce8e02cc609f0717dd",
            "72d741d75594414bbbbb37f1cc775dcd",
            "cacecc3c797e48adb2ca4e50efa3a7b5",
            "fe7fd9b8fea541be8d5574a52d68bb6e",
            "f3de6db1b6824e619bb8dd4961ab9006",
            "684611b1a2a04c72bb052fc17fe3ce3f",
            "2f4f4883499345319f973a7d8006a63c",
            "a5a9feedccfa4124a001c8d3e8e0bc08",
            "ed857605e6074187a5ab77e6ea21277f",
            "86e29ee0c56b434a9e956f145286a22c",
            "afe78a5465474fdbad72670cd7943fde",
            "c4155ec7fded4932853d1034c4c16c84",
            "bfadd685e1dd492bbbb020c39165c6c4",
            "2f8d89bfeb3e43eaaf3fcd842a581f6f",
            "22421af7eaf342a886e69281d3a77597",
            "7b3d910d40b449c69d78734db5cb9ce6",
            "15f1da8b3c9e4f12a36df3359bd02b37",
            "ef008e9be1cd48da82787c3d5135ffa5",
            "35a15dadef31410b832b8ff5fb2000de",
            "23d95b64b61447d2b2d100a8784dfdae",
            "5ed051acdcfe4a8fbda27d9282b45ce6",
            "af0896a5d1fb417cb227577a32c3624e",
            "50fb68eeb8754de49c19b77e8bf278ba",
            "2635d97cc4b848f3a1fe20d5f50597e8",
            "007392afb06e42e19ba8d838ba8ec01b",
            "7e434a9809cf4034ac132e88f0e94127",
            "6d887fd86a544358be94fe8ed7d42107",
            "a07669fb844d46698e667779d290eb63",
            "ec9e07f1f14d4d99a37b838c530de8e1",
            "00084c2392944e95b55adaeeac9d7c66",
            "3574d62cc7be428eb07f92ae11647c65",
            "6e4f370e401244c29ef4759e26fb9334",
            "8247661683dd493fbf1be2fe1f339758",
            "314dc4033b1640e8adef839b291940d4",
            "b337edd123514a9b874e128830d1959b",
            "6185fc21129f490b9d49a7879936e93d",
            "aa5dfd092c4b4a8fb0645743762954bd",
            "1981c4a97f7e4653a68882539cb20bab",
            "33e42c974dd545b9a9ad8ef981a7d8a8",
            "47921d3e9cfd4d26b469af79cb0024bc",
            "744f468845bf4811bd2bd23cb162b963",
            "b2f2134440bf4aef9e3857ac49ea0897",
            "e78a4d7c0d27495eaccaf77b9f0948ca",
            "59e6fb7a33c0417cbfb6a0c38b5e780d",
            "ef474b814ceb451094ed4bebef24ae01",
            "8a8fbf1845d64d23a485aa98c121576e",
            "ebdd1e4e459a4b1480ecec598ab73982",
            "12dbc33037914895bd87188392cb4fbb",
            "f87cebc04fb64a1888a96c438aa9b31c",
            "a723b3239d1a40b48fdf3aead16e4e79",
            "0e0be00ac1e34152a623fca249498886",
            "2fa606d03117408b9bd311d2cd691746",
            "c262889daafe483d8a9c84fab65d1140",
            "2bcc0bac7f7342ef9aab0f2ea1c395c8",
            "c21856871e1c412093e0551af380dcf6",
            "f587a00d215a4c98b35ea9eee8d51b7f",
            "8a65d119cbf440bfbd5abe54a1849fd3",
            "98b3b20670624c0886ac1336a9fbc2eb",
            "5c553942a0954e0a95c4a97d885ed112",
            "f9d44b6d92d0422da9517ad19966bac6",
            "957db9cdc91d4f859e7c20308ba2d8ac",
            "79c314ece5fa4b8f853c9b4520d24392",
            "c1ad6b223e5946c2a07aa1f9b2d67530",
            "e5528c7f7a124ebfb1fb58409512d955",
            "077ef9909cc04af7b0e6011d74db33ed",
            "f7e6ce002e6f401fab31d99112554b4b",
            "dc004049d4954161a48c6d5aa99a6fb2",
            "599fe391bc8b410a9eb9ffb48c759708",
            "673cc64aa4694c56aa06d3ea3e29b36b",
            "f25fc9b617814aa7b2602c7f54ca6cc8",
            "f1b0bc7bcd6f4c968cb414e6131eda83",
            "d6b14af18ea7496e833c4867a291a7ad",
            "e4fb0eb401654505950a4ddaf68961c4",
            "494ec6f14412485cb7879a77dddaac41",
            "aedc2d2135c34490a5f44c69d6e0479c",
            "6fbbe909f7f04a3b863ee139396b8b32",
            "c5585051b9204460be2ac5f5b40e0f1a",
            "2cb3d89c75684b19a8ad512fb9a8f357",
            "282138f5cbca4dde93e8cda2c78bcb63"
          ]
        },
        "id": "SE4bOJ2KsdnZ",
        "outputId": "be38480b-c21e-4808-83fc-860bb76701e0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "modules.json:   0%|          | 0.00/385 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fe0709f1c70643d2874600e54f3aad04"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md:   0%|          | 0.00/68.1k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "abe9e25d8e174915b3e5172b9fab47dc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "sentence_bert_config.json:   0%|          | 0.00/57.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d80d2639362346ce91d8e908c745c3c0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/583 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "62ffad936be74274bbc117871235fdd5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/66.7M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cacecc3c797e48adb2ca4e50efa3a7b5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/394 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2f8d89bfeb3e43eaaf3fcd842a581f6f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "007392afb06e42e19ba8d838ba8ec01b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/712k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6185fc21129f490b9d49a7879936e93d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ebdd1e4e459a4b1480ecec598ab73982"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "1_Pooling%2Fconfig.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "98b3b20670624c0886ac1336a9fbc2eb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Batches:   0%|          | 0/1405 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "673cc64aa4694c56aa06d3ea3e29b36b"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"TheBloke/zephyr-7B-alpha-GGUF\",\n",
        "    model_file=\"zephyr-7b-alpha.Q4_K_M.gguf\",\n",
        "    model_type=\"mistral\",\n",
        "    gpu_layers=50,\n",
        "    hf=True\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceH4/zephyr-7b-alpha\")\n",
        "generator = pipeline(\n",
        "    model=model, tokenizer=tokenizer,\n",
        "    task='text-generation',\n",
        "    max_new_tokens=50,\n",
        "    repetition_penalty=1.1\n",
        ")\n",
        "\n",
        "prompt = \"\"\"<|system|>You are a helpful, respectful and honest assistant for labeling topics..</s>\n",
        "<|user|>\n",
        "I have a topic that contains the following documents:\n",
        "[DOCUMENTS]\n",
        "\n",
        "The topic is described by the following keywords: '[KEYWORDS]'.\n",
        "\n",
        "Based on the information about the topic above, please create a short label of this topic. Make sure you to only return the label and nothing more.</s>\n",
        "<|assistant|>\"\"\"\n",
        "\n",
        "\n",
        "zephyr = TextGeneration(generator, prompt=prompt)\n",
        "representation_model = {\"Zephyr\": zephyr}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 412,
          "referenced_widgets": [
            "3da675de37074ebcb32cb1e65b581b00",
            "08f793230a1d4e9d85208cee7f1d8a4a",
            "be1a6d60991f4d4984164cb1016c137b",
            "377586755a424849b4ef9a846a59611d",
            "6ebcb229a2e34cbbb7cc3fad4c4fdf51",
            "d6cb083ab8d547a3958b0639786b7a3e",
            "df440e48aeb642fe82fa4edad66f8d85",
            "33954e2986eb461badb674728f58cc84",
            "a17598e3c5c84932bb21c22864fbc122",
            "94f545a3d2b845d4827fa66d0599471d",
            "f5f5ded240154d9394bfd34aecb6b9f2",
            "a3a2bbe6a4774f61b67620c2ec13df8e",
            "1c4559d365044f81a36eff6cd3424bdf",
            "439baa10f9c448acb176735e2f0b9e1e",
            "30f8e882d6434389bb1b8fa8f632ba35",
            "3132967d837043ceb6d4a6416d394c60",
            "30fd7ccff8414125b742096ecaa3cb05",
            "682d10718cd54c41a1dec40feab8ec40",
            "5df98896f84d437c8fd45b9a80a41add",
            "6d4b242d31994847a666f6a41ca84985",
            "4611bb8b57864781a3df5ad9aa2884ce",
            "249e41084e11414191e50d15610c8d47",
            "e951e0be021f4a2e9e7e2d438709e58c",
            "5c1b72bfd99845cb9d0cedf94399534b",
            "5936169bef7549898ebc15a0b20183b5",
            "9da12ab1ffce46d7a789f0d735bede69",
            "0cd71b3ae40848448f979e247cf778b7",
            "90046946acf54424a8935522d5ef36c7",
            "6c9022e72d374d44a2591cc93e489ecb",
            "c9d47c959a8548849785f82cf2478e74",
            "497f466eb2684a6abeb312d988bf425d",
            "b23fdc5a998849b8a67d532a20754d75",
            "04913e603a024bc09f5a5df18a64a13d",
            "15c50aded4f245a7b62316252108898c",
            "5ed55225189946e38e8fe1b64db43c23",
            "75ac977658b34657a553e705c32e0e13",
            "9097ce966b134be1999444470d8b3837",
            "11bc8be87783481985954badcad32b65",
            "1d4c6a41c0b349008440de522b6cfee9",
            "70e72d35a82c436ea4d14d41ef0eea71",
            "ba3f9daf092e41f7a345337a30f90d48",
            "5dad5845af4142bca8e1f80cd94af2c0",
            "e944014f134b40948c31c2f2240d99b4",
            "eab437ae93514c69920ebd24c2340064",
            "5df15fb0cec04d05b3f0e236665dc9f1",
            "c0c73aa484d94d3ba6ceee813c6f3f01",
            "67e4497b7b9b4164a165b9a7dd1976b6",
            "56c0956c8d924048904dfadd19cbcca0",
            "461d65b4084b4437913e041a904bb5f8",
            "5fde867f24504125b07ec0aef4627ee2",
            "3149001d1fe54f51883b2b58854256e7",
            "eeb42725fd6547548ede1df7a64f2e27",
            "cf4e072472b543ff95e8f64bce404c01",
            "81c948d09de54610a1e6ceb85de17885",
            "f5d39d5aaf7243bbbb419ee5e5038d8b",
            "024f5e2506b44f19a2ab06a75614be2d",
            "b423ac7985b24a9eb079bff5d5e3cfac",
            "14e38e26b69545b49c195aa4d036605c",
            "610955c453cb491ab49debe93e06bf60",
            "3fc2ff3fb1634eb3be030a4234ff7ce2",
            "2781d8f6256440d5a4c423020b92c374",
            "e7b7952c0c6a4680854a0d2c81737094",
            "e17c272c294b47a6bcb49045766a951b",
            "38ac05b10da741a4b0423aac4434a67a",
            "90ea7fdfe1d84cc8a30c8210809cc8b3",
            "73ccd49037104c83adb336b172b0d7ea",
            "e0c8f706f70142e6870c4499ee9b5bfa",
            "d96a5d8d91aa476e8bdb4d59a7c7f328",
            "8fc46b526da2471fb7e106af83a7d42b",
            "ad1dc12621d340fab58d4b65b867b942",
            "74084662c69b421a9046dc995aa6e1ac",
            "6681db9a83ce492783d6a44944c9edfc",
            "7d32a8d66d504a3aaa986f8057fa1caa",
            "bb01cdf8a8d1445ca6202e86b28fe1ce",
            "ee496ba2eecc4f699ba7b1873c2d7fde",
            "6c1b53e895be4ea49735d4b9a89138f9",
            "a248144bd5444c7c90496909f046b03e",
            "2f8c4838d26f4140a43438b0f421081c",
            "0beacf615fd043b5a7f3420fcadde514",
            "68ce2d81807c4bc885e941140110e9b5",
            "80dd97d656fe437d81d0fb8e9d312a34",
            "1638528905454fba8d0c62b9a87dda85",
            "9be4c601fe0848d88ae3398951aab830",
            "f16df2a0504b4f62a735e04f4c2f1357",
            "8d1d9401722d41b48684c070c5463b31",
            "8a1587968d1b4c6692ba283bf581ef82",
            "bcbf00253ab042dbb4ef4623c6d429ca",
            "1f55370bf98442d3be4b96760fc7ef4a",
            "c141505bbd0d43e0bd665c199063737d",
            "96bd5899c6c14020816281026b5e0ded",
            "9586c9f4c6dd48e68ad3f75203cd468f",
            "315dbfaaa53f405785ea2418039d3704",
            "ecf7493dbfb5479f83be65385fd409d3",
            "def996ec909343bb9566c990c428acb9",
            "00f0a6d2768d45009909ef8d7cee7712",
            "220cfd4fa04e4ca1874f0de1e294c791",
            "8c1c335051d048dead16bd93e550fd6d",
            "e0ed9e626ac74446aa41eedd63d0494f",
            "19b7996eaba74e29ab349d9d87eb1595"
          ]
        },
        "id": "BJE3Y2NbvuIF",
        "outputId": "2e3c107d-1030-4d98-aec3-b2a9af78ee6b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3da675de37074ebcb32cb1e65b581b00"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/31.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a3a2bbe6a4774f61b67620c2ec13df8e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e951e0be021f4a2e9e7e2d438709e58c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "zephyr-7b-alpha.Q4_K_M.gguf:   0%|          | 0.00/4.37G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "15c50aded4f245a7b62316252108898c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "CTransformersModel has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
            "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
            "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
            "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/1.43k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5df15fb0cec04d05b3f0e236665dc9f1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "024f5e2506b44f19a2ab06a75614be2d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e0c8f706f70142e6870c4499ee9b5bfa"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "added_tokens.json:   0%|          | 0.00/42.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2f8c4838d26f4140a43438b0f421081c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/168 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c141505bbd0d43e0bd665c199063737d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"I have a topic that contains the following documents:\n",
        "[DOCUMENTS]\n",
        "\n",
        "The topic is described by the following keywords: '[KEYWORDS]'.\n",
        "\n",
        "Based on the documents and keywords, what is this topic about?\"\"\"\n",
        "\n",
        "# Update our topic representations using Flan-T5\n",
        "generator = pipeline('text2text-generation', model='google/flan-t5-small')\n",
        "representation_model = TextGeneration(\n",
        "    generator, prompt=prompt, doc_length=50, tokenizer=\"whitespace\"\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258,
          "referenced_widgets": [
            "2be8c4184f8843ed85b728b88e2763a4",
            "9d386126a002492eb192fad62d14ddfa",
            "f500fc4c56af47bb8172903e7f8ccb28",
            "d1f3d04737274ee58fefbdab1fa02cc0",
            "6999822b75404b2282673e6803ab020a",
            "70cd847bab024cc6995e54f375c29ebe",
            "f1f94ee51f9d49649335c73a0cb57b5e",
            "77130c4ee6cb4080ae0d510515e5c066",
            "3775f7b1db044b07bdaf3658580a763c",
            "07c63f5873194197a3b59fb6e173fcde",
            "7a0df5c7a500423c987e8242b0b993af",
            "4b79b7197502486dac3b080856eb6149",
            "c59ba5d4add44862b450d5eb9157ba09",
            "e6f1569903d84ed8a0ac5d0c3253e975",
            "a64b4a5abaf34094bd155a7a790a5663",
            "384a883726fd4564957174bfc7cfee9c",
            "3cfdd160330f415d93b5de3ef9d7e190",
            "c9f81ee447834ee889e70d0a35d82f57",
            "ce648ed1356d471cbe837d7f9a5f04b1",
            "7e2a3de771ae4ca5bc93bf2d4bf65dd6",
            "f66072c1ae934ecdb5479ca6ca756511",
            "0500277c739841bfa91a9d0172b3a533",
            "af4309cfc8e44353935dd5e97cf3d1b2",
            "e613f48e2e564731b1f13dec39bd249d",
            "e8761f44640542cb862b8e786c2c0498",
            "f024c6b3aff944d5ab50d08c4b12a476",
            "c1dcb7acf18342c7adafc719cff992f8",
            "31d50a5aaf954ba8b485b0a1b3bfbee6",
            "d000aafd059844a2b9ff5d0439991ed0",
            "cd386ad4705346e9801107c701d5a5c2",
            "7b8b7973730742289fcc1ddc97e86794",
            "310a911492c54d5bb122e5ca1a024607",
            "83f37c1ef4954e2689c86e464fab9f15",
            "7bcaadea7a7a42daaa32cf3f75db7f0d",
            "e9c1f89d58004db48b36c314832fd09c",
            "89e01fb7aed9460f967816e72856bff4",
            "43c0cd87944d43d88acb6cc5bde32c15",
            "f3abd6dfc34141389a8e1b34f657ecc1",
            "39aaece502344d82b513ebefa57a65c9",
            "ff9fddc95e584c7baded77215a3901c4",
            "3557d8d7eb2a4064a63b523c31cd39ca",
            "70c02e57e1354c0c8dc76eb9acb807c1",
            "ad36570582de4b24b62e43d25a263730",
            "10895bc78b874cbf81a109e8b0969ff9",
            "44c82d2752f54352a0195b765fd1eaeb",
            "c3b435f1fd0f485482d70715c4e380a3",
            "7898e0d34754490693d5b203ecca80b7",
            "ab25396f47a14e85a52dcf28819ddf4e",
            "5b33767d6a2b4fbcab4b1cd4f1a0d7e3",
            "9d96002987354bd1b106addcf118d8d5",
            "cee9f25416b24ae4aa24d16c98ab6148",
            "65dffb18535945fb96b587a2bc86bb1c",
            "562772e91d204f7c8fc0057f53bf7758",
            "c0181ef6f45149b396b12b76b87f8fda",
            "9ba1ef5dbc714dbb834dfd2dda6b7467",
            "ff1358f1374940afbe9c849d37c3d758",
            "aec44cd4887c4f81b0b99df0b5c51f0a",
            "5cdef7aee3de4f328e7eb9d179eaf78c",
            "d9734653376b46cb9fa8fad0657d487d",
            "e64105093aa24e44a50ef0417febc926",
            "b4858d5d1ade4b26b3e9943ed114b116",
            "a1645d0254de4f12bb546e951143c57e",
            "f161fc5bbaf04999a06754458e6b922d",
            "e3c753b418bc426d83875b2243e8e21f",
            "bae15804491c46fab31295fc09ce403b",
            "1942969ddfd7483e95f68a41287f5d53",
            "9ef3078ef2864c109b277b9c1e5eb1a2",
            "5c4ba008e6ea4696bc3b87cb4aa88702",
            "0f160dd4c1c04166ab90b7ca8eb39872",
            "2897ec4e9a334649b0611e5a53d5b5d4",
            "344a8edb435d4fc899ca03ac924deed7",
            "ed924ec452cb48739c95a0aa95d21b85",
            "aadee93982bf4854bdcf2c4bf90d429e",
            "b9f6113d323a414ab2f2ab9c4cd1831b",
            "d7d3a18d2a5f43aab9dbc7710c5bc53f",
            "b676ca0968644331bba4077f6cd72e28",
            "ade3e7300bb642118251b4d24fd7e8f6"
          ]
        },
        "id": "fDfMPd0X-vSl",
        "outputId": "786c4a03-e408-4a96-b394-4e5dc382996c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/1.40k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2be8c4184f8843ed85b728b88e2763a4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/308M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4b79b7197502486dac3b080856eb6149"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "af4309cfc8e44353935dd5e97cf3d1b2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7bcaadea7a7a42daaa32cf3f75db7f0d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "44c82d2752f54352a0195b765fd1eaeb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ff1358f1374940afbe9c849d37c3d758"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9ef3078ef2864c109b277b9c1e5eb1a2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "topic_model = BERTopic(\n",
        "    embedding_model=embedding_model,\n",
        "    umap_model=umap_model,\n",
        "    hdbscan_model=hdbscan_model,\n",
        "    representation_model=representation_model,\n",
        "    verbose=True\n",
        ")\n",
        "topics, probs = topic_model.fit_transform(abstracts, embeddings)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d5JO9HDouWNU",
        "outputId": "bfc274d7-c988-4927-9261-2dbfd87ae6d5"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-03-03 12:29:20,251 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
            "2025-03-03 12:30:22,988 - BERTopic - Dimensionality - Completed ✓\n",
            "2025-03-03 12:30:22,991 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
            "2025-03-03 12:30:30,450 - BERTopic - Cluster - Completed ✓\n",
            "2025-03-03 12:30:30,463 - BERTopic - Representation - Extracting topics from clusters using representation models.\n",
            "100%|██████████| 153/153 [00:13<00:00, 11.03it/s]\n",
            "2025-03-03 12:30:52,484 - BERTopic - Representation - Completed ✓\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "topic_model.get_topic(4, full=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w9Kk8mqz4_SD",
        "outputId": "fd325dfb-cd47-4841-bc39-0d195eae068a"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Main': [('Science/Tech', 1),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0)]}"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "topic_model.get_topics()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YJAba6c0_lt2",
        "outputId": "7e65382a-ac79-453d-98c8-326c0270628a"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{-1: [('Word representation', 1),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0)],\n",
              " 0: [('Question answering systems', 1),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0)],\n",
              " 1: [('Speech recognition', 1),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0)],\n",
              " 2: [('Image-language learning', 1),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0)],\n",
              " 3: [('Summarization', 1),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0)],\n",
              " 4: [('Science/Tech', 1),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0)],\n",
              " 5: [('Hate Speech', 1),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0)],\n",
              " 6: [('Science/Tech', 1),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0)],\n",
              " 7: [('Relation extraction', 1),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0)],\n",
              " 8: [('named entity recognition', 1),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0)],\n",
              " 9: [('Science/Tech', 1),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0)],\n",
              " 10: [('Legal', 1),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0)],\n",
              " 11: [('Pre-trained language models', 1),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0)],\n",
              " 12: [('Dependency grammar induction', 1),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0)],\n",
              " 13: [('Aspect-based sentiment analysis', 1),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0)],\n",
              " 14: [('Science/Tech', 1),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0)],\n",
              " 15: [('Human-machine interaction', 1),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0)],\n",
              " 16: [('Machine Translation', 1),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0)],\n",
              " 17: [('PPT', 1),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0)],\n",
              " 18: [('Sentiment analysis', 1),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0)],\n",
              " 19: [('Word embeddings', 1),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0)],\n",
              " 20: [('word frequency', 1),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0)],\n",
              " 21: [('Science/Tech', 1),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0)],\n",
              " 22: [('Topics', 1),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0)],\n",
              " 23: [('Learning from adversarial text attacks', 1),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0)],\n",
              " 24: [('Dialogue policy learning', 1),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0)],\n",
              " 25: [('Science/Tech', 1),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0)],\n",
              " 26: [('Classification', 1),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0)],\n",
              " 27: [('Text simplification', 1),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0)],\n",
              " 28: [('Argumentation', 1),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0)],\n",
              " 29: [('Controllable text generation', 1),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0)],\n",
              " 30: [('Science/Tech', 1),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0)],\n",
              " 31: [('A coreference resolution system is to find expressions that refer to the same entity in a text',\n",
              "   1),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0)],\n",
              " 32: [('Text style analysis', 1),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0)],\n",
              " 33: [('Attention', 1),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0)],\n",
              " 34: [('Event extraction', 1),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0)],\n",
              " 35: [('query language', 1),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0)],\n",
              " 36: [('Science/Tech', 1),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0)],\n",
              " 37: [('Word sense disambiguation', 1),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0)],\n",
              " 38: [('Science/Tech', 1),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0)],\n",
              " 39: [('Code Generation', 1),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0)],\n",
              " 40: [('Entity linking', 1),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0)],\n",
              " 41: [('Threat', 1),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0)],\n",
              " 42: [('Depressive disorder', 1),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0)],\n",
              " 43: [('Natural language inference', 1),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0)],\n",
              " 44: [('linguistic processing', 1),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0)],\n",
              " 45: [('Science/Tech', 1),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0)],\n",
              " 46: [('Science/Tech', 1),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0)],\n",
              " 47: [('semantics', 1),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0)],\n",
              " 48: [('Science/Tech', 1),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0)],\n",
              " 49: [('Dialogue state tracking', 1),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0)],\n",
              " 50: [('citation', 1),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0)],\n",
              " 51: [('Story generation', 1),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0)],\n",
              " 52: [('Text style transfer', 1),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0)],\n",
              " 53: [('grammatical error correction', 1),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0)],\n",
              " 54: [('stance detection', 1),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0)],\n",
              " 55: [('Multilingual language models', 1),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0)],\n",
              " 56: [('Discourse analysis', 1),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0)],\n",
              " 57: [('Paraphrase generation', 1),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0)],\n",
              " 58: [('Language', 1),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0)],\n",
              " 59: [('Emotion', 1),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0)],\n",
              " 60: [('Dialogue', 1),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0)],\n",
              " 61: [('Science/Tech', 1),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0)],\n",
              " 62: [('Instruction tuning', 1),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0)],\n",
              " 63: [('Science/Tech', 1),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0)],\n",
              " 64: [('Chain-of-Thought', 1),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0)],\n",
              " 65: [('Reinforcement Learning from Human Feedback', 1),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0)],\n",
              " 66: [('World', 1),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0)],\n",
              " 67: [('Science/Tech', 1),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0)],\n",
              " 68: [('semantic parsing', 1),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0)],\n",
              " 69: [('Natural Language Generation', 1),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0)],\n",
              " 70: [('Poetry Generation', 1),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0)],\n",
              " 71: [('Knowledge editing', 1),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0)],\n",
              " 72: [('Biomedical name recognition', 1),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0)],\n",
              " 73: [('Languages', 1),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0)],\n",
              " 74: [('Intent Detection', 1),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0)],\n",
              " 75: [('Pre-training', 1),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0)],\n",
              " 76: [('Knowledge distillation', 1),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0)],\n",
              " 77: [('Science/Tech', 1),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0)],\n",
              " 78: [('empathy', 1),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0)],\n",
              " 79: [('Keyphrase annotation', 1),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0)],\n",
              " 80: [('Science/Tech', 1),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0)],\n",
              " 81: [('Human', 1),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0)],\n",
              " 82: [('Radiology', 1),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0)],\n",
              " 83: [('Parameters of abstract meaning representation parsing', 1),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0)],\n",
              " 84: [('Science/Tech', 1),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0)],\n",
              " 85: [('sarcasm detection', 1),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0)],\n",
              " 86: [('Continual learning', 1),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0)],\n",
              " 87: [('Meetings', 1),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0)],\n",
              " 88: [('idioms', 1),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0)],\n",
              " 89: [('User-Oriented Dialogue', 1),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0)],\n",
              " 90: [('Word alignment', 1),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0)],\n",
              " 91: [('Personality', 1),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0)],\n",
              " 92: [('Documents', 1),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0)],\n",
              " 93: [('Learning cross-lingual word embeddings', 1),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0)],\n",
              " 94: [('Knowledge-grounded dialogue systems', 1),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0)],\n",
              " 95: [('semantics of a sentence', 1),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0)],\n",
              " 96: [('Science/Tech', 1),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0)],\n",
              " 97: [('grammar', 1),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0)],\n",
              " 98: [('Compositional generalization', 1),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0)],\n",
              " 99: [('Science/Tech', 1),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0)],\n",
              " 100: [('Authoregressive machine translation', 1),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0)],\n",
              " 101: [('Video-visual scene-aware dialogue', 1),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0)],\n",
              " 102: [('Automated essay scoring', 1),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0)],\n",
              " 103: [('location-based services', 1),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0)],\n",
              " 104: [('Text augmentation', 1),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0)],\n",
              " 105: [('nlp, annotation, benchmarks', 1),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0)],\n",
              " 106: [('morphological inflection', 1),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0)],\n",
              " 107: [('Science/Tech', 1),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0)],\n",
              " 108: [('Code-switching', 1),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0)],\n",
              " 109: [('Dialect identification', 1),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0)],\n",
              " 110: [('Science/Tech', 1),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0)],\n",
              " 111: [('Multi-criteria Chinese word segmentation', 1),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0)],\n",
              " 112: [('Biomedical literature', 1),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0)],\n",
              " 113: [('Solving math word problems', 1),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0)],\n",
              " 114: [('Sign Language Translation', 1),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0)],\n",
              " 115: [('Science/Tech', 1),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0)],\n",
              " 116: [('e-commerce product catalogs', 1),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0)],\n",
              " 117: [('Chinese spelling error correction', 1),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0)],\n",
              " 118: [('Emotion Recognition in Conversation', 1),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0)],\n",
              " 119: [('ad', 1),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0)],\n",
              " 120: [('Antarlekhaka', 1),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0)],\n",
              " 121: [('Science/Tech', 1),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0)],\n",
              " 122: [('Science/Tech', 1),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0)],\n",
              " 123: [('Science/Tech', 1),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0)],\n",
              " 124: [('Learning in contrastive learning', 1),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0)],\n",
              " 125: [('Confounding', 1),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0)],\n",
              " 126: [('personality', 1),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0)],\n",
              " 127: [('Speech Emotion Recognition', 1),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0)],\n",
              " 128: [('Sequence labeling', 1),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0)],\n",
              " 129: [('Human', 1),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0)],\n",
              " 130: [('Taggers, tagging, POS', 1),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0)],\n",
              " 131: [('Science/Tech', 1),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0)],\n",
              " 132: [('Active Learning', 1),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0)],\n",
              " 133: [('Computers and languages', 1),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0)],\n",
              " 134: [('user', 1),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0)],\n",
              " 135: [('Science/Tech', 1),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0)],\n",
              " 136: [('Counterfactuals', 1),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0)],\n",
              " 137: [('Temporal and causal relations', 1),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0)],\n",
              " 138: [('Science/Tech', 1),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0)],\n",
              " 139: [('Sentence embedding', 1),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0)],\n",
              " 140: [('Image memes', 1),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0)],\n",
              " 141: [('Job', 1),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0)],\n",
              " 142: [('Graphics and classification', 1),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0)],\n",
              " 143: [('Science/Tech', 1),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0)],\n",
              " 144: [('Dialogue act identification', 1),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0)],\n",
              " 145: [('emoji', 1),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0)],\n",
              " 146: [('Generation', 1),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0)],\n",
              " 147: [('Psychiatric', 1),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0)],\n",
              " 148: [('Science/Tech', 1),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0)],\n",
              " 149: [('Code-mixing', 1),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0)],\n",
              " 150: [('Diffusion', 1),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0)],\n",
              " 151: [('Coherence', 1),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0),\n",
              "  ('', 0)]}"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "topic_info = topic_model.get_topic_info()\n",
        "print(topic_info)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6mBw3hbK_y5r",
        "outputId": "a39efe48-c828-4708-b066-9d2206b4df7b"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     Topic  Count                             Name  \\\n",
            "0       -1  14462        -1_Word representation___   \n",
            "1        0   2241  0_Question answering systems___   \n",
            "2        1   2098          1_Speech recognition___   \n",
            "3        2    903     2_Image-language learning___   \n",
            "4        3    887               3_Summarization___   \n",
            "..     ...    ...                              ...   \n",
            "148    147     54               147_Psychiatric___   \n",
            "149    148     53              148_Science/Tech___   \n",
            "150    149     52               149_Code-mixing___   \n",
            "151    150     51                 150_Diffusion___   \n",
            "152    151     51                 151_Coherence___   \n",
            "\n",
            "                                     Representation  \\\n",
            "0           [Word representation, , , , , , , , , ]   \n",
            "1    [Question answering systems, , , , , , , , , ]   \n",
            "2            [Speech recognition, , , , , , , , , ]   \n",
            "3       [Image-language learning, , , , , , , , , ]   \n",
            "4                 [Summarization, , , , , , , , , ]   \n",
            "..                                              ...   \n",
            "148                 [Psychiatric, , , , , , , , , ]   \n",
            "149                [Science/Tech, , , , , , , , , ]   \n",
            "150                 [Code-mixing, , , , , , , , , ]   \n",
            "151                   [Diffusion, , , , , , , , , ]   \n",
            "152                   [Coherence, , , , , , , , , ]   \n",
            "\n",
            "                                   Representative_Docs  \n",
            "0    [  Cross-lingual text classification aims at t...  \n",
            "1    [  Question generation (QG) attempts to solve ...  \n",
            "2    [  End-to-end models have achieved impressive ...  \n",
            "3    [  In this paper we propose a model to learn m...  \n",
            "4    [  We present a novel divide-and-conquer metho...  \n",
            "..                                                 ...  \n",
            "148  [  Mental health care poses an increasingly se...  \n",
            "149  [  Over the last few years, large language mod...  \n",
            "150  [  In today's interconnected and multilingual ...  \n",
            "151  [  Diffusion models have achieved great succes...  \n",
            "152  [  While there has been significant progress t...  \n",
            "\n",
            "[153 rows x 5 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_abstract = [\"In recent years, Question Answering (QA) systems have gained significant attention due to their ability to provide precise answers to user queries. These systems leverage natural language processing (NLP) and deep learning models to extract relevant information from structured and unstructured data sources. Modern QA architectures, such as retrieval-based and generative models, enhance response accuracy by integrating transformer-based models like BERT, GPT, and T5. Challenges such as context understanding, ambiguity resolution, and domain adaptation remain crucial for improving performance. This study explores advancements in QA systems, emphasizing knowledge graph integration, fine-tuning strategies, and real-world applications in customer support, education, and healthcare.\"]\n",
        "\n",
        "topic, prob = topic_model.transform(new_abstract)\n",
        "print(f\"Predicted Topic: {topic[0]}, Probability: {prob[0]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136,
          "referenced_widgets": [
            "de28307598e149deb6652e8682f8f244",
            "cf316b8d0b9d472eaabbfaecde9088ce",
            "6faaa257d1ed4aea981d986606851a06",
            "e11458e6d5e8499eae13b94bea484f0d",
            "dbc492974f8e47a9af028a13bfc482fe",
            "c0d543b508844380873eafe3d8d26522",
            "c70e28971c4346bd87e9ef93fc7ecd76",
            "c2d3ee339cef4b2ebfc6e1063282a6a6",
            "c19f4067b6ee4084a1ffa7d207563e31",
            "9442aa3fc9884616998f68262bacff27",
            "a9ec4250a2ac43dba1a8a27033e1a40f"
          ]
        },
        "id": "rdUJVizQAU-h",
        "outputId": "b05b39e8-9f93-4758-e64b-e9f66f1162e7"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "de28307598e149deb6652e8682f8f244"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-03-03 12:38:14,009 - BERTopic - Dimensionality - Reducing dimensionality of input embeddings.\n",
            "2025-03-03 12:38:14,022 - BERTopic - Dimensionality - Completed ✓\n",
            "2025-03-03 12:38:14,024 - BERTopic - Clustering - Approximating new points with `hdbscan_model`\n",
            "2025-03-03 12:38:14,027 - BERTopic - Cluster - Completed ✓\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted Topic: 0, Probability: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "topic_model.get_topic(0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LWuOrczHAb1B",
        "outputId": "d9e62d20-4275-4821-effb-52df165e2166"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Question answering systems', 1),\n",
              " ('', 0),\n",
              " ('', 0),\n",
              " ('', 0),\n",
              " ('', 0),\n",
              " ('', 0),\n",
              " ('', 0),\n",
              " ('', 0),\n",
              " ('', 0),\n",
              " ('', 0)]"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "topic_model.get_representative_docs(148)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CP_usCLGBDrJ",
        "outputId": "5e97f92f-8926-4f06-ca30-bbcf16e3f1fc"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['  Over the last few years, large language models (LLMs) have emerged as the\\nmost important breakthroughs in natural language processing (NLP) that\\nfundamentally transform research and developments in the field. ChatGPT\\nrepresents one of the most exciting LLM systems developed recently to showcase\\nimpressive skills for language generation and highly attract public attention.\\nAmong various exciting applications discovered for ChatGPT in English, the\\nmodel can process and generate texts for multiple languages due to its\\nmultilingual training data. Given the broad adoption of ChatGPT for English in\\ndifferent problems and areas, a natural question is whether ChatGPT can also be\\napplied effectively for other languages or it is necessary to develop more\\nlanguage-specific technologies. The answer to this question requires a thorough\\nevaluation of ChatGPT over multiple tasks with diverse languages and large\\ndatasets (i.e., beyond reported anecdotes), which is still missing or limited\\nin current research. Our work aims to fill this gap for the evaluation of\\nChatGPT and similar LLMs to provide more comprehensive information for\\nmultilingual NLP applications. While this work will be an ongoing effort to\\ninclude additional experiments in the future, our current paper evaluates\\nChatGPT on 7 different tasks, covering 37 diverse languages with high, medium,\\nlow, and extremely low resources. We also focus on the zero-shot learning\\nsetting for ChatGPT to improve reproducibility and better simulate the\\ninteractions of general users. Compared to the performance of previous models,\\nour extensive experimental results demonstrate a worse performance of ChatGPT\\nfor different NLP tasks and languages, calling for further research to develop\\nbetter models and understanding for multilingual learning.\\n',\n",
              " \"  The development of large language models (LLMs) such as ChatGPT has brought a\\nlot of attention recently. However, their evaluation in the benchmark academic\\ndatasets remains under-explored due to the difficulty of evaluating the\\ngenerative outputs produced by this model against the ground truth. In this\\npaper, we aim to present a thorough evaluation of ChatGPT's performance on\\ndiverse academic datasets, covering tasks like question-answering, text\\nsummarization, code generation, commonsense reasoning, mathematical\\nproblem-solving, machine translation, bias detection, and ethical\\nconsiderations. Specifically, we evaluate ChatGPT across 140 tasks and analyze\\n255K responses it generates in these datasets. This makes our work the largest\\nevaluation of ChatGPT in NLP benchmarks. In short, our study aims to validate\\nthe strengths and weaknesses of ChatGPT in various tasks and provide insights\\nfor future research using LLMs. We also report a new emergent ability to follow\\nmulti-query instructions that we mostly found in ChatGPT and other\\ninstruction-tuned models. Our extensive evaluation shows that even though\\nChatGPT is capable of performing a wide variety of tasks, and may obtain\\nimpressive performance in several benchmark datasets, it is still far from\\nachieving the ability to reliably solve many challenging tasks. By providing a\\nthorough assessment of ChatGPT's performance across diverse NLP tasks, this\\npaper sets the stage for a targeted deployment of ChatGPT-like LLMs in\\nreal-world applications.\\n\",\n",
              " \"  Recently, ChatGPT has attracted great attention, as it can generate fluent\\nand high-quality responses to human inquiries. Several prior studies have shown\\nthat ChatGPT attains remarkable generation ability compared with existing\\nmodels. However, the quantitative analysis of ChatGPT's understanding ability\\nhas been given little attention. In this report, we explore the understanding\\nability of ChatGPT by evaluating it on the most popular GLUE benchmark, and\\ncomparing it with 4 representative fine-tuned BERT-style models. We find that:\\n1) ChatGPT falls short in handling paraphrase and similarity tasks; 2) ChatGPT\\noutperforms all BERT models on inference tasks by a large margin; 3) ChatGPT\\nachieves comparable performance compared with BERT on sentiment analysis and\\nquestion-answering tasks. Additionally, by combining some advanced prompting\\nstrategies, we show that the understanding ability of ChatGPT can be further\\nimproved.\\n\"]"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "topics, probs = topic_model.transform([\"  Over the last few years, large language models (LLMs) have emerged as the\\nmost important breakthroughs in natural language processing (NLP) that\\nfundamentally transform research and developments in the field. ChatGPT\\nrepresents one of the most exciting LLM systems developed recently to showcase\\nimpressive skills for language generation and highly attract public attention.\\nAmong various exciting applications discovered for ChatGPT in English, the\\nmodel can process and generate texts for multiple languages due to its\\nmultilingual training data. Given the broad adoption of ChatGPT for English in\\ndifferent problems and areas, a natural question is whether ChatGPT can also be\\napplied effectively for other languages or it is necessary to develop more\\nlanguage-specific technologies. The answer to this question requires a thorough\\nevaluation of ChatGPT over multiple tasks with diverse languages and large\\ndatasets (i.e., beyond reported anecdotes), which is still missing or limited\\nin current research. Our work aims to fill this gap for the evaluation of\\nChatGPT and similar LLMs to provide more comprehensive information for\\nmultilingual NLP applications. While this work will be an ongoing effort to\\ninclude additional experiments in the future, our current paper evaluates\\nChatGPT on 7 different tasks, covering 37 diverse languages with high, medium,\\nlow, and extremely low resources. We also focus on the zero-shot learning\\nsetting for ChatGPT to improve reproducibility and better simulate the\\ninteractions of general users. Compared to the performance of previous models,\\nour extensive experimental results demonstrate a worse performance of ChatGPT\\nfor different NLP tasks and languages, calling for further research to develop\\nbetter models and understanding for multilingual learning.\\n\"])\n",
        "print(f\"Predicted Topic: {topics[0]}, Probability: {probs[0]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136,
          "referenced_widgets": [
            "74e6383d59624698905bf858efaacbc5",
            "f823601892d1474bbbce4c0ce0a32760",
            "d27bdc9783e74d5cac82c346bbc4eaa4",
            "67ec1bdd9fa746b592b3b59eb9f6fbf6",
            "395f7e1356004d3f9a1e84581db7ee5f",
            "81154e2d4f4b406fbf7f7ee643b639ca",
            "26363cbfb1b0437b9476949f5818bc58",
            "fef046e5fd094098ab6519516166872e",
            "fbddce631f824743876d352e6654ec43",
            "1c5a346f327644f98631295fc954e019",
            "75eafad6463a403a84241541836903fa"
          ]
        },
        "id": "MtzyV2XVBZR6",
        "outputId": "f35063bb-d45a-49eb-938f-dca48321de81"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "74e6383d59624698905bf858efaacbc5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-03-03 12:42:03,801 - BERTopic - Dimensionality - Reducing dimensionality of input embeddings.\n",
            "2025-03-03 12:42:03,809 - BERTopic - Dimensionality - Completed ✓\n",
            "2025-03-03 12:42:03,811 - BERTopic - Clustering - Approximating new points with `hdbscan_model`\n",
            "2025-03-03 12:42:03,815 - BERTopic - Cluster - Completed ✓\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted Topic: 148, Probability: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "topic_model.save(\"bertopic_model\")\n",
        "# loaded_model = BERTopic.load(\"bertopic_model\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zr8IWU8KBqPy",
        "outputId": "d66fc8ee-c257-44f8-cbca-5f852663992a"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-03-03 12:55:28,898 - BERTopic - WARNING: When you use `pickle` to save/load a BERTopic model,please make sure that the environments in which you saveand load the model are **exactly** the same. The version of BERTopic,its dependencies, and python need to remain the same.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_model = \"thenlper/gte-small\"\n",
        "topic_model.save(\"my_model_dir\", serialization=\"safetensors\", save_ctfidf=True, save_embedding_model=embedding_model)"
      ],
      "metadata": {
        "id": "eh5krCqzE1Gy"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from bertopic import BERTopic\n",
        "embedding_model1 = \"thenlper/gte-small\"\n",
        "loaded_model = BERTopic.load(\"my_model_dir\", embedding_model=embedding_model1)\n"
      ],
      "metadata": {
        "id": "pPL2RLiQFFYi"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topics, probs = loaded_model.transform([\"  Over the last few years, large language models (LLMs) have emerged as the\\nmost important breakthroughs in natural language processing (NLP) that\\nfundamentally transform research and developments in the field. ChatGPT\\nrepresents one of the most exciting LLM systems developed recently to showcase\\nimpressive skills for language generation and highly attract public attention.\\nAmong various exciting applications discovered for ChatGPT in English, the\\nmodel can process and generate texts for multiple languages due to its\\nmultilingual training data. Given the broad adoption of ChatGPT for English in\\ndifferent problems and areas, a natural question is whether ChatGPT can also be\\napplied effectively for other languages or it is necessary to develop more\\nlanguage-specific technologies. The answer to this question requires a thorough\\nevaluation of ChatGPT over multiple tasks with diverse languages and large\\ndatasets (i.e., beyond reported anecdotes), which is still missing or limited\\nin current research. Our work aims to fill this gap for the evaluation of\\nChatGPT and similar LLMs to provide more comprehensive information for\\nmultilingual NLP applications. While this work will be an ongoing effort to\\ninclude additional experiments in the future, our current paper evaluates\\nChatGPT on 7 different tasks, covering 37 diverse languages with high, medium,\\nlow, and extremely low resources. We also focus on the zero-shot learning\\nsetting for ChatGPT to improve reproducibility and better simulate the\\ninteractions of general users. Compared to the performance of previous models,\\nour extensive experimental results demonstrate a worse performance of ChatGPT\\nfor different NLP tasks and languages, calling for further research to develop\\nbetter models and understanding for multilingual learning.\\n\"])\n",
        "print(f\"Predicted Topic: {topics[0]}, Probability: {probs[0]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84,
          "referenced_widgets": [
            "ad5503e2d7384a08887de7dd5cce3aac",
            "ec9b90e1ff7e4f49b12d4aaeccdf5268",
            "d30b485abd4d4e62af64ff1b18b968ad",
            "7b48154ae5fe44daa98d49fd6e8f62c8",
            "c3b40ae835d44b89979d6dc682aee2fc",
            "534b03f360b449edbcf024f84b2c8ef6",
            "a35130645bd94e28bde198404369b65c",
            "fb24bd4f0f9b424c81cbf9459f5ff360",
            "2fdd6b82469b47ddb636446c74ee407a",
            "c9a1734c8b514ffe968758052e80f081",
            "2db4ef61953a46ae9f74c405e89e7a0f"
          ]
        },
        "id": "xC1BsHO0FWcH",
        "outputId": "115e518e-b665-48ce-8de5-38a1cc2cb943"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ad5503e2d7384a08887de7dd5cce3aac"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-03-03 13:00:07,071 - BERTopic - Predicting topic assignments through cosine similarity of topic and document embeddings.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted Topic: 148, Probability: 0.9579033255577087\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "shutil.make_archive('my_model_dir', 'zip', 'my_model_dir')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "XMFY9ROTFcae",
        "outputId": "a309d6af-d4d6-4c51-fbea-d055634ecc81"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/my_model_dir.zip'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pqJWJUo5F2Zv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}